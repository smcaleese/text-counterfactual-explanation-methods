{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup\n",
        "\n",
        "Mount Google Drive and clone the repository containing the methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUgYxHsWZU8y",
        "outputId": "4254c890-6bd2-4194-c3cc-c99744f2d96c"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "\n",
        "github_username = input(\"Enter your GitHub username: \")\n",
        "github_token = getpass.getpass(\"Enter your GitHub personal access token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPOKvD_RafHj",
        "outputId": "96ffda6e-c3e6-466a-d559-0a2b44931081"
      },
      "outputs": [],
      "source": [
        "repo_name = \"smcaleese/masters-thesis-code\"\n",
        "!git clone https://{github_username}:{github_token}@github.com/{repo_name}.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "EDPv3DJ8cER4",
        "outputId": "04fbecf9-3acc-4856-de56-f2c2d36c60a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'masters-thesis-code'\n",
            "/Users/smcaleese/Documents/masters-thesis-code\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
            "  bkms = self.shell.db.get('bookmarks', {})\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd masters-thesis-code\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers datasets textdistance openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download datasets\n",
        "\n",
        "Download the SST-2, QNLI and AG News datasets, clean the sentences, and create a list of input sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "sst = load_dataset(\"stanfordnlp/sst2\")\n",
        "qnli = load_dataset(\"glue\", \"qnli\")\n",
        "ag_news = load_dataset(\"fancyzhx/ag_news\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 67349\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 1821\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'sentence', 'label', 'idx'],\n",
              "        num_rows: 104743\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'sentence', 'label', 'idx'],\n",
              "        num_rows: 5463\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'sentence', 'label', 'idx'],\n",
              "        num_rows: 5463\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qnli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ag_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'sst' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[57], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 3\u001b[0m sst_sentences \u001b[38;5;241m=\u001b[39m \u001b[43msst\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m sst_labels \u001b[38;5;241m=\u001b[39m sst[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m sst_sentences_subset \u001b[38;5;241m=\u001b[39m sst_sentences[:num_samples]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sst' is not defined"
          ]
        }
      ],
      "source": [
        "num_samples = 100\n",
        "\n",
        "sst_sentences = sst[\"validation\"][\"sentence\"]\n",
        "sst_labels = sst[\"validation\"][\"label\"]\n",
        "\n",
        "sst_sentences_subset = sst_sentences[:num_samples]\n",
        "sst_labels_subset = sst_labels[:num_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "qnli_questions = qnli[\"validation\"][\"question\"]\n",
        "qnli_answers = qnli[\"validation\"][\"sentence\"]\n",
        "qnli_labels = qnli[\"validation\"][\"label\"]\n",
        "\n",
        "qnli_questions_subset = qnli_questions[:num_samples]\n",
        "qnli_answers_subset = qnli_answers[:num_samples]\n",
        "qnli_labels_subset = qnli_labels[:num_samples]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Format the text in the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def format_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # remove two spaces around a comma:\n",
        "    sentence = re.sub(r\"\\s(')\\s(ve|re|s|t|ll|d)\", r\"\\1\\2\", sentence)\n",
        "\n",
        "    # remove spaces around hyphens:\n",
        "    sentence = re.sub(r\"-\\s-\", \"--\", sentence)\n",
        "    sentence = re.sub(r\"(\\w)\\s-\\s(\\w)\", r\"\\1-\\2\", sentence)\n",
        "\n",
        "    def replace(match):\n",
        "        return match.group(1)\n",
        "\n",
        "    # remove spaces before punctuation and n't:\n",
        "    sentence = re.sub(r\"\\s([.!,?:;')]|n't)\", replace, sentence)\n",
        "\n",
        "    # remove spaces after opening parenthesis:\n",
        "    sentence = re.sub(r\"([(])\\s\", replace, sentence)\n",
        "    \n",
        "    return sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'sst_sentences_subset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sst_sentences_subset_formatted \u001b[38;5;241m=\u001b[39m [format_sentence(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43msst_sentences_subset\u001b[49m]\n\u001b[1;32m      3\u001b[0m qnli_questions_subset_formatted \u001b[38;5;241m=\u001b[39m [format_sentence(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m qnli_questions_subset]\n\u001b[1;32m      4\u001b[0m qnli_answers_subset_formatted \u001b[38;5;241m=\u001b[39m [format_sentence(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m qnli_answers_subset]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sst_sentences_subset' is not defined"
          ]
        }
      ],
      "source": [
        "sst_sentences_subset_formatted = [format_sentence(sentence) for sentence in sst_sentences_subset]\n",
        "\n",
        "qnli_questions_subset_formatted = [format_sentence(sentence) for sentence in qnli_questions_subset]\n",
        "qnli_answers_subset_formatted = [format_sentence(sentence) for sentence in qnli_answers_subset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write the sentences to a file named `sst-input.csv` and `qnli-input.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_sst = pd.DataFrame({\n",
        "    \"original_text\": sst_sentences_subset_formatted,\n",
        "    \"original_label\": sst_labels_subset\n",
        "})\n",
        "df_qnli = pd.DataFrame({\n",
        "    \"original_question\": qnli_questions_subset_formatted,\n",
        "    \"original_answer\": qnli_answers_subset_formatted,\n",
        "    \"original_label\": qnli_labels_subset\n",
        "})\n",
        "\n",
        "df_sst.to_csv(\"./input/sst-input.csv\", index=False)\n",
        "df_qnli.to_csv(\"./input/qnli-input.csv\", index=False)\n",
        "\n",
        "# df_ag_news = pd.DataFrame(random_ag_news_sentences_subset_formatted, columns=[\"original_text\"])\n",
        "# df_ag_news.to_csv(\"./input/ag-news-input.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choose dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = \"sst_2\"\n",
        "# dataset = \"qnli\"\n",
        "# dataset = \"ag_news\"\n",
        "\n",
        "if dataset == \"sst_2\":\n",
        "    input_file = \"sst-input\"\n",
        "    model_name = \"textattack/bert-base-uncased-SST-2\"\n",
        "    fizle_task = \"sentiment analysis on the SST-2 dataset\"\n",
        "elif dataset == \"qnli\":\n",
        "    input_file = \"qnli-input\"\n",
        "    model_name = \"textattack/bert-base-uncased-QNLI\"\n",
        "    fizle_task = \"natural language inference on the QNLI dataset\"\n",
        "elif dataset == \"ag_news\":\n",
        "    input_file = \"ag-news-input\"\n",
        "    model_name =  \"textattack/bert-base-uncased-ag-news\"\n",
        "    fizle_task = \"topic classification on the AG News dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create input dataframe\n",
        "\n",
        "Columns to add to create output dataframe:\n",
        "- original_score\n",
        "- original_perplexity\n",
        "- counterfactual_text\n",
        "- counterfactual_score\n",
        "- counterfactual_perplexity\n",
        "- found_flip\n",
        "- frac_tokens_same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>original_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>it's a charming and often affecting journey.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>unflinchingly bleak and desperate</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>allows us to hope that nolan is poised to emba...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the acting, costumes, music, cinematography an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it's slow -- very, very slow.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       original_text  original_label\n",
              "0      it's a charming and often affecting journey.                1\n",
              "1                 unflinchingly bleak and desperate                0\n",
              "2  allows us to hope that nolan is poised to emba...               1\n",
              "3  the acting, costumes, music, cinematography an...               1\n",
              "4                     it's slow -- very, very slow.                0"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_input = pd.read_csv(f\"input/{input_file}.csv\")\n",
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 2)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the sentiment model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "if dataset == \"sst_2\":\n",
        "    id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "    label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    ).to(device)\n",
        "\n",
        "elif dataset == \"qnli\":\n",
        "    id2label = {0: \"entailment\", 1: \"not_entailment\"}\n",
        "    label2id = {\"entailment\": 0, \"not_entailment\": 1}\n",
        "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    ).to(device)\n",
        "\n",
        "# elif dataset == \"ag_news\":\n",
        "#     id2label = {\n",
        "#         0: \"World\",\n",
        "#         1: \"Sports\",\n",
        "#         2: \"Business\",\n",
        "#         3: \"Sci/Tech\"\n",
        "#     }\n",
        "#     label2id = {\n",
        "#         \"World\": 0,\n",
        "#         \"Sports\": 1,\n",
        "#         \"Business\": 2,\n",
        "#         \"Sci/Tech\": 3\n",
        "#     }\n",
        "#     sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#         model_name,\n",
        "#         num_labels=4,\n",
        "#         id2label=id2label,\n",
        "#         label2id=label2id\n",
        "#     ).to(device)\n",
        "\n",
        "sentiment_model_tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the GPT-2 model for calculating perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the language model for CLOSS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "# TODO: try using a larger model to improve performance: https://arxiv.org/pdf/2111.09543\n",
        "LM_model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "LM_model.lm_head = LM_model.cls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import textdistance\n",
        "\n",
        "def calculate_score(text, sentiment_model_tokenizer, dataset, device):\n",
        "    def tokenize_with_correct_token_type_ids(input_text, tokenizer):\n",
        "        # Tokenize the input\n",
        "        tokens = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "        \n",
        "        # Get the position of the first [SEP] token\n",
        "        sep_pos = (tokens.input_ids == tokenizer.sep_token_id).nonzero()[0, 1].item()\n",
        "        \n",
        "        # Create token_type_ids\n",
        "        token_type_ids = torch.zeros_like(tokens.input_ids)\n",
        "        token_type_ids[0, sep_pos+1:] = 1  # Set to 1 after the first [SEP] token\n",
        "        \n",
        "        # Update the tokens dictionary\n",
        "        tokens['token_type_ids'] = token_type_ids\n",
        "        \n",
        "        return tokens\n",
        "\n",
        "    if type(text) == list:\n",
        "        if type(text[0]) == str:\n",
        "            tokens = text\n",
        "            ids = sentiment_model_tokenizer.convert_tokens_to_ids(tokens)\n",
        "            text = sentiment_model_tokenizer.decode(ids[1:-1])\n",
        "        elif type(text[0]) == int:\n",
        "            ids = text\n",
        "            text = sentiment_model_tokenizer.decode(ids[1:-1])\n",
        "\n",
        "    if dataset == \"sst_2\":\n",
        "        inputs = sentiment_model_tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    elif dataset == \"qnli\":\n",
        "        inputs = tokenize_with_correct_token_type_ids(text, sentiment_model_tokenizer).to(device)\n",
        "\n",
        "    logits = sentiment_model(**inputs).logits\n",
        "    prob_positive = torch.nn.functional.softmax(logits, dim=1)[0][1].item()\n",
        "    return prob_positive\n",
        "\n",
        "def calculate_perplexity(text):\n",
        "    inputs = gpt2_tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    loss = gpt2_model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
        "    perplexity = torch.exp(loss).item()\n",
        "    return perplexity\n",
        "\n",
        "def is_flip(original_score, counterfactual_score):\n",
        "    # might need to be updated for AG News\n",
        "    positive_to_negative = original_score >= 0.5 and counterfactual_score < 0.5\n",
        "    negative_to_positive = original_score < 0.5 and counterfactual_score >= 0.5\n",
        "    return positive_to_negative or negative_to_positive\n",
        "\n",
        "def truncate_text(text, max_length=100):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) > max_length:\n",
        "        text = \" \".join(tokens[:max_length])\n",
        "    return text\n",
        "\n",
        "def get_all_embeddings(model, tokenizer):\n",
        "    all_word_embeddings = torch.zeros((tokenizer.vocab_size, 768)).detach().to(device)\n",
        "    for i in range(tokenizer.vocab_size):\n",
        "        input_tensor = torch.tensor(i).view(1, 1).to(device)\n",
        "        word_embedding = model.bert.embeddings.word_embeddings(input_tensor)\n",
        "        all_word_embeddings[i, :] = word_embedding\n",
        "    all_word_embeddings = all_word_embeddings.detach().requires_grad_(False)\n",
        "    return all_word_embeddings\n",
        "\n",
        "def get_levenshtein_similarity_score(original_text, counterfactual_text):\n",
        "    score = 1 - textdistance.levenshtein.normalized_distance(original_text, counterfactual_text)\n",
        "    return score\n",
        "\n",
        "def get_output(df_input, counterfactual_method, args):\n",
        "    df_input = df_input.copy()\n",
        "    output_data = {\n",
        "        \"original_text\": [],\n",
        "        \"original_score\": [],\n",
        "        \"original_perplexity\": [],\n",
        "        \"counterfactual_text\": [],\n",
        "        \"counterfactual_score\": [],\n",
        "        \"counterfactual_perplexity\": [],\n",
        "        \"found_flip\": [],\n",
        "        \"levenshtein_similarity_score\": []\n",
        "    }\n",
        "    if dataset == \"qnli\":\n",
        "        output_data[\"original_question\"] = []\n",
        "\n",
        "    for i in range(len(df_input)):\n",
        "        if dataset == \"sst_2\":\n",
        "            original_text = df_input.iloc[i][\"original_text\"]\n",
        "            original_text = truncate_text(original_text)\n",
        "            print(f\"Processing input {i + 1}/{len(df_input)}: num tokens: {len(original_text.split())}\")\n",
        "\n",
        "            original_score = calculate_score(original_text, sentiment_model_tokenizer, dataset, device)\n",
        "            original_perplexity = calculate_perplexity(original_text)\n",
        "\n",
        "            args = {**args, \"original_score\": original_score}\n",
        "            counterfactual_text = counterfactual_method(original_text, calculate_score, args)\n",
        "            counterfactual_text = format_sentence(counterfactual_text)\n",
        "\n",
        "            label_width = 20\n",
        "            print(f\"\\n{'original_text:'.ljust(label_width)} {original_text}\")\n",
        "            print(f\"{'counterfactual_text:'.ljust(label_width)} {counterfactual_text}\\n\")\n",
        "\n",
        "            counterfactual_score = calculate_score(counterfactual_text, sentiment_model_tokenizer, dataset, device)\n",
        "            counterfactual_perplexity = calculate_perplexity(counterfactual_text)\n",
        "            found_flip = is_flip(original_score, counterfactual_score)\n",
        "            levenshtein_similarity_score = get_levenshtein_similarity_score(original_text, counterfactual_text)\n",
        "\n",
        "            output_data[\"original_text\"].append(original_text)\n",
        "            output_data[\"original_score\"].append(original_score)\n",
        "            output_data[\"original_perplexity\"].append(original_perplexity)\n",
        "            output_data[\"counterfactual_text\"].append(counterfactual_text)\n",
        "            output_data[\"counterfactual_score\"].append(counterfactual_score)\n",
        "            output_data[\"counterfactual_perplexity\"].append(counterfactual_perplexity)\n",
        "            output_data[\"found_flip\"].append(found_flip)\n",
        "            output_data[\"levenshtein_similarity_score\"].append(levenshtein_similarity_score)\n",
        "\n",
        "        elif dataset == \"qnli\":\n",
        "            row = df_input.iloc[i]\n",
        "            original_question, original_answer = row[\"original_question\"], row[\"original_answer\"]\n",
        "            original_input = f\"{original_question} [SEP] {original_answer}\"\n",
        "\n",
        "            print(f\"Processing input {i + 1}/{len(df_input)}: num tokens: {len(f\"{original_question} {original_answer}\".split())}\")\n",
        "\n",
        "            original_score = calculate_score(original_input, sentiment_model_tokenizer, dataset, device)\n",
        "            original_perplexity = calculate_perplexity(original_answer)\n",
        "\n",
        "            args = {**args, \"original_score\": original_score}\n",
        "            # TODO for QNLI:\n",
        "\n",
        "            counterfactual_answer = counterfactual_method(original_input, calculate_score, calculate_score, args)\n",
        "            counterfactual_answer = format_sentence(counterfactual_answer)\n",
        "            counterfactual_input = (original_question, counterfactual_answer)\n",
        "\n",
        "            label_width = 20\n",
        "            print(f\"\\n{'original_answer:'.ljust(label_width)} {original_answer}\")\n",
        "            print(f\"{'counterfactual_answer:'.ljust(label_width)} {counterfactual_answer}\\n\")\n",
        "\n",
        "            counterfactual_score = calculate_score(counterfactual_input, sentiment_model_tokenizer, dataset, device)\n",
        "            counterfactual_perplexity = calculate_perplexity(counterfactual_answer)\n",
        "            found_flip = is_flip(original_score, counterfactual_score)\n",
        "            levenshtein_similarity_score = get_levenshtein_similarity_score(original_answer, counterfactual_answer)\n",
        "\n",
        "            output_data[\"original_question\"].append(original_question)\n",
        "            output_data[\"original_text\"].append(counterfactual_answer)\n",
        "            output_data[\"original_score\"].append(original_score)\n",
        "            output_data[\"original_perplexity\"].append(original_perplexity)\n",
        "            output_data[\"counterfactual_text\"].append(counterfactual_answer)\n",
        "            output_data[\"counterfactual_score\"].append(counterfactual_score)\n",
        "            output_data[\"counterfactual_perplexity\"].append(counterfactual_perplexity)\n",
        "            output_data[\"found_flip\"].append(found_flip)\n",
        "            output_data[\"levenshtein_similarity_score\"].append(levenshtein_similarity_score)\n",
        "\n",
        "    df_output = pd.DataFrame(output_data)\n",
        "    return df_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_word_embeddings = get_all_embeddings(sentiment_model, sentiment_model_tokenizer).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39muserdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get(\"API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i: 0\n",
            "i: 1\n",
            "i: 2\n",
            "i: 3\n",
            "i: 4\n",
            "i: 5\n",
            "i: 6\n",
            "i: 7\n",
            "i: 8\n",
            "i: 9\n",
            "i: 10\n",
            "i: 11\n",
            "i: 12\n",
            "i: 13\n",
            "i: 14\n",
            "i: 15\n",
            "i: 16\n",
            "i: 17\n",
            "i: 18\n",
            "i: 19\n",
            "i: 20\n",
            "i: 21\n",
            "i: 22\n",
            "i: 23\n",
            "i: 24\n",
            "i: 25\n",
            "i: 26\n",
            "i: 27\n",
            "i: 28\n",
            "i: 29\n",
            "i: 30\n",
            "i: 31\n",
            "i: 32\n",
            "i: 33\n",
            "i: 34\n",
            "i: 35\n",
            "i: 36\n",
            "i: 37\n",
            "i: 38\n",
            "i: 39\n",
            "i: 40\n",
            "i: 41\n",
            "i: 42\n",
            "i: 43\n",
            "i: 44\n",
            "i: 45\n",
            "i: 46\n",
            "i: 47\n",
            "i: 48\n",
            "i: 49\n",
            "i: 50\n",
            "i: 51\n",
            "i: 52\n",
            "i: 53\n",
            "i: 54\n",
            "i: 55\n",
            "i: 56\n",
            "i: 57\n",
            "i: 58\n",
            "i: 59\n",
            "i: 60\n",
            "i: 61\n",
            "i: 62\n",
            "i: 63\n",
            "i: 64\n",
            "i: 65\n",
            "i: 66\n",
            "i: 67\n",
            "i: 68\n",
            "i: 69\n",
            "i: 70\n",
            "i: 71\n",
            "i: 72\n",
            "i: 73\n",
            "i: 74\n",
            "i: 75\n",
            "i: 76\n",
            "i: 77\n",
            "i: 78\n",
            "i: 79\n",
            "i: 80\n",
            "i: 81\n",
            "i: 82\n",
            "i: 83\n",
            "i: 84\n",
            "i: 85\n",
            "i: 86\n",
            "i: 87\n",
            "i: 88\n",
            "i: 89\n",
            "i: 90\n",
            "i: 91\n",
            "i: 92\n",
            "i: 93\n",
            "i: 94\n",
            "i: 95\n",
            "i: 96\n",
            "i: 97\n",
            "i: 98\n",
            "i: 99\n",
            "accuracy: 0.92\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "\n",
        "for i in range(len(df_input)):\n",
        "    print(f\"i: {i}\")\n",
        "    row = df_input.iloc[i]\n",
        "\n",
        "    if dataset == \"sst_2\":\n",
        "        original_text, original_label = row[\"original_text\"], row[\"original_label\"]\n",
        "    elif dataset == \"qnli\":\n",
        "        original_question, original_answer, original_label = row[\"original_question\"], row[\"original_answer\"], row[\"original_label\"]\n",
        "        original_text = f\"{original_question} [SEP] {original_answer}\"\n",
        "\n",
        "    score = calculate_score(original_text, sentiment_model_tokenizer, dataset, device)\n",
        "    y_hat = 1 if score >= 0.5 else 0\n",
        "    if y_hat == original_label:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(df_input)\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Counterfactual generator functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %cd \"CLOSS\"\n",
        "# %cd ..\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "from CLOSS.closs import generate_counterfactual\n",
        "import re\n",
        "\n",
        "def generate_polyjuice_counterfactual(original_text, calculate_score, args):\n",
        "    perturbations = pj.perturb(\n",
        "        orig_sent=original_text,\n",
        "        ctrl_code=\"negation\",\n",
        "        num_perturbations=1,\n",
        "        perplex_thred=None\n",
        "    )\n",
        "    counterfactual_text = perturbations[0]\n",
        "    return counterfactual_text\n",
        "\n",
        "def generate_closs_counterfactual(original_text, calculate_score, args):\n",
        "    # TODO: move target label from inside CLOSS to here for AG News dataset\n",
        "    counterfactual_text = generate_counterfactual(\n",
        "        original_text,\n",
        "        sentiment_model,\n",
        "        LM_model,\n",
        "        calculate_score,\n",
        "        sentiment_model_tokenizer,\n",
        "        all_word_embeddings,\n",
        "        device,\n",
        "        args\n",
        "    )\n",
        "    return counterfactual_text\n",
        "\n",
        "def call_openai_api(system_prompt, model):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt}\n",
        "        ],\n",
        "        top_p=1,\n",
        "        temperature=0.4,\n",
        "        frequency_penalty=1.1\n",
        "    )\n",
        "    output = completion.choices[0].message.content\n",
        "    return output\n",
        "\n",
        "def generate_naive_fizle_counterfactual(original_text, calculate_score, args):\n",
        "    original_score, model = args[\"original_score\"], args[\"model\"]\n",
        "    original_label = 1 if original_score >= 0.5 else 0\n",
        "    cf_label = 0 if original_label == 1 else 1\n",
        "\n",
        "    system_prompt = f\"\"\"In the task of {fizle_task}, a trained black-box classifier correctly predicted the label '{original_label}' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from '{original_label}' to '{cf_label}'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
        "    -\n",
        "    Text: {original_text}\"\"\"\n",
        "\n",
        "    correct_output_format = False\n",
        "    for i in range(10):\n",
        "        print(f\"attempt: {i + 1}\")\n",
        "        output = call_openai_api(system_prompt, model)\n",
        "        counterfactual_text = re.search(\"<new>(.*?)</new>\", output).group(1)\n",
        "        if counterfactual_text:\n",
        "            correct_output_format = True\n",
        "            break\n",
        "\n",
        "    if not correct_output_format:\n",
        "        print(\"Failed to generate counterfactual surrounded by <new> tags\")\n",
        "        counterfactual_text = output[5:-6]\n",
        "\n",
        "    return counterfactual_text\n",
        "\n",
        "def generate_guided_fizle_counterfactual(original_text, calculate_score, args):\n",
        "    original_score, model = args[\"original_score\"], args[\"model\"]\n",
        "    original_label = 1 if original_score >= 0.5 else 0\n",
        "    cf_label = 0 if original_label == 1 else 1\n",
        "    system_prompt = \"\"\n",
        "\n",
        "    # 1. Find important words\n",
        "    step1_system_prompt = \" \".join([\n",
        "        f\"In the task of {fizle_task}, a trained black-box classifier correctly predicted the label '{original_label}' for the following text.\",\n",
        "        f\"Explain why the model predicted the '{original_label}' label by identifying the words in the input that caused the label. List ONLY the words as a comma separated list.\",\n",
        "        f\"\\n-\\nText: {original_text}\",\n",
        "        f\"\\nImportant words identified: \"\n",
        "    ])\n",
        "    system_prompt += step1_system_prompt\n",
        "    important_words = call_openai_api(step1_system_prompt, model)\n",
        "    system_prompt += important_words + \"\\n\"\n",
        "\n",
        "    # 2. Generate the final counterfactual\n",
        "    correct_output_format = False\n",
        "    for i in range(10):\n",
        "        step2_system_prompt = \" \".join([\n",
        "            f\"Generate a counterfactual explanation for the original text by ONLY changing a minimal set of the words you identified, so that the label changes from '{original_label}' to '{cf_label}'.\",\n",
        "            f\"Use the following definition of 'counterfactual explanation': 'A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.'\",\n",
        "            f\"Enclose the generated text within <new> tags.\"\n",
        "        ])\n",
        "        final_system_prompt = system_prompt + step2_system_prompt\n",
        "        print(f\"final_system_prompt: {final_system_prompt}\")\n",
        "        step2_output = call_openai_api(final_system_prompt, model)\n",
        "        counterfactual_text = re.search(\"<new>(.*?)</new>\", step2_output).group(1)\n",
        "        if counterfactual_text:\n",
        "            correct_output_format = True\n",
        "            break\n",
        "\n",
        "    if not correct_output_format:\n",
        "        print(\"Failed to generate counterfactual surrounded by <new> tags\")\n",
        "        counterfactual_text = step2_output[5:-6]\n",
        "\n",
        "    return counterfactual_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.9994090795516968\n",
            "19 21\n",
            "Old tokens           :  [CLS] i really loved the \u001b[31mmovie   \u001b[0m and thought it was one of the \u001b[31mbest    \u001b[0m i ' ve ever seen . [SEP]\n",
            "New tokens           :  [CLS] i really loved the \u001b[31msmallpox\u001b[0m and thought it was one of the \u001b[31msmallest\u001b[0m i ' ve ever seen . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.905\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"i really loved the smallpox and thought it was one of the smallest i ' ve ever seen.\""
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# s = \"What came into force after the new constitution was herald? [SEP] As of that day, the new constitution heralding the Second Republic came into force.\"\n",
        "# s = \"I really hated the movie.\"\n",
        "s = \"I really loved the movie and thought it was one of the best I've ever seen.\"\n",
        "\n",
        "args = {\n",
        "    \"beam_width\": 15,\n",
        "    \"w\": 5,\n",
        "    \"K\": 30,\n",
        "    \"substitution_evaluation_method\": \"hotflip_only\",\n",
        "    \"substitution_gen_method\": \"hotflip_only\",\n",
        "    \"dataset\": dataset\n",
        "}\n",
        "counterfactual = generate_closs_counterfactual(s, calculate_score, args)\n",
        "counterfactual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run CLOSS and HotFlip\n",
        "\n",
        "First run the method without optimization (`CLOSS-EO`) and without retraining the language modeling head.\n",
        "\n",
        "- `CLOSS-EO:` skip optimizing the embedding. This increases failures but lowers perplexity.\n",
        "- `CLOSS-RTL:` skip retraining the language modeling head. This has no effect on perplexity but increases the failure rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Move to the main parent directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %cd \"CLOSS\"\n",
        "# %cd ..\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>original_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>it's a charming and often affecting journey.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>unflinchingly bleak and desperate</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>allows us to hope that nolan is poised to emba...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the acting, costumes, music, cinematography an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it's slow -- very, very slow.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       original_text  original_label\n",
              "0      it's a charming and often affecting journey.                1\n",
              "1                 unflinchingly bleak and desperate                0\n",
              "2  allows us to hope that nolan is poised to emba...               1\n",
              "3  the acting, costumes, music, cinematography an...               1\n",
              "4                     it's slow -- very, very slow.                0"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Run HotFlip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 1/100: num tokens: 7\n",
            "Final eval prob pos: 0.9997661709785461\n",
            "12 12\n",
            "Old tokens           :  [CLS] it ' s a charming and often affecting journey . [SEP]\n",
            "New tokens           :  [CLS] it ' s a charming and often affecting journey . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       it's a charming and often affecting journey. \n",
            "counterfactual_text: it's a charming and often affecting journey.\n",
            "\n",
            "Processing input 2/100: num tokens: 4\n",
            "Final eval prob pos: 0.014329418540000916\n",
            "10 10\n",
            "Old tokens           :  [CLS] unflinchingly bleak and desperate [SEP]\n",
            "New tokens           :  [CLS] unfl  in  ching  ly   bleak and desperate [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       unflinchingly bleak and desperate \n",
            "counterfactual_text: unflinchingly bleak and desperate\n",
            "\n",
            "Processing input 3/100: num tokens: 19\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.9995738863945007\n",
            "23 24\n",
            "Old tokens           :  [CLS] allows us to hope that nolan is poised to embark a major career as a commercial \u001b[31myet    \u001b[0m inventive filmmaker . [SEP]\n",
            "New tokens           :  [CLS] allows us to hope that nolan is poised to embark a major career as a commercial \u001b[31mdisused\u001b[0m invent  ive   filmmaker . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.958\n",
            "\n",
            "original_text:       allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker. \n",
            "counterfactual_text: allows us to hope that nolan is poised to embark a major career as a commercial disused inventive filmmaker.\n",
            "\n",
            "Processing input 4/100: num tokens: 15\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.996027946472168\n",
            "27 28\n",
            "Old tokens           :  [CLS] the acting , costumes , music , cinematography and sound are all astounding   given the production ' s austere locales . [SEP]\n",
            "New tokens           :  [CLS] the acting , costumes , music , cinematography and sound are all astou  farlane   given the production ' s auster  e   locales   . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.964\n",
            "\n",
            "original_text:       the acting, costumes, music, cinematography and sound are all astounding given the production's austere locales. \n",
            "counterfactual_text: the acting, costumes, music, cinematography and sound are all astoufarlane given the production's austere locales.\n",
            "\n",
            "Processing input 5/100: num tokens: 6\n",
            "Final eval prob pos: 0.00204812316223979\n",
            "13 13\n",
            "Old tokens           :  [CLS] it ' s slow - - very , very slow . [SEP]\n",
            "New tokens           :  [CLS] it ' s slow - - very , very slow . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       it's slow -- very, very slow. \n",
            "counterfactual_text: it's slow -- very, very slow.\n",
            "\n",
            "Processing input 6/100: num tokens: 19\n",
            "Final eval prob pos: 0.9996365308761597\n",
            "25 25\n",
            "Old tokens           :  [CLS] although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women . [SEP]\n",
            "New tokens           :  [CLS] although laced with humor and a few fanciful   touches , the film is a refreshingly   serious look at young women . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women. \n",
            "counterfactual_text: although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women.\n",
            "\n",
            "Processing input 7/100: num tokens: 4\n",
            "Final eval prob pos: 0.003931403625756502\n",
            "8 8\n",
            "Old tokens           :  [CLS] a sometimes tedious film . [SEP]\n",
            "New tokens           :  [CLS] a sometimes tedious   film . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       a sometimes tedious film. \n",
            "counterfactual_text: a sometimes tedious film.\n",
            "\n",
            "Processing input 8/100: num tokens: 8\n",
            "Final eval prob pos: 0.005927856545895338\n",
            "15 15\n",
            "Old tokens           :  [CLS] or doing last year ' s taxes with your ex - wife . [SEP]\n",
            "New tokens           :  [CLS] or doing last year ' s taxes with your ex - wife . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       or doing last year's taxes with your ex-wife. \n",
            "counterfactual_text: or doing last year's taxes with your ex-wife.\n",
            "\n",
            "Processing input 9/100: num tokens: 17\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.9996370077133179\n",
            "24 25\n",
            "Old tokens           :  [CLS] you don ' t have to know about music to appreciate the film ' s \u001b[31measy \u001b[0mgoing blend of comedy and romance . [SEP]\n",
            "New tokens           :  [CLS] you don ' t have to know about music to appreciate the film ' s \u001b[31mleast\u001b[0mgoing   blend of comedy and romance . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.96\n",
            "\n",
            "original_text:       you don't have to know about music to appreciate the film's easygoing blend of comedy and romance. \n",
            "counterfactual_text: you don't have to know about music to appreciate the film's leastgoing blend of comedy and romance.\n",
            "\n",
            "Processing input 10/100: num tokens: 29\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.0029613752849400043\n",
            "37 40\n",
            "Old tokens           :  [CLS] in exactly 89 minutes , most of which passed as slowly as if i ' d been sitting naked on an igloo , \u001b[31mformula\u001b[0m 51 \u001b[31msank     \u001b[0m from quirky to jerky to utter \u001b[31mturkey  \u001b[0m . [SEP]\n",
            "New tokens           :  [CLS] in exactly 89 minutes , most of which passed as slowly as if i ' d been sitting naked on an igl  oo   , \u001b[31mherald \u001b[0m 51 \u001b[31mimmersion\u001b[0m from quirky   to jerky   to utter \u001b[31mheraldic\u001b[0m . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.925\n",
            "\n",
            "original_text:       in exactly 89 minutes, most of which passed as slowly as if i'd been sitting naked on an igloo, formula 51 sank from quirky to jerky to utter turkey. \n",
            "counterfactual_text: in exactly 89 minutes, most of which passed as slowly as if i'd been sitting naked on an igloo, herald 51 immersion from quirky to jerky to utter heraldic.\n",
            "\n",
            "Processing input 11/100: num tokens: 15\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.9996545314788818\n",
            "21 23\n",
            "Old tokens           :  [CLS] the mesmerizing performances of the leads keep the filmgrounded and keep the audience ri \u001b[31mvet    \u001b[0med . [SEP]\n",
            "New tokens           :  [CLS] the mesm  eri  zing   performances of the leads keep the filmuth      and keep the audience ri \u001b[31mwehrmacht\u001b[0med   . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.913\n",
            "\n",
            "original_text:       the mesmerizing performances of the leads keep the film grounded and keep the audience riveted. \n",
            "counterfactual_text: the mesmerizing performances of the leads keep the filmuth and keep the audience ri wehrmachted.\n",
            "\n",
            "Processing input 12/100: num tokens: 26\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.0010519711067900062\n",
            "37 39\n",
            "Old tokens           :  [CLS] it takes a strange kind of laziness to \u001b[31mwaste    \u001b[0m the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same movie . [SEP]\n",
            "New tokens           :  [CLS] it takes a strange kind of laerie  ss   to \u001b[31mproviding\u001b[0m the talents of robert forster , anne meara   , eugene levy , and reginald vel  jo  hn  son   all in the same movie . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.949\n",
            "\n",
            "original_text:       it takes a strange kind of laziness to waste the talents of robert forster, anne meara, eugene levy, and reginald veljohnson all in the same movie. \n",
            "counterfactual_text: it takes a strange kind of laeriess to providing the talents of robert forster, anne meara, eugene levy, and reginald veljohnson all in the same movie.\n",
            "\n",
            "Processing input 13/100: num tokens: 16\n",
            "Final eval prob pos: 0.0006940726307220757\n",
            "25 25\n",
            "Old tokens           :  [CLS] . . . the film suffers from a lack of humor ( something needed to balance out the violence ) . . . [SEP]\n",
            "New tokens           :  [CLS] . . . the film suffers from a lack of humor ( something needed to balance out the violence ) . . . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       ... the film suffers from a lack of humor (something needed to balance out the violence)... \n",
            "counterfactual_text: .. the film suffers from a lack of humor (something needed to balance out the violence)...\n",
            "\n",
            "Processing input 14/100: num tokens: 17\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.3586733639240265\n",
            "25 26\n",
            "Old tokens           :  [CLS] we root for ( clara and paul ) , even like them , though perhaps it ' s an emotion closer to \u001b[31mpity    \u001b[0m . [SEP]\n",
            "New tokens           :  [CLS] we root for ( clara and paul ) , even like them , though perhaps it ' s an emotion closer to \u001b[31mconsider\u001b[0m . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.962\n",
            "\n",
            "original_text:       we root for (clara and paul), even like them, though perhaps it's an emotion closer to pity. \n",
            "counterfactual_text: we root for (clara and paul), even like them, though perhaps it's an emotion closer to consider.\n",
            "\n",
            "Processing input 15/100: num tokens: 22\n",
            "torch.Size([29, 768]) torch.Size([29, 768]) torch.Size([29, 768]) torch.Size([29])\n",
            "Token derivs\n",
            " [CLS] \u001b[31meven\u001b[0m horror fans \u001b[31mwill\u001b[0m \u001b[33mmost\u001b[0m \u001b[31mlikely\u001b[0m \u001b[33mnot\u001b[0m find what \u001b[31mthey\u001b[0m ' \u001b[31mre\u001b[0m seeking with trouble \u001b[31mevery\u001b[0m day ;gible \u001b[32mmovie\u001b[0m \u001b[34mlacks\u001b[0m both thrill\u001b[33ms\u001b[0m \u001b[34mand\u001b[0m \u001b[35mhumor\u001b[0m \u001b[34m.\u001b[0m [SEP]\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.0008692166884429753\n",
            "27 29\n",
            "Old tokens           :  [CLS] even horror fans will most likely not find what they ' re seeking with trouble every day ; \u001b[31mthe  \u001b[0m movie \u001b[31mlacks   \u001b[0m both thrills and humor . [SEP]\n",
            "New tokens           :  [CLS] even horror fans will most likely not find what they ' re seeking with trouble every day ; \u001b[31m[CLS]\u001b[0m movie \u001b[31mfeatures\u001b[0m both thrills   and humor . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.931\n",
            "\n",
            "original_text:       even horror fans will most likely not find what they're seeking with trouble every day; the movie lacks both thrills and humor. \n",
            "counterfactual_text: even horror fans will most likely not find what they're seeking with trouble every day; [cls] movie features both thrills and humor.\n",
            "\n",
            "Processing input 16/100: num tokens: 15\n",
            "Final eval prob pos: 0.999749481678009\n",
            "26 26\n",
            "Old tokens           :  [CLS] a gorgeous , high - spirited musical from india that exquisitely blends music , dance , song , and high drama . [SEP]\n",
            "New tokens           :  [CLS] a gorgeous , high - spirited musical from india that exquisitely   blends   music , dance , song , and high drama . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       a gorgeous, high-spirited musical from india that exquisitely blends music, dance, song, and high drama. \n",
            "counterfactual_text: a gorgeous, high-spirited musical from india that exquisitely blends music, dance, song, and high drama.\n",
            "\n",
            "Processing input 17/100: num tokens: 16\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.9993094205856323\n",
            "20 21\n",
            "Old tokens           :  [CLS] the emotions are \u001b[31mraw  \u001b[0m and will strike a nerve with anyone who ' s ever had family trauma . [SEP]\n",
            "New tokens           :  [CLS] the emotions are \u001b[31mbrute\u001b[0m and will strike a nerve with anyone who ' s ever had family trauma . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.952\n",
            "\n",
            "original_text:       the emotions are raw and will strike a nerve with anyone who's ever had family trauma. \n",
            "counterfactual_text: the emotions are brute and will strike a nerve with anyone who's ever had family trauma.\n",
            "\n",
            "Processing input 18/100: num tokens: 28\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.9996262788772583\n",
            "39 44\n",
            "Old tokens           :  [CLS] audrey tatou has a knack for picking roles that magnify \u001b[31mher \u001b[0moutrageous \u001b[31mcharm   \u001b[0m , and in this \u001b[31mliter     \u001b[0mate french comedy , she ' s as morning - glory exuberant as she was in amelie . [SEP]\n",
            "New tokens           :  [CLS] audrey tatou   has a knack   for picking roles that magni  fy   \u001b[31mdish\u001b[0mwashed     \u001b[31msounding\u001b[0m , and in this \u001b[31msegregated\u001b[0mate   french comedy , she ' s as morning - glory exanto  ant   as she was in amelie . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.886\n",
            "\n",
            "original_text:       audrey tatou has a knack for picking roles that magnify her outrageous charm, and in this literate french comedy, she's as morning-glory exuberant as she was in amélie. \n",
            "counterfactual_text: audrey tatou has a knack for picking roles that magnify dishwashed sounding, and in this segregatedate french comedy, she's as morning-glory exantoant as she was in amelie.\n",
            "\n",
            "Processing input 19/100: num tokens: 9\n",
            "Final eval prob pos: 0.0008032924961298704\n",
            "14 14\n",
            "Old tokens           :  [CLS] . . . the movie is just a plain old monster . [SEP]\n",
            "New tokens           :  [CLS] . . . the movie is just a plain old monster . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       ... the movie is just a plain old monster. \n",
            "counterfactual_text: .. the movie is just a plain old monster.\n",
            "\n",
            "Processing input 20/100: num tokens: 16\n",
            "Final eval prob pos: 0.0010158075019717216\n",
            "21 21\n",
            "Old tokens           :  [CLS] in its best moments , resembles a bad high school production of grease , without benefit of song . [SEP]\n",
            "New tokens           :  [CLS] in its best moments , resembles a bad high school production of grease , without benefit of song . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       in its best moments, resembles a bad high school production of grease, without benefit of song. \n",
            "counterfactual_text: in its best moments, resembles a bad high school production of grease, without benefit of song.\n",
            "\n",
            "Processing input 21/100: num tokens: 30\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.057879358530044556\n",
            "40 41\n",
            "Old tokens           :  [CLS] pumpkin takes an admirable look at the hypocrisy of political correctness , but it does so with such an \u001b[31muneven      \u001b[0m tone that you never know when humor ends and tragedy begins . [SEP]\n",
            "New tokens           :  [CLS] pumpkin takes an admir  able   look at the hyp  oc  ris  y   of political correctness   , but it does so with such an \u001b[31msynchronized\u001b[0m tone that you never know when humor ends and tragedy begins . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.976\n",
            "\n",
            "original_text:       pumpkin takes an admirable look at the hypocrisy of political correctness, but it does so with such an uneven tone that you never know when humor ends and tragedy begins. \n",
            "counterfactual_text: pumpkin takes an admirable look at the hypocrisy of political correctness, but it does so with such an synchronized tone that you never know when humor ends and tragedy begins.\n",
            "\n",
            "Processing input 22/100: num tokens: 10\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.05384004861116409\n",
            "16 17\n",
            "Old tokens           :  [CLS] the iditarod \u001b[31mlasts\u001b[0m for days - this just felt like it did . [SEP]\n",
            "New tokens           :  [CLS] the idita  rod   \u001b[31mhappy\u001b[0m for days - this just felt like it did . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.941\n",
            "\n",
            "original_text:       the iditarod lasts for days-this just felt like it did. \n",
            "counterfactual_text: the iditarod happy for days-this just felt like it did.\n",
            "\n",
            "Processing input 23/100: num tokens: 5\n",
            "Final eval prob pos: 0.9942765831947327\n",
            "10 10\n",
            "Old tokens           :  [CLS] holden caulfield did it better . [SEP]\n",
            "New tokens           :  [CLS] holden caulf  ield   did it better . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       holden caulfield did it better. \n",
            "counterfactual_text: holden caulfield did it better.\n",
            "\n",
            "Processing input 24/100: num tokens: 14\n",
            "Final eval prob pos: 0.9997273087501526\n",
            "20 20\n",
            "Old tokens           :  [CLS] a delectable and intriguing thriller filled with surprises , read my lips is an original . [SEP]\n",
            "New tokens           :  [CLS] a delect  able   and intriguing thriller filled with surprises , read my lips is an original . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       a delectable and intriguing thriller filled with surprises, read my lips is an original. \n",
            "counterfactual_text: a delectable and intriguing thriller filled with surprises, read my lips is an original.\n",
            "\n",
            "Processing input 25/100: num tokens: 15\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.9986729621887207\n",
            "17 18\n",
            "Old tokens           :  [CLS] seldom has a movie so closelymatched the spirit of a man and his work . [SEP]\n",
            "New tokens           :  [CLS] seldom has a movie so closelyecure   the spirit of a man and his work . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.944\n",
            "\n",
            "original_text:       seldom has a movie so closely matched the spirit of a man and his work. \n",
            "counterfactual_text: seldom has a movie so closelyecure the spirit of a man and his work.\n",
            "\n",
            "Processing input 26/100: num tokens: 23\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.007230778224766254\n",
            "33 35\n",
            "Old tokens           :  [CLS] nicks , seemingly uncertain what ' s going to make people laugh , runs the gamut from \u001b[31mstale\u001b[0m parody to raunchy sex gags to \u001b[31mformula\u001b[0m romantic comedy . [SEP]\n",
            "New tokens           :  [CLS] nicks   , seemingly uncertain what ' s going to make people laugh , runs the gamut   from \u001b[31mfunny\u001b[0m parody to raun  chy   sex gags   to \u001b[31meffect \u001b[0m romantic comedy . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 0.943\n",
            "\n",
            "original_text:       nicks, seemingly uncertain what's going to make people laugh, runs the gamut from stale parody to raunchy sex gags to formula romantic comedy. \n",
            "counterfactual_text: nicks, seemingly uncertain what's going to make people laugh, runs the gamut from funny parody to raunchy sex gags to effect romantic comedy.\n",
            "\n",
            "Processing input 27/100: num tokens: 26\n"
          ]
        }
      ],
      "source": [
        "args = {\n",
        "    \"beam_width\": 15,\n",
        "    \"w\": 5,\n",
        "    \"K\": 30,\n",
        "    \"substitution_evaluation_method\": \"hotflip_only\",\n",
        "    \"substitution_gen_method\": \"hotflip_only\",\n",
        "    \"dataset\": dataset\n",
        "}\n",
        "\n",
        "df_output = get_output(df_input, generate_closs_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(f\"./output/hotflip-output-{dataset}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Run CLOSS without optimization and without retraining the language modeling head:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\n",
        "    \"beam_width\": 15,\n",
        "    \"w\": 5,\n",
        "    \"K\": 30,\n",
        "    \"substitution_evaluation_method\": \"SVs\",\n",
        "    \"substitution_gen_method\": \"no_opt_lmh\",\n",
        "    \"dataset\": dataset\n",
        "}\n",
        "\n",
        "df_output = get_output(df_input, generate_closs_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(f\"./output/closs-output-{dataset}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Polyjuice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd polyjuice\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make sure the model is being imported properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import polyjuice\n",
        "\n",
        "importlib.reload(polyjuice)\n",
        "print(polyjuice.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from polyjuice import Polyjuice\n",
        "\n",
        "pj = Polyjuice(model_path=\"uw-hai/polyjuice\", is_cuda=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"julia is played with exasperating blandness by laura regan .\"\n",
        "perturbations = pj.perturb(\n",
        "    orig_sent=text,\n",
        "    ctrl_code=\"negation\",\n",
        "    num_perturbations=5,\n",
        "    # perplex_thred=None\n",
        ")\n",
        "perturbations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the model and get the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output = get_output(df_input, generate_polyjuice_counterfactual, {})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ..\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(f\"./output/polyjuice-output-{dataset}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FIZLE\n",
        "\n",
        "Two variants:\n",
        "* Naive: uses a single prompt.\n",
        "* Guided: Uses two prompts. The first prompt identifies important words and the second prompt generates the counterfactual.\n",
        "\n",
        "Hyperparameters:\n",
        "\n",
        "For all LLMs, we use top_p sampling with p = 1, temperature t = 0.4 and a repetition penalty of 1.1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. FIZLE naive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\"model\": \"gpt-4-turbo\"}\n",
        "df_output = get_output(df_input, generate_naive_fizle_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(\"./output/fizlenaive-output.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FIZLE guided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\"model\": \"gpt-4-turbo\"}\n",
        "df_output = get_output(df_input, generate_naive_fizle_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(\"./output/fizleguided-output.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
