{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup\n",
        "\n",
        "Mount Google Drive and clone the repository containing the methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUgYxHsWZU8y",
        "outputId": "4254c890-6bd2-4194-c3cc-c99744f2d96c"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "\n",
        "github_username = input(\"Enter your GitHub username: \")\n",
        "github_token = getpass.getpass(\"Enter your GitHub personal access token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPOKvD_RafHj",
        "outputId": "96ffda6e-c3e6-466a-d559-0a2b44931081"
      },
      "outputs": [],
      "source": [
        "repo_name = \"smcaleese/masters-thesis-code\"\n",
        "!git clone https://{github_username}:{github_token}@github.com/{repo_name}.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "EDPv3DJ8cER4",
        "outputId": "04fbecf9-3acc-4856-de56-f2c2d36c60a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'masters-thesis-code'\n",
            "/Users/smcaleese/Documents/masters-thesis-code\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
            "  bkms = self.shell.db.get('bookmarks', {})\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd masters-thesis-code\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers datasets textdistance openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download datasets\n",
        "\n",
        "Download the SST-2, QNLI and AG News datasets, clean the sentences, and create a list of input sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "sst = load_dataset(\"stanfordnlp/sst2\")\n",
        "qnli = load_dataset(\"glue\", \"qnli\")\n",
        "ag_news = load_dataset(\"fancyzhx/ag_news\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 67349\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 1821\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'sentence', 'label', 'idx'],\n",
              "        num_rows: 104743\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'sentence', 'label', 'idx'],\n",
              "        num_rows: 5463\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'sentence', 'label', 'idx'],\n",
              "        num_rows: 5463\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qnli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ag_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_samples = 100\n",
        "\n",
        "sst_sentences = sst[\"validation\"][\"sentence\"]\n",
        "sst_labels = sst[\"validation\"][\"label\"]\n",
        "\n",
        "sst_sentences_subset = sst_sentences[:num_samples]\n",
        "sst_labels_subset = sst_labels[:num_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "qnli_questions = qnli[\"validation\"][\"question\"]\n",
        "qnli_answers = qnli[\"validation\"][\"sentence\"]\n",
        "qnli_labels = qnli[\"validation\"][\"label\"]\n",
        "\n",
        "qnli_questions_subset = qnli_questions[:num_samples]\n",
        "qnli_answers_subset = qnli_answers[:num_samples]\n",
        "qnli_labels_subset = qnli_labels[:num_samples]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Format the text in the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def format_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # remove two spaces around a comma:\n",
        "    sentence = re.sub(r\"\\s(')\\s(ve|re|s|t|ll|d)\", r\"\\1\\2\", sentence)\n",
        "\n",
        "    # remove spaces around hyphens:\n",
        "    sentence = re.sub(r\"-\\s-\", \"--\", sentence)\n",
        "    sentence = re.sub(r\"(\\w)\\s-\\s(\\w)\", r\"\\1-\\2\", sentence)\n",
        "\n",
        "    def replace(match):\n",
        "        return match.group(1)\n",
        "\n",
        "    # remove spaces before punctuation and n't:\n",
        "    sentence = re.sub(r\"\\s([.!,?:;')]|n't)\", replace, sentence)\n",
        "\n",
        "    # remove spaces after opening parenthesis:\n",
        "    sentence = re.sub(r\"([(])\\s\", replace, sentence)\n",
        "    \n",
        "    return sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "sst_sentences_subset_formatted = [format_sentence(sentence) for sentence in sst_sentences_subset]\n",
        "\n",
        "qnli_questions_subset_formatted = [format_sentence(sentence) for sentence in qnli_questions_subset]\n",
        "qnli_answers_subset_formatted = [format_sentence(sentence) for sentence in qnli_answers_subset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write the sentences to a file named `sst-input.csv` and `qnli-input.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_sst = pd.DataFrame({\n",
        "    \"original_text\": sst_sentences_subset_formatted,\n",
        "    \"original_label\": sst_labels_subset\n",
        "})\n",
        "df_qnli = pd.DataFrame({\n",
        "    \"original_question\": qnli_questions_subset_formatted,\n",
        "    \"original_answer\": qnli_answers_subset_formatted,\n",
        "    \"original_label\": qnli_labels_subset\n",
        "})\n",
        "\n",
        "df_sst.to_csv(\"./input/sst-input.csv\", index=False)\n",
        "df_qnli.to_csv(\"./input/qnli-input.csv\", index=False)\n",
        "\n",
        "# df_ag_news = pd.DataFrame(random_ag_news_sentences_subset_formatted, columns=[\"original_text\"])\n",
        "# df_ag_news.to_csv(\"./input/ag-news-input.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choose dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = \"sst_2\"\n",
        "# dataset = \"qnli\"\n",
        "# dataset = \"ag_news\"\n",
        "\n",
        "if dataset == \"sst_2\":\n",
        "    input_file = \"sst-input\"\n",
        "    model_name = \"textattack/bert-base-uncased-SST-2\"\n",
        "    fizle_task = \"sentiment analysis on the SST-2 dataset\"\n",
        "elif dataset == \"qnli\":\n",
        "    input_file = \"qnli-input\"\n",
        "    model_name = \"textattack/bert-base-uncased-QNLI\"\n",
        "    fizle_task = \"natural language inference on the QNLI dataset\"\n",
        "elif dataset == \"ag_news\":\n",
        "    input_file = \"ag-news-input\"\n",
        "    model_name =  \"textattack/bert-base-uncased-ag-news\"\n",
        "    fizle_task = \"topic classification on the AG News dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create input dataframe\n",
        "\n",
        "Columns to add to create output dataframe:\n",
        "- original_score\n",
        "- original_perplexity\n",
        "- counterfactual_text\n",
        "- counterfactual_score\n",
        "- counterfactual_perplexity\n",
        "- found_flip\n",
        "- frac_tokens_same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>original_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>it's a charming and often affecting journey.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>unflinchingly bleak and desperate</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>allows us to hope that nolan is poised to emba...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the acting, costumes, music, cinematography an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it's slow -- very, very slow.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       original_text  original_label\n",
              "0      it's a charming and often affecting journey.                1\n",
              "1                 unflinchingly bleak and desperate                0\n",
              "2  allows us to hope that nolan is poised to emba...               1\n",
              "3  the acting, costumes, music, cinematography an...               1\n",
              "4                     it's slow -- very, very slow.                0"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_input = pd.read_csv(f\"input/{input_file}.csv\")\n",
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 2)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the sentiment model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "if dataset == \"sst_2\":\n",
        "    id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "    label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    ).to(device)\n",
        "\n",
        "elif dataset == \"qnli\":\n",
        "    id2label = {0: \"entailment\", 1: \"not_entailment\"}\n",
        "    label2id = {\"entailment\": 0, \"not_entailment\": 1}\n",
        "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    ).to(device)\n",
        "\n",
        "# elif dataset == \"ag_news\":\n",
        "#     id2label = {\n",
        "#         0: \"World\",\n",
        "#         1: \"Sports\",\n",
        "#         2: \"Business\",\n",
        "#         3: \"Sci/Tech\"\n",
        "#     }\n",
        "#     label2id = {\n",
        "#         \"World\": 0,\n",
        "#         \"Sports\": 1,\n",
        "#         \"Business\": 2,\n",
        "#         \"Sci/Tech\": 3\n",
        "#     }\n",
        "#     sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#         model_name,\n",
        "#         num_labels=4,\n",
        "#         id2label=id2label,\n",
        "#         label2id=label2id\n",
        "#     ).to(device)\n",
        "\n",
        "sentiment_model_tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the GPT-2 model for calculating perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the language model for CLOSS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "# TODO: try using a larger model to improve performance: https://arxiv.org/pdf/2111.09543\n",
        "LM_model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "LM_model.lm_head = LM_model.cls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import textdistance\n",
        "\n",
        "# def probability_positive(tokenizer, sentiment_model, text, device):\n",
        "#     if type(text) == torch.Tensor:\n",
        "#         ids = text.view(1, -1).to(device)\n",
        "#     elif type(text) == list:\n",
        "#         if type(text[0]) == int:\n",
        "#             ids = torch.tensor(text).view(1, -1).to(device)\n",
        "#         else:\n",
        "#             ids = torch.tensor(tokenizer.convert_tokens_to_ids(text)).view(1, -1).to(device)\n",
        "#     else:\n",
        "#         ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True, truncation=True)).view(1, -1).to(device)\n",
        "    \n",
        "#     if isBert(sentiment_model):\n",
        "#         outputs = sentiment_model(ids.to(device), token_type_ids=get_token_type_ids(ids).to(device))\n",
        "#     elif isRoberta(sentiment_model):\n",
        "#         outputs = sentiment_model(ids.to(device))\n",
        "#     loss = outputs.loss\n",
        "#     logits = outputs.logits\n",
        "        \n",
        "#     prob_positive = torch.nn.functional.softmax(logits, dim=1)[0][1].item()\n",
        "#     return prob_positive\n",
        "\n",
        "def calculate_score(text, sentiment_model_tokenizer, dataset, device):\n",
        "    def tokenize_with_correct_token_type_ids(input_text, tokenizer):\n",
        "        # Tokenize the input\n",
        "        tokens = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "        \n",
        "        # Get the position of the first [SEP] token\n",
        "        sep_pos = (tokens.input_ids == tokenizer.sep_token_id).nonzero()[0, 1].item()\n",
        "        \n",
        "        # Create token_type_ids\n",
        "        token_type_ids = torch.zeros_like(tokens.input_ids)\n",
        "        token_type_ids[0, sep_pos+1:] = 1  # Set to 1 after the first [SEP] token\n",
        "        \n",
        "        # Update the tokens dictionary\n",
        "        tokens['token_type_ids'] = token_type_ids\n",
        "        \n",
        "        return tokens\n",
        "\n",
        "    if type(text) == list:\n",
        "        if type(text[0]) == str:\n",
        "            tokens = text\n",
        "            ids = sentiment_model_tokenizer.convert_tokens_to_ids(tokens)\n",
        "            text = sentiment_model_tokenizer.decode(ids[1:-1])\n",
        "        elif type(text[0]) == int:\n",
        "            ids = text\n",
        "            text = sentiment_model_tokenizer.decode(ids[1:-1])\n",
        "\n",
        "    if dataset == \"sst_2\":\n",
        "        inputs = sentiment_model_tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    elif dataset == \"qnli\":\n",
        "        inputs = tokenize_with_correct_token_type_ids(text, sentiment_model_tokenizer).to(device)\n",
        "\n",
        "    logits = sentiment_model(**inputs).logits\n",
        "    prob_positive = torch.nn.functional.softmax(logits, dim=1)[0][1].item()\n",
        "    return prob_positive\n",
        "\n",
        "def calculate_perplexity(text):\n",
        "    inputs = gpt2_tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    loss = gpt2_model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
        "    perplexity = torch.exp(loss).item()\n",
        "    return perplexity\n",
        "\n",
        "def is_flip(original_score, counterfactual_score):\n",
        "    # might need to be updated for AG News\n",
        "    positive_to_negative = original_score >= 0.5 and counterfactual_score < 0.5\n",
        "    negative_to_positive = original_score < 0.5 and counterfactual_score >= 0.5\n",
        "    return positive_to_negative or negative_to_positive\n",
        "\n",
        "def truncate_text(text, max_length=100):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) > max_length:\n",
        "        text = \" \".join(tokens[:max_length])\n",
        "    return text\n",
        "\n",
        "def get_all_embeddings(model, tokenizer):\n",
        "    all_word_embeddings = torch.zeros((tokenizer.vocab_size, 768)).detach().to(device)\n",
        "    for i in range(tokenizer.vocab_size):\n",
        "        input_tensor = torch.tensor(i).view(1, 1).to(device)\n",
        "        word_embedding = model.bert.embeddings.word_embeddings(input_tensor)\n",
        "        all_word_embeddings[i, :] = word_embedding\n",
        "    all_word_embeddings = all_word_embeddings.detach().requires_grad_(False)\n",
        "    return all_word_embeddings\n",
        "\n",
        "def get_levenshtein_similarity_score(original_text, counterfactual_text):\n",
        "    score = 1 - textdistance.levenshtein.normalized_distance(original_text, counterfactual_text)\n",
        "    return score\n",
        "\n",
        "def get_output(df_input, counterfactual_method, args):\n",
        "    df_input = df_input.copy()\n",
        "    output_data = {\n",
        "        \"original_text\": [],\n",
        "        \"original_score\": [],\n",
        "        \"original_perplexity\": [],\n",
        "        \"counterfactual_text\": [],\n",
        "        \"counterfactual_score\": [],\n",
        "        \"counterfactual_perplexity\": [],\n",
        "        \"found_flip\": [],\n",
        "        \"levenshtein_similarity_score\": []\n",
        "    }\n",
        "    if dataset == \"qnli\":\n",
        "        output_data[\"original_question\"] = []\n",
        "\n",
        "    for i in range(len(df_input)):\n",
        "        if dataset == \"sst_2\":\n",
        "            original_text = df_input.iloc[i][\"original_text\"]\n",
        "            original_text = truncate_text(original_text)\n",
        "            print(f\"Processing input {i + 1}/{len(df_input)}: num tokens: {len(original_text.split())}\")\n",
        "\n",
        "            original_score = calculate_score(original_text, sentiment_model_tokenizer, dataset, device)\n",
        "            original_perplexity = calculate_perplexity(original_text)\n",
        "\n",
        "            args = {**args, \"original_score\": original_score}\n",
        "            counterfactual_text = counterfactual_method(original_text, calculate_score, args)\n",
        "            counterfactual_text = format_sentence(counterfactual_text)\n",
        "\n",
        "            label_width = 20\n",
        "            print(f\"\\n{'original_text:'.ljust(label_width)} {original_text}\")\n",
        "            print(f\"{'counterfactual_text:'.ljust(label_width)} {counterfactual_text}\\n\")\n",
        "\n",
        "            counterfactual_score = calculate_score(counterfactual_text, sentiment_model_tokenizer, dataset, device)\n",
        "            counterfactual_perplexity = calculate_perplexity(counterfactual_text)\n",
        "            found_flip = is_flip(original_score, counterfactual_score)\n",
        "            levenshtein_similarity_score = get_levenshtein_similarity_score(original_text, counterfactual_text)\n",
        "\n",
        "            output_data[\"original_text\"].append(original_text)\n",
        "            output_data[\"original_score\"].append(original_score)\n",
        "            output_data[\"original_perplexity\"].append(original_perplexity)\n",
        "            output_data[\"counterfactual_text\"].append(counterfactual_text)\n",
        "            output_data[\"counterfactual_score\"].append(counterfactual_score)\n",
        "            output_data[\"counterfactual_perplexity\"].append(counterfactual_perplexity)\n",
        "            output_data[\"found_flip\"].append(found_flip)\n",
        "            output_data[\"levenshtein_similarity_score\"].append(levenshtein_similarity_score)\n",
        "\n",
        "        elif dataset == \"qnli\":\n",
        "            row = df_input.iloc[i]\n",
        "            original_question, original_answer = row[\"original_question\"], row[\"original_answer\"]\n",
        "            original_input = f\"{original_question} [SEP] {original_answer}\"\n",
        "\n",
        "            print(f\"Processing input {i + 1}/{len(df_input)}: num tokens: {len(f\"{original_question} {original_answer}\".split())}\")\n",
        "\n",
        "            original_score = calculate_score(original_input, sentiment_model_tokenizer, dataset, device)\n",
        "            original_perplexity = calculate_perplexity(original_answer)\n",
        "\n",
        "            args = {**args, \"original_score\": original_score}\n",
        "            #Â TODO for QNLI:\n",
        "\n",
        "            counterfactual_answer = counterfactual_method(original_input, calculate_score, calculate_score, args)\n",
        "            counterfactual_answer = format_sentence(counterfactual_answer)\n",
        "            counterfactual_input = (original_question, counterfactual_answer)\n",
        "\n",
        "            label_width = 20\n",
        "            print(f\"\\n{'original_answer:'.ljust(label_width)} {original_answer}\")\n",
        "            print(f\"{'counterfactual_answer:'.ljust(label_width)} {counterfactual_answer}\\n\")\n",
        "\n",
        "            counterfactual_score = calculate_score(counterfactual_input, sentiment_model_tokenizer, dataset, device)\n",
        "            counterfactual_perplexity = calculate_perplexity(counterfactual_answer)\n",
        "            found_flip = is_flip(original_score, counterfactual_score)\n",
        "            levenshtein_similarity_score = get_levenshtein_similarity_score(original_answer, counterfactual_answer)\n",
        "\n",
        "            output_data[\"original_question\"].append(original_question)\n",
        "            output_data[\"original_text\"].append(counterfactual_answer)\n",
        "            output_data[\"original_score\"].append(original_score)\n",
        "            output_data[\"original_perplexity\"].append(original_perplexity)\n",
        "            output_data[\"counterfactual_text\"].append(counterfactual_answer)\n",
        "            output_data[\"counterfactual_score\"].append(counterfactual_score)\n",
        "            output_data[\"counterfactual_perplexity\"].append(counterfactual_perplexity)\n",
        "            output_data[\"found_flip\"].append(found_flip)\n",
        "            output_data[\"levenshtein_similarity_score\"].append(levenshtein_similarity_score)\n",
        "\n",
        "    df_output = pd.DataFrame(output_data)\n",
        "    return df_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_word_embeddings = get_all_embeddings(sentiment_model, sentiment_model_tokenizer).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39muserdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get(\"API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i: 0\n",
            "i: 1\n",
            "i: 2\n",
            "i: 3\n",
            "i: 4\n",
            "i: 5\n",
            "i: 6\n",
            "i: 7\n",
            "i: 8\n",
            "i: 9\n",
            "i: 10\n",
            "i: 11\n",
            "i: 12\n",
            "i: 13\n",
            "i: 14\n",
            "i: 15\n",
            "i: 16\n",
            "i: 17\n",
            "i: 18\n",
            "i: 19\n",
            "i: 20\n",
            "i: 21\n",
            "i: 22\n",
            "i: 23\n",
            "i: 24\n",
            "i: 25\n",
            "i: 26\n",
            "i: 27\n",
            "i: 28\n",
            "i: 29\n",
            "i: 30\n",
            "i: 31\n",
            "i: 32\n",
            "i: 33\n",
            "i: 34\n",
            "i: 35\n",
            "i: 36\n",
            "i: 37\n",
            "i: 38\n",
            "i: 39\n",
            "i: 40\n",
            "i: 41\n",
            "i: 42\n",
            "i: 43\n",
            "i: 44\n",
            "i: 45\n",
            "i: 46\n",
            "i: 47\n",
            "i: 48\n",
            "i: 49\n",
            "i: 50\n",
            "i: 51\n",
            "i: 52\n",
            "i: 53\n",
            "i: 54\n",
            "i: 55\n",
            "i: 56\n",
            "i: 57\n",
            "i: 58\n",
            "i: 59\n",
            "i: 60\n",
            "i: 61\n",
            "i: 62\n",
            "i: 63\n",
            "i: 64\n",
            "i: 65\n",
            "i: 66\n",
            "i: 67\n",
            "i: 68\n",
            "i: 69\n",
            "i: 70\n",
            "i: 71\n",
            "i: 72\n",
            "i: 73\n",
            "i: 74\n",
            "i: 75\n",
            "i: 76\n",
            "i: 77\n",
            "i: 78\n",
            "i: 79\n",
            "i: 80\n",
            "i: 81\n",
            "i: 82\n",
            "i: 83\n",
            "i: 84\n",
            "i: 85\n",
            "i: 86\n",
            "i: 87\n",
            "i: 88\n",
            "i: 89\n",
            "i: 90\n",
            "i: 91\n",
            "i: 92\n",
            "i: 93\n",
            "i: 94\n",
            "i: 95\n",
            "i: 96\n",
            "i: 97\n",
            "i: 98\n",
            "i: 99\n",
            "accuracy: 0.92\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "\n",
        "for i in range(len(df_input)):\n",
        "    print(f\"i: {i}\")\n",
        "    row = df_input.iloc[i]\n",
        "\n",
        "    if dataset == \"sst_2\":\n",
        "        original_text, original_label = row[\"original_text\"], row[\"original_label\"]\n",
        "    elif dataset == \"qnli\":\n",
        "        original_question, original_answer, original_label = row[\"original_question\"], row[\"original_answer\"], row[\"original_label\"]\n",
        "        original_text = f\"{original_question} [SEP] {original_answer}\"\n",
        "\n",
        "    score = calculate_score(original_text, sentiment_model_tokenizer, dataset, device)\n",
        "    y_hat = 1 if score >= 0.5 else 0\n",
        "    if y_hat == original_label:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(df_input)\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Counterfactual generator functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %cd \"CLOSS\"\n",
        "# %cd ..\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from CLOSS.closs import generate_counterfactual\n",
        "import re\n",
        "\n",
        "def generate_polyjuice_counterfactual(original_text, calculate_score, args):\n",
        "    perturbations = pj.perturb(\n",
        "        orig_sent=original_text,\n",
        "        ctrl_code=\"negation\",\n",
        "        num_perturbations=1,\n",
        "        perplex_thred=None\n",
        "    )\n",
        "    counterfactual_text = perturbations[0]\n",
        "    return counterfactual_text\n",
        "\n",
        "def generate_closs_counterfactual(original_text, calculate_score, args):\n",
        "    # TODO: move target label from inside CLOSS to here for AG News dataset\n",
        "    counterfactual_text = generate_counterfactual(\n",
        "        original_text,\n",
        "        sentiment_model,\n",
        "        LM_model,\n",
        "        calculate_score,\n",
        "        sentiment_model_tokenizer,\n",
        "        all_word_embeddings,\n",
        "        device,\n",
        "        args\n",
        "    )\n",
        "    return counterfactual_text\n",
        "\n",
        "def call_openai_api(system_prompt, model):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt}\n",
        "        ],\n",
        "        top_p=1,\n",
        "        temperature=0.4,\n",
        "        frequency_penalty=1.1\n",
        "    )\n",
        "    output = completion.choices[0].message.content\n",
        "    return output\n",
        "\n",
        "def generate_naive_fizle_counterfactual(original_text, calculate_score, args):\n",
        "    original_score, model = args[\"original_score\"], args[\"model\"]\n",
        "    original_label = 1 if original_score >= 0.5 else 0\n",
        "    cf_label = 0 if original_label == 1 else 1\n",
        "\n",
        "    system_prompt = f\"\"\"In the task of {fizle_task}, a trained black-box classifier correctly predicted the label '{original_label}' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from '{original_label}' to '{cf_label}'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
        "    -\n",
        "    Text: {original_text}\"\"\"\n",
        "\n",
        "    correct_output_format = False\n",
        "    for i in range(10):\n",
        "        print(f\"attempt: {i + 1}\")\n",
        "        output = call_openai_api(system_prompt, model)\n",
        "        counterfactual_text = re.search(\"<new>(.*?)</new>\", output).group(1)\n",
        "        if counterfactual_text:\n",
        "            correct_output_format = True\n",
        "            break\n",
        "\n",
        "    if not correct_output_format:\n",
        "        print(\"Failed to generate counterfactual surrounded by <new> tags\")\n",
        "        counterfactual_text = output[5:-6]\n",
        "\n",
        "    return counterfactual_text\n",
        "\n",
        "def generate_guided_fizle_counterfactual(original_text, calculate_score, args):\n",
        "    original_score, model = args[\"original_score\"], args[\"model\"]\n",
        "    original_label = 1 if original_score >= 0.5 else 0\n",
        "    cf_label = 0 if original_label == 1 else 1\n",
        "    system_prompt = \"\"\n",
        "\n",
        "    # 1. Find important words\n",
        "    step1_system_prompt = \" \".join([\n",
        "        f\"In the task of {fizle_task}, a trained black-box classifier correctly predicted the label '{original_label}' for the following text.\",\n",
        "        f\"Explain why the model predicted the '{original_label}' label by identifying the words in the input that caused the label. List ONLY the words as a comma separated list.\",\n",
        "        f\"\\n-\\nText: {original_text}\",\n",
        "        f\"\\nImportant words identified: \"\n",
        "    ])\n",
        "    system_prompt += step1_system_prompt\n",
        "    important_words = call_openai_api(step1_system_prompt, model)\n",
        "    system_prompt += important_words + \"\\n\"\n",
        "\n",
        "    # 2. Generate the final counterfactual\n",
        "    correct_output_format = False\n",
        "    for i in range(10):\n",
        "        step2_system_prompt = \" \".join([\n",
        "            f\"Generate a counterfactual explanation for the original text by ONLY changing a minimal set of the words you identified, so that the label changes from '{original_label}' to '{cf_label}'.\",\n",
        "            f\"Use the following definition of 'counterfactual explanation': 'A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.'\",\n",
        "            f\"Enclose the generated text within <new> tags.\"\n",
        "        ])\n",
        "        final_system_prompt = system_prompt + step2_system_prompt\n",
        "        print(f\"final_system_prompt: {final_system_prompt}\")\n",
        "        step2_output = call_openai_api(final_system_prompt, model)\n",
        "        counterfactual_text = re.search(\"<new>(.*?)</new>\", step2_output).group(1)\n",
        "        if counterfactual_text:\n",
        "            correct_output_format = True\n",
        "            break\n",
        "\n",
        "    if not correct_output_format:\n",
        "        print(\"Failed to generate counterfactual surrounded by <new> tags\")\n",
        "        counterfactual_text = step2_output[5:-6]\n",
        "\n",
        "    return counterfactual_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final eval prob pos: 0.9994229078292847\n",
            "8 8\n",
            "Old tokens           :  [CLS] i really loved the movie . [SEP]\n",
            "New tokens           :  [CLS] i really loved the movie . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'i really loved the movie.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# s = \"What came into force after the new constitution was herald? [SEP] As of that day, the new constitution heralding the Second Republic came into force.\"\n",
        "# s = \"I really hated the movie.\"\n",
        "s = \"I really loved the movie.\"\n",
        "\n",
        "args = {\n",
        "    \"beam_width\": 15,\n",
        "    \"w\": 5,\n",
        "    \"K\": 30,\n",
        "    \"substitution_evaluation_method\": \"hotflip_only\",\n",
        "    \"substitution_gen_method\": \"hotflip_only\",\n",
        "    \"dataset\": dataset\n",
        "}\n",
        "counterfactual = generate_closs_counterfactual(s, calculate_score, args)\n",
        "counterfactual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run CLOSS and HotFlip\n",
        "\n",
        "First run the method without optimization (`CLOSS-EO`) and without retraining the language modeling head.\n",
        "\n",
        "- `CLOSS-EO:` skip optimizing the embedding. This increases failures but lowers perplexity.\n",
        "- `CLOSS-RTL:` skip retraining the language modeling head. This has no effect on perplexity but increases the failure rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Move to the main parent directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %cd \"CLOSS\"\n",
        "# %cd ..\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Run HotFlip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\n",
        "    \"beam_width\": 15,\n",
        "    \"w\": 5,\n",
        "    \"K\": 30,\n",
        "    \"substitution_evaluation_method\": \"hotflip_only\",\n",
        "    \"substitution_gen_method\": \"hotflip_only\",\n",
        "    \"dataset\": dataset\n",
        "}\n",
        "\n",
        "df_output = get_output(df_input, generate_closs_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(f\"./output/hotflip-output-{dataset}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Run CLOSS without optimization and without retraining the language modeling head:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\n",
        "    \"beam_width\": 15,\n",
        "    \"w\": 5,\n",
        "    \"K\": 30,\n",
        "    \"substitution_evaluation_method\": \"SVs\",\n",
        "    \"substitution_gen_method\": \"no_opt_lmh\",\n",
        "    \"dataset\": dataset\n",
        "}\n",
        "\n",
        "df_output = get_output(df_input, generate_closs_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(f\"./output/closs-output-{dataset}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Polyjuice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd polyjuice\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make sure the model is being imported properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import polyjuice\n",
        "\n",
        "importlib.reload(polyjuice)\n",
        "print(polyjuice.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from polyjuice import Polyjuice\n",
        "\n",
        "pj = Polyjuice(model_path=\"uw-hai/polyjuice\", is_cuda=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"julia is played with exasperating blandness by laura regan .\"\n",
        "perturbations = pj.perturb(\n",
        "    orig_sent=text,\n",
        "    ctrl_code=\"negation\",\n",
        "    num_perturbations=5,\n",
        "    # perplex_thred=None\n",
        ")\n",
        "perturbations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the model and get the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output = get_output(df_input, generate_polyjuice_counterfactual, {})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ..\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(f\"./output/polyjuice-output-{dataset}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FIZLE\n",
        "\n",
        "Two variants:\n",
        "* Naive: uses a single prompt.\n",
        "* Guided: Uses two prompts. The first prompt identifies important words and the second prompt generates the counterfactual.\n",
        "\n",
        "Hyperparameters:\n",
        "\n",
        "For all LLMs, we use top_p sampling with p = 1, temperature t = 0.4 and a repetition penalty of 1.1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. FIZLE naive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\"model\": \"gpt-4-turbo\"}\n",
        "df_output = get_output(df_input, generate_naive_fizle_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(\"./output/fizlenaive-output.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FIZLE guided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\"model\": \"gpt-4-turbo\"}\n",
        "df_output = get_output(df_input, generate_naive_fizle_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(\"./output/fizleguided-output.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
