{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If using Google Colab (recommended), run these setup cells to download the rest of the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPOKvD_RafHj",
        "outputId": "96ffda6e-c3e6-466a-d559-0a2b44931081"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/smcaleese/text-counterfactual-explanation-methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "EDPv3DJ8cER4",
        "outputId": "04fbecf9-3acc-4856-de56-f2c2d36c60a8"
      },
      "outputs": [],
      "source": [
        "%cd text-counterfactual-explanation-methods\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers datasets textdistance openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download datasets\n",
        "\n",
        "Download the SST-2 and QNLI datasets, clean the sentences, and create a list of input sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select a dataset and the number of samples to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_samples = 1000\n",
        "\n",
        "dataset = \"sst_2\"\n",
        "# dataset = \"qnli\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def format_sentence(sentence, dataset):\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # remove two spaces around a comma:\n",
        "    sentence = re.sub(r\"\\s(')\\s(ve|re|s|t|ll|d)\", r\"\\1\\2\", sentence)\n",
        "\n",
        "    # remove spaces around hyphens:\n",
        "    sentence = re.sub(r\"-\\s-\", \"--\", sentence)\n",
        "    sentence = re.sub(r\"(\\w)\\s-\\s(\\w)\", r\"\\1-\\2\", sentence)\n",
        "\n",
        "    def replace(match):\n",
        "        return match.group(1)\n",
        "\n",
        "    # remove spaces before punctuation and n't:\n",
        "    sentence = re.sub(r\"\\s([.!,?:;')]|n't)\", replace, sentence)\n",
        "\n",
        "    # remove spaces after opening parenthesis:\n",
        "    sentence = re.sub(r\"([(])\\s\", replace, sentence)\n",
        "\n",
        "    if dataset == \"qnli\":\n",
        "        sentence = re.sub(r\"\\s(\\[sep\\])\\s\", \" [SEP] \", sentence)\n",
        "    \n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "if dataset == \"sst_2\":\n",
        "    sst = load_dataset(\"stanfordnlp/sst2\")\n",
        "\n",
        "    sst_sentences = sst[\"train\"][\"sentence\"]\n",
        "    sst_labels = sst[\"train\"][\"label\"]\n",
        "\n",
        "    sst_sentences_subset = sst_sentences[:num_samples]\n",
        "    sst_labels_subset = sst_labels[:num_samples]\n",
        "\n",
        "    sst_sentences_subset_formatted = [format_sentence(sentence, dataset) for sentence in sst_sentences_subset]\n",
        "\n",
        "elif dataset == \"qnli\":\n",
        "    qnli = load_dataset(\"glue\", \"qnli\")\n",
        "\n",
        "    qnli_questions = qnli[\"train\"][\"question\"]\n",
        "    qnli_answers = qnli[\"train\"][\"sentence\"]\n",
        "    qnli_labels = qnli[\"train\"][\"label\"]\n",
        "\n",
        "    qnli_questions_subset = qnli_questions[:num_samples]\n",
        "    qnli_answers_subset = qnli_answers[:num_samples]\n",
        "    qnli_labels_subset = qnli_labels[:num_samples]\n",
        "\n",
        "    qnli_questions_subset_formatted = [format_sentence(sentence, dataset) for sentence in qnli_questions_subset]\n",
        "    qnli_answers_subset_formatted = [format_sentence(sentence, dataset) for sentence in qnli_answers_subset]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write the sentences to a file named `sst-input.csv` or `qnli-input.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "if dataset == \"sst_2\":\n",
        "    df_sst = pd.DataFrame({\n",
        "        \"original_text\": sst_sentences_subset_formatted,\n",
        "        \"original_label\": sst_labels_subset\n",
        "    })\n",
        "    df_sst.to_csv(f\"./input/{dataset}-input.csv\", index=False)\n",
        "\n",
        "elif dataset == \"qnli\":\n",
        "    df_qnli = pd.DataFrame({\n",
        "        \"original_question\": qnli_questions_subset_formatted,\n",
        "        \"original_answer\": qnli_answers_subset_formatted,\n",
        "        \"original_label\": qnli_labels_subset\n",
        "    })\n",
        "    df_qnli.to_csv(f\"./input/{dataset}-input.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choose dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dataset == \"sst_2\":\n",
        "    input_file = f\"{dataset}-input\"\n",
        "    model_name = \"textattack/bert-base-uncased-SST-2\"\n",
        "    fizle_task = \"sentiment analysis on the SST-2 dataset\"\n",
        "elif dataset == \"qnli\":\n",
        "    input_file = f\"{dataset}-input\"\n",
        "    model_name = \"textattack/bert-base-uncased-QNLI\"\n",
        "    fizle_task = \"natural language inference on the QNLI dataset\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create input dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_input = pd.read_csv(f\"input/{input_file}.csv\")\n",
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the sentiment model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "if dataset == \"sst_2\":\n",
        "    id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "    label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    ).to(device)\n",
        "\n",
        "elif dataset == \"qnli\":\n",
        "    id2label = {0: \"entailment\", 1: \"not_entailment\"}\n",
        "    label2id = {\"entailment\": 0, \"not_entailment\": 1}\n",
        "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    ).to(device)\n",
        "\n",
        "sentiment_model_tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the GPT-2 model for calculating perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the language model for CLOSS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "LM_model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "LM_model.lm_head = LM_model.cls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import textdistance\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "\n",
        "def calculate_score(text, sentiment_model_tokenizer, dataset, device):\n",
        "    def tokenize_with_correct_token_type_ids(input_text, tokenizer):\n",
        "        # Tokenize the input\n",
        "        tokens = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Get the position of the first [SEP] token\n",
        "        sep_pos = (tokens.input_ids == tokenizer.sep_token_id).nonzero()[0, 1].item()\n",
        "\n",
        "        # Create token_type_ids\n",
        "        token_type_ids = torch.zeros_like(tokens.input_ids)\n",
        "        token_type_ids[0, sep_pos+1:] = 1  # Set to 1 after the first [SEP] token\n",
        "\n",
        "        # Update the tokens dictionary\n",
        "        tokens['token_type_ids'] = token_type_ids\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    if type(text) == list:\n",
        "        if type(text[0]) == str:\n",
        "            tokens = text\n",
        "            ids = sentiment_model_tokenizer.convert_tokens_to_ids(tokens)\n",
        "            text = sentiment_model_tokenizer.decode(ids[1:-1])\n",
        "        elif type(text[0]) == int:\n",
        "            ids = text\n",
        "            text = sentiment_model_tokenizer.decode(ids[1:-1])\n",
        "\n",
        "    if dataset == \"sst_2\":\n",
        "        inputs = sentiment_model_tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    elif dataset == \"qnli\":\n",
        "        inputs = tokenize_with_correct_token_type_ids(text, sentiment_model_tokenizer).to(device)\n",
        "\n",
        "    logits = sentiment_model(**inputs).logits\n",
        "    prob_positive = torch.nn.functional.softmax(logits, dim=1)[0][1].item()\n",
        "    return prob_positive\n",
        "\n",
        "def calculate_perplexity(text):\n",
        "    inputs = gpt2_tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    loss = gpt2_model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
        "    perplexity = torch.exp(loss).item()\n",
        "    return perplexity\n",
        "\n",
        "def is_flip(original_score, counterfactual_score):\n",
        "    # might need to be updated for AG News\n",
        "    positive_to_negative = original_score >= 0.5 and counterfactual_score < 0.5\n",
        "    negative_to_positive = original_score < 0.5 and counterfactual_score >= 0.5\n",
        "    return positive_to_negative or negative_to_positive\n",
        "\n",
        "def truncate_text(text, max_length=100):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) > max_length:\n",
        "        text = \" \".join(tokens[:max_length])\n",
        "    return text\n",
        "\n",
        "def get_all_embeddings(model, tokenizer):\n",
        "    all_word_embeddings = torch.zeros((tokenizer.vocab_size, 768)).detach().to(device)\n",
        "    for i in range(tokenizer.vocab_size):\n",
        "        input_tensor = torch.tensor(i).view(1, 1).to(device)\n",
        "        word_embedding = model.bert.embeddings.word_embeddings(input_tensor)\n",
        "        all_word_embeddings[i, :] = word_embedding\n",
        "    all_word_embeddings = all_word_embeddings.detach().requires_grad_(False)\n",
        "    return all_word_embeddings\n",
        "\n",
        "def get_levenshtein_similarity_score(original_text, counterfactual_text):\n",
        "    score = 1 - textdistance.levenshtein.normalized_distance(original_text, counterfactual_text)\n",
        "    return score\n",
        "\n",
        "def format_polyjuice_output(polyjuice_output, original_question, original_answer):\n",
        "    # Helper function to calculate cosine similarity\n",
        "    def get_cosine_similarity(text1, text2):\n",
        "        vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
        "        return cosine_similarity(vectorizer)[0][1]\n",
        "\n",
        "    sep_token = \" [SEP] \"\n",
        "\n",
        "    # 1. Return the output if it's already valid\n",
        "    if sep_token in polyjuice_output:\n",
        "        return polyjuice_output\n",
        "\n",
        "    # Replace invalid separator tokens\n",
        "    polyjuice_output = re.sub(r\"\\[(\\w+)\\]\", sep_token, polyjuice_output)\n",
        "\n",
        "    # If it's still valid after replacement, return it\n",
        "    if sep_token in polyjuice_output:\n",
        "        return polyjuice_output\n",
        "\n",
        "    # Check if the output is more similar to a question or an answer\n",
        "    similarity_to_question = get_cosine_similarity(polyjuice_output, original_question)\n",
        "    similarity_to_answer = get_cosine_similarity(polyjuice_output, original_answer)\n",
        "\n",
        "    if polyjuice_output.strip().endswith(\"?\") or similarity_to_question > similarity_to_answer:\n",
        "        # It's likely a question, so use the new question with the original answer\n",
        "        return f\"{polyjuice_output} [SEP] {original_answer}\"\n",
        "    else:\n",
        "        # It's likely an answer, so use the original question with the new answer\n",
        "        return f\"{original_question} [SEP] {polyjuice_output}\"\n",
        "\n",
        "def get_output(df_input, counterfactual_method, args, timeout=10, batch_size=10):\n",
        "    df_input = df_input.copy()\n",
        "    output_data = {\n",
        "        \"original_text\": [],\n",
        "        \"original_score\": [],\n",
        "        \"original_perplexity\": [],\n",
        "        \"counterfactual_text\": [],\n",
        "        \"counterfactual_score\": [],\n",
        "        \"counterfactual_perplexity\": [],\n",
        "        \"found_flip\": [],\n",
        "        \"levenshtein_similarity_score\": []\n",
        "    }\n",
        "    exclude_indices = [399]\n",
        "    for i, row in df_input.iterrows():\n",
        "        if counterfactual_method.__name__ == \"generate_polyjuice_counterfactual\" and i in exclude_indices:\n",
        "            continue\n",
        "        try:\n",
        "            if dataset == \"sst_2\":\n",
        "                original_text = row[\"original_text\"]\n",
        "            elif dataset == \"qnli\":\n",
        "                original_question, original_answer = row[\"original_question\"], row[\"original_answer\"]\n",
        "                original_text = f\"{original_question} [SEP] {original_answer}\"\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported dataset: {dataset}\")\n",
        "\n",
        "            original_text = format_sentence(original_text, dataset)\n",
        "            print(f\"Processing input {i + 1}/{len(df_input)}: num tokens: {len(original_text.split())}\")\n",
        "\n",
        "            original_score = calculate_score(original_text, sentiment_model_tokenizer, dataset, device)\n",
        "            original_perplexity = calculate_perplexity(original_text)\n",
        "\n",
        "            args_with_score = {**args, \"original_score\": original_score}\n",
        "            counterfactual_text = counterfactual_method(original_text, calculate_score, args)\n",
        "\n",
        "            if counterfactual_text is None:\n",
        "                print(f\"Timeout occurred for sample {i}, skipping\")\n",
        "                continue\n",
        "\n",
        "            counterfactual_text = format_sentence(counterfactual_text, dataset)\n",
        "            if dataset == \"qnli\" and counterfactual_method.__name__ == \"generate_polyjuice_counterfactual\":\n",
        "                counterfactual_text = format_polyjuice_output(counterfactual_text, original_question, original_answer)\n",
        "\n",
        "            label_width = 20\n",
        "            print(f\"\\n{'original_text:'.ljust(label_width)} {original_text}\")\n",
        "            print(f\"{'counterfactual_text:'.ljust(label_width)} {counterfactual_text}\\n\")\n",
        "\n",
        "            counterfactual_score = calculate_score(counterfactual_text, sentiment_model_tokenizer, dataset, device)\n",
        "            counterfactual_perplexity = calculate_perplexity(counterfactual_text)\n",
        "            found_flip = is_flip(original_score, counterfactual_score)\n",
        "            levenshtein_similarity_score = get_levenshtein_similarity_score(original_text, counterfactual_text)\n",
        "\n",
        "            output_data[\"original_text\"].append(original_text)\n",
        "            output_data[\"original_score\"].append(original_score)\n",
        "            output_data[\"original_perplexity\"].append(original_perplexity)\n",
        "            output_data[\"counterfactual_text\"].append(counterfactual_text)\n",
        "            output_data[\"counterfactual_score\"].append(counterfactual_score)\n",
        "            output_data[\"counterfactual_perplexity\"].append(counterfactual_perplexity)\n",
        "            output_data[\"found_flip\"].append(found_flip)\n",
        "            output_data[\"levenshtein_similarity_score\"].append(levenshtein_similarity_score)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception {e}\")\n",
        "            print(f\"Failed to generate counterfactual, skipping sample\")\n",
        "            continue\n",
        "\n",
        "    df_output = pd.DataFrame(output_data)\n",
        "    return df_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_word_embeddings = get_all_embeddings(sentiment_model, sentiment_model_tokenizer).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get(\"API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct = 0\n",
        "\n",
        "for i in range(len(df_input)):\n",
        "    print(f\"i: {i}\")\n",
        "    row = df_input.iloc[i]\n",
        "\n",
        "    if dataset == \"sst_2\":\n",
        "        original_text, original_label = row[\"original_text\"], row[\"original_label\"]\n",
        "    elif dataset == \"qnli\":\n",
        "        original_question, original_answer, original_label = row[\"original_question\"], row[\"original_answer\"], row[\"original_label\"]\n",
        "        original_text = f\"{original_question} [SEP] {original_answer}\"\n",
        "\n",
        "    score = calculate_score(original_text, sentiment_model_tokenizer, dataset, device)\n",
        "    y_hat = 1 if score >= 0.5 else 0\n",
        "    if y_hat == original_label:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(df_input)\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Counterfactual generator functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from CLOSS.closs import generate_counterfactual\n",
        "import re\n",
        "\n",
        "def generate_polyjuice_counterfactual(original_text, _, args):\n",
        "    ctrl_code = None if dataset == \"qnli\" else \"negation\"\n",
        "    perturbations = pj.perturb(\n",
        "        orig_sent=original_text,\n",
        "        ctrl_code=ctrl_code,\n",
        "        num_perturbations=1,\n",
        "        perplex_thred=None\n",
        "    )\n",
        "    counterfactual_text = perturbations[0]\n",
        "    return counterfactual_text\n",
        "\n",
        "def generate_closs_counterfactual(original_text, calculate_score, args):\n",
        "    counterfactual_text = generate_counterfactual(\n",
        "        original_text,\n",
        "        sentiment_model,\n",
        "        LM_model,\n",
        "        calculate_score,\n",
        "        sentiment_model_tokenizer,\n",
        "        all_word_embeddings,\n",
        "        device,\n",
        "        args\n",
        "    )\n",
        "    return counterfactual_text\n",
        "\n",
        "def call_openai_api(system_prompt, model):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt}\n",
        "        ],\n",
        "        top_p=1,\n",
        "        temperature=0.4,\n",
        "        frequency_penalty=1.1\n",
        "    )\n",
        "    output = completion.choices[0].message.content\n",
        "    return output\n",
        "\n",
        "def generate_naive_fizle_counterfactual(original_text, _, args):\n",
        "    original_score, model = args[\"original_score\"], args[\"model\"]\n",
        "    original_id = 1 if original_score >= 0.5 else 0\n",
        "    cf_id = 0 if original_id == 1 else 1\n",
        "\n",
        "    original_label = id2label[original_id]\n",
        "    cf_label = id2label[cf_id]\n",
        "\n",
        "    system_prompt = f\"\"\"In the task of {fizle_task}, a trained black-box classifier correctly predicted the label '{original_label}' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from '{original_label}' to '{cf_label}'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
        "    -\n",
        "    Text: {original_text}\"\"\"\n",
        "\n",
        "    for i in range(10):\n",
        "        print(f\"attempt: {i + 1}\")\n",
        "        output = call_openai_api(system_prompt, model)\n",
        "        if not output:\n",
        "            continue\n",
        "        counterfactual_text = re.search(\"<new>(.*?)</new>\", output).group(1)\n",
        "        if counterfactual_text:\n",
        "            return counterfactual_text\n",
        "\n",
        "    if not output:\n",
        "        print(\"No counterfactual generated.\")\n",
        "\n",
        "    print(\"Failed to generate counterfactual surrounded by <new> tags\")\n",
        "    counterfactual_text = output[5:-6]\n",
        "\n",
        "    return counterfactual_text\n",
        "\n",
        "def generate_guided_fizle_counterfactual(original_text, _, args):\n",
        "    original_score, model = args[\"original_score\"], args[\"model\"]\n",
        "    original_id = 1 if original_score >= 0.5 else 0\n",
        "    cf_id = 0 if original_id == 1 else 1\n",
        "\n",
        "    original_label = id2label[original_id]\n",
        "    cf_label = id2label[cf_id]\n",
        "\n",
        "    system_prompt = \"\"\n",
        "\n",
        "    # 1. Find important words\n",
        "    step1_system_prompt = \" \".join([\n",
        "        f\"In the task of {fizle_task}, a trained black-box classifier correctly predicted the label '{original_label}' for the following text.\",\n",
        "        f\"Explain why the model predicted the '{original_label}' label by identifying the words in the input that caused the label. List ONLY the words as a comma separated list.\",\n",
        "        f\"\\n-\\nText: {original_text}\",\n",
        "        f\"\\nImportant words identified: \"\n",
        "    ])\n",
        "    system_prompt += step1_system_prompt\n",
        "    important_words = call_openai_api(step1_system_prompt, model)\n",
        "    system_prompt += important_words + \"\\n\"\n",
        "\n",
        "    # 2. Generate the final counterfactual\n",
        "    correct_output_format = False\n",
        "    for i in range(10):\n",
        "        step2_system_prompt = \" \".join([\n",
        "            f\"Generate a counterfactual explanation for the original text by ONLY changing a minimal set of the words you identified, so that the label changes from '{original_label}' to '{cf_label}'.\",\n",
        "            f\"Use the following definition of 'counterfactual explanation': 'A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.'\",\n",
        "            f\"Enclose the generated text within <new> tags.\"\n",
        "        ])\n",
        "        final_system_prompt = system_prompt + step2_system_prompt\n",
        "        step2_output = call_openai_api(final_system_prompt, model)\n",
        "        if not step2_output:\n",
        "            continue\n",
        "        counterfactual_text = re.search(\"<new>(.*?)</new>\", step2_output).group(1)\n",
        "        if counterfactual_text:\n",
        "            return counterfactual_text\n",
        "\n",
        "    if not output:\n",
        "        print(\"No counterfactual generated.\")\n",
        "\n",
        "    print(\"Failed to generate counterfactual surrounded by <new> tags\")\n",
        "    counterfactual_text = output[5:-6]\n",
        "\n",
        "    return counterfactual_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run CLOSS and HotFlip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Move to the main parent directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd \"CLOSS\"\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Run HotFlip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\n",
        "    \"beam_width\": 15,\n",
        "    \"w\": 5,\n",
        "    \"K\": 30,\n",
        "    \"tree_depth\": 0.3,\n",
        "    \"substitution_evaluation_method\": \"hotflip_only\",\n",
        "    \"substitution_gen_method\": \"hotflip_only\",\n",
        "    \"dataset\": dataset\n",
        "}\n",
        "\n",
        "df_output_hotflip = get_output(df_input, generate_closs_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output_hotflip.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output_hotflip.to_csv(f\"./output/hotflip-output-{dataset}-{num_samples}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Run CLOSS without optimization and without retraining the language modeling head:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\n",
        "    \"beam_width\": 15,\n",
        "    \"w\": 5,\n",
        "    \"K\": 30,\n",
        "    \"substitutions_after_loc\": 0.3,\n",
        "    \"tree_depth\": 0.3,\n",
        "    \"substitution_evaluation_method\": \"SVs\",\n",
        "    \"substitution_gen_method\": \"no_opt_lmh\",\n",
        "    \"dataset\": dataset\n",
        "}\n",
        "\n",
        "df_output_closs = get_output(df_input, generate_closs_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output_closs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output_closs.to_csv(f\"./output/closs-output-{dataset}-{num_samples}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Polyjuice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd polyjuice\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make sure the model is being imported properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import polyjuice\n",
        "\n",
        "importlib.reload(polyjuice)\n",
        "print(polyjuice.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from polyjuice import Polyjuice\n",
        "\n",
        "pj = Polyjuice(model_path=\"uw-hai/polyjuice\", is_cuda=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"julia is played with exasperating blandness by laura regan .\"\n",
        "perturbations = pj.perturb(\n",
        "    orig_sent=text,\n",
        "    ctrl_code=\"negation\",\n",
        "    perplex_thred=None\n",
        ")\n",
        "perturbations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the model and get the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output_polyjuice = get_output(df_input, generate_polyjuice_counterfactual, {})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output_polyjuice.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ..\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output_polyjuice.to_csv(f\"./output/polyjuice-output-{dataset}-{num_samples}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FIZLE\n",
        "\n",
        "* Naive: uses a single prompt.\n",
        "* Guided: Uses two prompts. The first prompt identifies important words and the second prompt generates the counterfactual.\n",
        "\n",
        "Hyperparameters: top_p sampling = 1, temperature t = 0.4 and repetition penalty = 1.1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. FIZLE naive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\"model\": \"gpt-4-turbo\"}\n",
        "df_output = get_output(df_input, generate_naive_fizle_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(f\"./output/fizlenaive-output-{dataset}-new-2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FIZLE guided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\"model\": \"gpt-4-turbo\"}\n",
        "df_output = get_output(df_input, generate_naive_fizle_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(f\"./output/fizleguided-output-{dataset}.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
