{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Mount Google Drive and clone the repository containing the methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUgYxHsWZU8y",
        "outputId": "4254c890-6bd2-4194-c3cc-c99744f2d96c"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "\n",
        "github_username = input(\"Enter your GitHub username: \")\n",
        "github_token = getpass.getpass(\"Enter your GitHub personal access token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPOKvD_RafHj",
        "outputId": "96ffda6e-c3e6-466a-d559-0a2b44931081"
      },
      "outputs": [],
      "source": [
        "repo_name = \"smcaleese/masters-thesis-code\"\n",
        "!git clone https://{github_username}:{github_token}@github.com/{repo_name}.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "EDPv3DJ8cER4",
        "outputId": "04fbecf9-3acc-4856-de56-f2c2d36c60a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'masters-thesis-code'\n",
            "/Users/smcaleese/Documents/masters-thesis-code\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
            "  bkms = self.shell.db.get('bookmarks', {})\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd masters-thesis-code\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers datasets textdistance openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download datasets\n",
        "\n",
        "Download the SST-2, QNLI and AG News datasets, clean the sentences, and create a list of input sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_samples = 100\n",
        "\n",
        "# dataset = \"sst_2\"\n",
        "dataset = \"qnli\"\n",
        "# dataset = \"ag_news\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def format_sentence(sentence, dataset):\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # remove two spaces around a comma:\n",
        "    sentence = re.sub(r\"\\s(')\\s(ve|re|s|t|ll|d)\", r\"\\1\\2\", sentence)\n",
        "\n",
        "    # remove spaces around hyphens:\n",
        "    sentence = re.sub(r\"-\\s-\", \"--\", sentence)\n",
        "    sentence = re.sub(r\"(\\w)\\s-\\s(\\w)\", r\"\\1-\\2\", sentence)\n",
        "\n",
        "    def replace(match):\n",
        "        return match.group(1)\n",
        "\n",
        "    # remove spaces before punctuation and n't:\n",
        "    sentence = re.sub(r\"\\s([.!,?:;')]|n't)\", replace, sentence)\n",
        "\n",
        "    # remove spaces after opening parenthesis:\n",
        "    sentence = re.sub(r\"([(])\\s\", replace, sentence)\n",
        "\n",
        "    if dataset == \"qnli\":\n",
        "        sentence = re.sub(r\"\\s(\\[sep\\])\\s\", \" [SEP] \", sentence)\n",
        "    \n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "if dataset == \"sst_2\":\n",
        "    sst = load_dataset(\"stanfordnlp/sst2\")\n",
        "\n",
        "    sst_sentences = sst[\"train\"][\"sentence\"]\n",
        "    sst_labels = sst[\"train\"][\"label\"]\n",
        "\n",
        "    sst_sentences_subset = sst_sentences[:num_samples]\n",
        "    sst_labels_subset = sst_labels[:num_samples]\n",
        "\n",
        "    sst_sentences_subset_formatted = [format_sentence(sentence, dataset) for sentence in sst_sentences_subset]\n",
        "\n",
        "elif dataset == \"qnli\":\n",
        "    qnli = load_dataset(\"glue\", \"qnli\")\n",
        "\n",
        "    qnli_questions = qnli[\"train\"][\"question\"]\n",
        "    qnli_answers = qnli[\"train\"][\"sentence\"]\n",
        "    qnli_labels = qnli[\"train\"][\"label\"]\n",
        "\n",
        "    qnli_questions_subset = qnli_questions[:num_samples]\n",
        "    qnli_answers_subset = qnli_answers[:num_samples]\n",
        "    qnli_labels_subset = qnli_labels[:num_samples]\n",
        "\n",
        "    qnli_questions_subset_formatted = [format_sentence(sentence, dataset) for sentence in qnli_questions_subset]\n",
        "    qnli_answers_subset_formatted = [format_sentence(sentence, dataset) for sentence in qnli_answers_subset]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write the sentences to a file named `sst-input.csv` and `qnli-input.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "if dataset == \"sst_2\":\n",
        "    df_sst = pd.DataFrame({\n",
        "        \"original_text\": sst_sentences_subset_formatted,\n",
        "        \"original_label\": sst_labels_subset\n",
        "    })\n",
        "    df_sst.to_csv(f\"./input/{dataset}-input.csv\", index=False)\n",
        "\n",
        "elif dataset == \"qnli\":\n",
        "    df_qnli = pd.DataFrame({\n",
        "        \"original_question\": qnli_questions_subset_formatted,\n",
        "        \"original_answer\": qnli_answers_subset_formatted,\n",
        "        \"original_label\": qnli_labels_subset\n",
        "    })\n",
        "    df_qnli.to_csv(f\"./input/{dataset}-input.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choose dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dataset == \"sst_2\":\n",
        "    input_file = f\"{dataset}-input\"\n",
        "    model_name = \"textattack/bert-base-uncased-SST-2\"\n",
        "    fizle_task = \"sentiment analysis on the SST-2 dataset\"\n",
        "elif dataset == \"qnli\":\n",
        "    input_file = f\"{dataset}-input\"\n",
        "    model_name = \"textattack/bert-base-uncased-QNLI\"\n",
        "    fizle_task = \"natural language inference on the QNLI dataset\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create input dataframe\n",
        "\n",
        "Columns to add to create output dataframe:\n",
        "- original_score\n",
        "- original_perplexity\n",
        "- counterfactual_text\n",
        "- counterfactual_score\n",
        "- counterfactual_perplexity\n",
        "- found_flip\n",
        "- frac_tokens_same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_question</th>\n",
              "      <th>original_answer</th>\n",
              "      <th>original_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when did the third digimon series begin?</td>\n",
              "      <td>unlike the two seasons before it and most of t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>which missile batteries often have individual ...</td>\n",
              "      <td>when manpads is operated by specialists, batte...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what two things does popper argue tarski's the...</td>\n",
              "      <td>he bases this interpretation on the fact that ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what is the name of the village 9 miles north ...</td>\n",
              "      <td>on 31 december 1853, the ottoman forces at cal...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what famous palace is located in london?</td>\n",
              "      <td>london contains four world heritage sites: the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   original_question  \\\n",
              "0           when did the third digimon series begin?   \n",
              "1  which missile batteries often have individual ...   \n",
              "2  what two things does popper argue tarski's the...   \n",
              "3  what is the name of the village 9 miles north ...   \n",
              "4           what famous palace is located in london?   \n",
              "\n",
              "                                     original_answer  original_label  \n",
              "0  unlike the two seasons before it and most of t...               1  \n",
              "1  when manpads is operated by specialists, batte...               1  \n",
              "2  he bases this interpretation on the fact that ...               0  \n",
              "3  on 31 december 1853, the ottoman forces at cal...               0  \n",
              "4  london contains four world heritage sites: the...               1  "
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_input = pd.read_csv(f\"input/{input_file}.csv\")\n",
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 3)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the sentiment model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "if dataset == \"sst_2\":\n",
        "    id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "    label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    ).to(device)\n",
        "\n",
        "elif dataset == \"qnli\":\n",
        "    id2label = {0: \"entailment\", 1: \"not_entailment\"}\n",
        "    label2id = {\"entailment\": 0, \"not_entailment\": 1}\n",
        "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    ).to(device)\n",
        "\n",
        "sentiment_model_tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the GPT-2 model for calculating perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the language model for CLOSS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "# TODO: try using a larger model to improve performance: https://arxiv.org/pdf/2111.09543\n",
        "LM_model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "LM_model.lm_head = LM_model.cls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import textdistance\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_score(text, sentiment_model_tokenizer, dataset, device):\n",
        "    def tokenize_with_correct_token_type_ids(input_text, tokenizer):\n",
        "        # Tokenize the input\n",
        "        tokens = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "        \n",
        "        # Get the position of the first [SEP] token\n",
        "        sep_pos = (tokens.input_ids == tokenizer.sep_token_id).nonzero()[0, 1].item()\n",
        "        \n",
        "        # Create token_type_ids\n",
        "        token_type_ids = torch.zeros_like(tokens.input_ids)\n",
        "        token_type_ids[0, sep_pos+1:] = 1  # Set to 1 after the first [SEP] token\n",
        "        \n",
        "        # Update the tokens dictionary\n",
        "        tokens['token_type_ids'] = token_type_ids\n",
        "        \n",
        "        return tokens\n",
        "\n",
        "    if type(text) == list:\n",
        "        if type(text[0]) == str:\n",
        "            tokens = text\n",
        "            ids = sentiment_model_tokenizer.convert_tokens_to_ids(tokens)\n",
        "            text = sentiment_model_tokenizer.decode(ids[1:-1])\n",
        "        elif type(text[0]) == int:\n",
        "            ids = text\n",
        "            text = sentiment_model_tokenizer.decode(ids[1:-1])\n",
        "\n",
        "    if dataset == \"sst_2\":\n",
        "        inputs = sentiment_model_tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    elif dataset == \"qnli\":\n",
        "        inputs = tokenize_with_correct_token_type_ids(text, sentiment_model_tokenizer).to(device)\n",
        "\n",
        "    logits = sentiment_model(**inputs).logits\n",
        "    prob_positive = torch.nn.functional.softmax(logits, dim=1)[0][1].item()\n",
        "    return prob_positive\n",
        "\n",
        "def calculate_perplexity(text):\n",
        "    inputs = gpt2_tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    loss = gpt2_model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
        "    perplexity = torch.exp(loss).item()\n",
        "    return perplexity\n",
        "\n",
        "def is_flip(original_score, counterfactual_score):\n",
        "    # might need to be updated for AG News\n",
        "    positive_to_negative = original_score >= 0.5 and counterfactual_score < 0.5\n",
        "    negative_to_positive = original_score < 0.5 and counterfactual_score >= 0.5\n",
        "    return positive_to_negative or negative_to_positive\n",
        "\n",
        "def truncate_text(text, max_length=100):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) > max_length:\n",
        "        text = \" \".join(tokens[:max_length])\n",
        "    return text\n",
        "\n",
        "def get_all_embeddings(model, tokenizer):\n",
        "    all_word_embeddings = torch.zeros((tokenizer.vocab_size, 768)).detach().to(device)\n",
        "    for i in range(tokenizer.vocab_size):\n",
        "        input_tensor = torch.tensor(i).view(1, 1).to(device)\n",
        "        word_embedding = model.bert.embeddings.word_embeddings(input_tensor)\n",
        "        all_word_embeddings[i, :] = word_embedding\n",
        "    all_word_embeddings = all_word_embeddings.detach().requires_grad_(False)\n",
        "    return all_word_embeddings\n",
        "\n",
        "def get_levenshtein_similarity_score(original_text, counterfactual_text):\n",
        "    score = 1 - textdistance.levenshtein.normalized_distance(original_text, counterfactual_text)\n",
        "    return score\n",
        "\n",
        "def format_polyjuice_output(polyjuice_output, original_question, original_answer):\n",
        "    # Helper function to calculate cosine similarity\n",
        "    def get_cosine_similarity(text1, text2):\n",
        "        vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
        "        return cosine_similarity(vectorizer)[0][1]\n",
        "\n",
        "    sep_token = \" [SEP] \"\n",
        "\n",
        "    # 1. Return the output if it's already valid\n",
        "    if sep_token in polyjuice_output:\n",
        "        return polyjuice_output\n",
        "\n",
        "    # Replace invalid separator tokens\n",
        "    polyjuice_output = re.sub(r\"\\[(\\w+)\\]\", sep_token, polyjuice_output)\n",
        "\n",
        "    # If it's still valid after replacement, return it\n",
        "    if sep_token in polyjuice_output:\n",
        "        return polyjuice_output\n",
        "\n",
        "    # Check if the output is more similar to a question or an answer\n",
        "    similarity_to_question = get_cosine_similarity(polyjuice_output, original_question)\n",
        "    similarity_to_answer = get_cosine_similarity(polyjuice_output, original_answer)\n",
        "\n",
        "    if polyjuice_output.strip().endswith(\"?\") or similarity_to_question > similarity_to_answer:\n",
        "        # It's likely a question, so use the new question with the original answer\n",
        "        return f\"{polyjuice_output} [SEP] {original_answer}\"\n",
        "    else:\n",
        "        # It's likely an answer, so use the original question with the new answer\n",
        "        return f\"{original_question} [SEP] {polyjuice_output}\"\n",
        "\n",
        "def get_output(df_input, counterfactual_method, args):\n",
        "    df_input = df_input.copy()\n",
        "    output_data = {\n",
        "        \"original_text\": [],\n",
        "        \"original_score\": [],\n",
        "        \"original_perplexity\": [],\n",
        "        \"counterfactual_text\": [],\n",
        "        \"counterfactual_score\": [],\n",
        "        \"counterfactual_perplexity\": [],\n",
        "        \"found_flip\": [],\n",
        "        \"levenshtein_similarity_score\": []\n",
        "    }\n",
        "    for i in range(len(df_input)):\n",
        "        try:\n",
        "            if dataset == \"sst_2\":\n",
        "                original_text = df_input.iloc[i][\"original_text\"]\n",
        "                original_text = truncate_text(original_text)\n",
        "                original_text = format_sentence(original_text, dataset)\n",
        "                print(f\"Processing input {i + 1}/{len(df_input)}: num tokens: {len(original_text.split())}\")\n",
        "\n",
        "                original_score = calculate_score(original_text, sentiment_model_tokenizer, dataset, device)\n",
        "                original_perplexity = calculate_perplexity(original_text)\n",
        "\n",
        "                try:\n",
        "                    args = {**args, \"original_score\": original_score}\n",
        "                    counterfactual_text = counterfactual_method(original_text, calculate_score, args)\n",
        "                    counterfactual_text = format_sentence(counterfactual_text, dataset)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}, failed to generate counterfactual, skipping sample\")\n",
        "                    counterfactual_text = original_text\n",
        "\n",
        "                label_width = 20\n",
        "                print(f\"\\n{'original_text:'.ljust(label_width)} {original_text}\")\n",
        "                print(f\"{'counterfactual_text:'.ljust(label_width)} {counterfactual_text}\\n\")\n",
        "\n",
        "                counterfactual_score = calculate_score(counterfactual_text, sentiment_model_tokenizer, dataset, device)\n",
        "                counterfactual_perplexity = calculate_perplexity(counterfactual_text)\n",
        "                found_flip = is_flip(original_score, counterfactual_score)\n",
        "                levenshtein_similarity_score = get_levenshtein_similarity_score(original_text, counterfactual_text)\n",
        "\n",
        "                output_data[\"original_text\"].append(original_text)\n",
        "                output_data[\"original_score\"].append(original_score)\n",
        "                output_data[\"original_perplexity\"].append(original_perplexity)\n",
        "                output_data[\"counterfactual_text\"].append(counterfactual_text)\n",
        "                output_data[\"counterfactual_score\"].append(counterfactual_score)\n",
        "                output_data[\"counterfactual_perplexity\"].append(counterfactual_perplexity)\n",
        "                output_data[\"found_flip\"].append(found_flip)\n",
        "                output_data[\"levenshtein_similarity_score\"].append(levenshtein_similarity_score)\n",
        "\n",
        "            elif dataset == \"qnli\":\n",
        "                row = df_input.iloc[i]\n",
        "                original_question, original_answer = row[\"original_question\"], row[\"original_answer\"]\n",
        "                original_text = f\"{original_question} [SEP] {original_answer}\"\n",
        "                original_text = format_sentence(original_text, dataset)\n",
        "\n",
        "                print(f\"Processing input {i + 1}/{len(df_input)}: num tokens: {len(original_text.split())}\")\n",
        "\n",
        "                original_score = calculate_score(original_text, sentiment_model_tokenizer, dataset, device)\n",
        "                original_perplexity = calculate_perplexity(original_text)\n",
        "\n",
        "                args = {**args, \"original_score\": original_score}\n",
        "                counterfactual_text = counterfactual_method(original_text, calculate_score, args)\n",
        "                if counterfactual_method.__name__ == \"generate_polyjuice_counterfactual\":\n",
        "                    counterfactual_text = format_polyjuice_output(counterfactual_text, original_question, original_answer)\n",
        "                counterfactual_text = format_sentence(counterfactual_text, dataset)\n",
        "\n",
        "                label_width = 20\n",
        "                print(f\"\\n{'original_text:'.ljust(label_width)} {original_text}\")\n",
        "                print(f\"{'counterfactual_text:'.ljust(label_width)} {counterfactual_text}\\n\")\n",
        "\n",
        "                counterfactual_score = calculate_score(counterfactual_text, sentiment_model_tokenizer, dataset, device)\n",
        "                counterfactual_perplexity = calculate_perplexity(counterfactual_text)\n",
        "                found_flip = is_flip(original_score, counterfactual_score)\n",
        "                levenshtein_similarity_score = get_levenshtein_similarity_score(original_text, counterfactual_text)\n",
        "\n",
        "                output_data[\"original_text\"].append(original_text)\n",
        "                output_data[\"original_score\"].append(original_score)\n",
        "                output_data[\"original_perplexity\"].append(original_perplexity)\n",
        "                output_data[\"counterfactual_text\"].append(counterfactual_text)\n",
        "                output_data[\"counterfactual_score\"].append(counterfactual_score)\n",
        "                output_data[\"counterfactual_perplexity\"].append(counterfactual_perplexity)\n",
        "                output_data[\"found_flip\"].append(found_flip)\n",
        "                output_data[\"levenshtein_similarity_score\"].append(levenshtein_similarity_score)\n",
        "        except Exception as e:\n",
        "            print(f\"Exception {e}\")\n",
        "            print(f\"Failed to generate counterfactual, skipping sample\")\n",
        "            continue\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Runtime error {e}\")\n",
        "            print(f\"Failed to generate counterfactual, skipping sample\")\n",
        "            continue\n",
        "\n",
        "    df_output = pd.DataFrame(output_data)\n",
        "    return df_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_word_embeddings = get_all_embeddings(sentiment_model, sentiment_model_tokenizer).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "# from google.colab import userdata\n",
        "\n",
        "# client = OpenAI(api_key=userdata.get(\"API_KEY\"))\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i: 0\n",
            "i: 1\n",
            "i: 2\n",
            "i: 3\n",
            "i: 4\n",
            "i: 5\n",
            "i: 6\n",
            "i: 7\n",
            "i: 8\n",
            "i: 9\n",
            "i: 10\n",
            "i: 11\n",
            "i: 12\n",
            "i: 13\n",
            "i: 14\n",
            "i: 15\n",
            "i: 16\n",
            "i: 17\n",
            "i: 18\n",
            "i: 19\n",
            "i: 20\n",
            "i: 21\n",
            "i: 22\n",
            "i: 23\n",
            "i: 24\n",
            "i: 25\n",
            "i: 26\n",
            "i: 27\n",
            "i: 28\n",
            "i: 29\n",
            "i: 30\n",
            "i: 31\n",
            "i: 32\n",
            "i: 33\n",
            "i: 34\n",
            "i: 35\n",
            "i: 36\n",
            "i: 37\n",
            "i: 38\n",
            "i: 39\n",
            "i: 40\n",
            "i: 41\n",
            "i: 42\n",
            "i: 43\n",
            "i: 44\n",
            "i: 45\n",
            "i: 46\n",
            "i: 47\n",
            "i: 48\n",
            "i: 49\n",
            "i: 50\n",
            "i: 51\n",
            "i: 52\n",
            "i: 53\n",
            "i: 54\n",
            "i: 55\n",
            "i: 56\n",
            "i: 57\n",
            "i: 58\n",
            "i: 59\n",
            "i: 60\n",
            "i: 61\n",
            "i: 62\n",
            "i: 63\n",
            "i: 64\n",
            "i: 65\n",
            "i: 66\n",
            "i: 67\n",
            "i: 68\n",
            "i: 69\n",
            "i: 70\n",
            "i: 71\n",
            "i: 72\n",
            "i: 73\n",
            "i: 74\n",
            "i: 75\n",
            "i: 76\n",
            "i: 77\n",
            "i: 78\n",
            "i: 79\n",
            "i: 80\n",
            "i: 81\n",
            "i: 82\n",
            "i: 83\n",
            "i: 84\n",
            "i: 85\n",
            "i: 86\n",
            "i: 87\n",
            "i: 88\n",
            "i: 89\n",
            "i: 90\n",
            "i: 91\n",
            "i: 92\n",
            "i: 93\n",
            "i: 94\n",
            "i: 95\n",
            "i: 96\n",
            "i: 97\n",
            "i: 98\n",
            "i: 99\n",
            "accuracy: 0.97\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "\n",
        "for i in range(len(df_input)):\n",
        "    print(f\"i: {i}\")\n",
        "    row = df_input.iloc[i]\n",
        "\n",
        "    if dataset == \"sst_2\":\n",
        "        original_text, original_label = row[\"original_text\"], row[\"original_label\"]\n",
        "    elif dataset == \"qnli\":\n",
        "        original_question, original_answer, original_label = row[\"original_question\"], row[\"original_answer\"], row[\"original_label\"]\n",
        "        original_text = f\"{original_question} [SEP] {original_answer}\"\n",
        "\n",
        "    score = calculate_score(original_text, sentiment_model_tokenizer, dataset, device)\n",
        "    y_hat = 1 if score >= 0.5 else 0\n",
        "    if y_hat == original_label:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(df_input)\n",
        "print(f\"accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Counterfactual generator functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %cd \"CLOSS\"\n",
        "# %cd ..\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "from CLOSS.closs import generate_counterfactual\n",
        "import re\n",
        "\n",
        "def generate_polyjuice_counterfactual(original_text, _, args):\n",
        "    ctrl_code = None if dataset == \"qnli\" else \"negation\"\n",
        "    perturbations = pj.perturb(\n",
        "        orig_sent=original_text,\n",
        "        ctrl_code=ctrl_code,\n",
        "        num_perturbations=1,\n",
        "        perplex_thred=None\n",
        "    )\n",
        "    counterfactual_text = perturbations[0]\n",
        "    return counterfactual_text\n",
        "\n",
        "def generate_closs_counterfactual(original_text, calculate_score, args):\n",
        "    counterfactual_text = generate_counterfactual(\n",
        "        original_text,\n",
        "        sentiment_model,\n",
        "        LM_model,\n",
        "        calculate_score,\n",
        "        sentiment_model_tokenizer,\n",
        "        all_word_embeddings,\n",
        "        device,\n",
        "        args\n",
        "    )\n",
        "    return counterfactual_text\n",
        "\n",
        "def call_openai_api(system_prompt, model):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt}\n",
        "        ],\n",
        "        top_p=1,\n",
        "        temperature=0.4,\n",
        "        frequency_penalty=1.1\n",
        "    )\n",
        "    output = completion.choices[0].message.content\n",
        "    return output\n",
        "\n",
        "def generate_naive_fizle_counterfactual(original_text, _, args):\n",
        "    original_score, model = args[\"original_score\"], args[\"model\"]\n",
        "    original_id = 1 if original_score >= 0.5 else 0\n",
        "    cf_id = 0 if original_id == 1 else 1\n",
        "\n",
        "    original_label = id2label[original_id]\n",
        "    cf_label = id2label[cf_id]\n",
        "\n",
        "    system_prompt = f\"\"\"In the task of {fizle_task}, a trained black-box classifier correctly predicted the label '{original_label}' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from '{original_label}' to '{cf_label}'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
        "    -\n",
        "    Text: {original_text}\"\"\"\n",
        "\n",
        "    for i in range(10):\n",
        "        print(f\"attempt: {i + 1}\")\n",
        "        output = call_openai_api(system_prompt, model)\n",
        "        if not output:\n",
        "            continue\n",
        "        counterfactual_text = re.search(\"<new>(.*?)</new>\", output).group(1)\n",
        "        if counterfactual_text:\n",
        "            return counterfactual_text\n",
        "\n",
        "    if not output:\n",
        "        print(\"No counterfactual generated.\")\n",
        "\n",
        "    print(\"Failed to generate counterfactual surrounded by <new> tags\")\n",
        "    counterfactual_text = output[5:-6]\n",
        "\n",
        "    return counterfactual_text\n",
        "\n",
        "def generate_guided_fizle_counterfactual(original_text, _, args):\n",
        "    original_score, model = args[\"original_score\"], args[\"model\"]\n",
        "    original_id = 1 if original_score >= 0.5 else 0\n",
        "    cf_id = 0 if original_id == 1 else 1\n",
        "\n",
        "    original_label = id2label[original_id]\n",
        "    cf_label = id2label[cf_id]\n",
        "\n",
        "    system_prompt = \"\"\n",
        "\n",
        "    # 1. Find important words\n",
        "    step1_system_prompt = \" \".join([\n",
        "        f\"In the task of {fizle_task}, a trained black-box classifier correctly predicted the label '{original_label}' for the following text.\",\n",
        "        f\"Explain why the model predicted the '{original_label}' label by identifying the words in the input that caused the label. List ONLY the words as a comma separated list.\",\n",
        "        f\"\\n-\\nText: {original_text}\",\n",
        "        f\"\\nImportant words identified: \"\n",
        "    ])\n",
        "    system_prompt += step1_system_prompt\n",
        "    important_words = call_openai_api(step1_system_prompt, model)\n",
        "    system_prompt += important_words + \"\\n\"\n",
        "\n",
        "    # 2. Generate the final counterfactual\n",
        "    correct_output_format = False\n",
        "    for i in range(10):\n",
        "        step2_system_prompt = \" \".join([\n",
        "            f\"Generate a counterfactual explanation for the original text by ONLY changing a minimal set of the words you identified, so that the label changes from '{original_label}' to '{cf_label}'.\",\n",
        "            f\"Use the following definition of 'counterfactual explanation': 'A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.'\",\n",
        "            f\"Enclose the generated text within <new> tags.\"\n",
        "        ])\n",
        "        final_system_prompt = system_prompt + step2_system_prompt\n",
        "        step2_output = call_openai_api(final_system_prompt, model)\n",
        "        if not step2_output:\n",
        "            continue\n",
        "        counterfactual_text = re.search(\"<new>(.*?)</new>\", step2_output).group(1)\n",
        "        if counterfactual_text:\n",
        "            return counterfactual_text\n",
        "\n",
        "    if not output:\n",
        "        print(\"No counterfactual generated.\")\n",
        "\n",
        "    print(\"Failed to generate counterfactual surrounded by <new> tags\")\n",
        "    counterfactual_text = output[5:-6]\n",
        "\n",
        "    return counterfactual_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run CLOSS and HotFlip\n",
        "\n",
        "First run the method without optimization (`CLOSS-EO`) and without retraining the language modeling head.\n",
        "\n",
        "- `CLOSS-EO:` skip optimizing the embedding. This increases failures but lowers perplexity.\n",
        "- `CLOSS-RTL:` skip retraining the language modeling head. This has no effect on perplexity but increases the failure rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Move to the main parent directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code'"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %cd \"CLOSS\"\n",
        "# %cd ..\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_question</th>\n",
              "      <th>original_answer</th>\n",
              "      <th>original_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when did the third digimon series begin?</td>\n",
              "      <td>unlike the two seasons before it and most of t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>which missile batteries often have individual ...</td>\n",
              "      <td>when manpads is operated by specialists, batte...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what two things does popper argue tarski's the...</td>\n",
              "      <td>he bases this interpretation on the fact that ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what is the name of the village 9 miles north ...</td>\n",
              "      <td>on 31 december 1853, the ottoman forces at cal...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what famous palace is located in london?</td>\n",
              "      <td>london contains four world heritage sites: the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   original_question  \\\n",
              "0           when did the third digimon series begin?   \n",
              "1  which missile batteries often have individual ...   \n",
              "2  what two things does popper argue tarski's the...   \n",
              "3  what is the name of the village 9 miles north ...   \n",
              "4           what famous palace is located in london?   \n",
              "\n",
              "                                     original_answer  original_label  \n",
              "0  unlike the two seasons before it and most of t...               1  \n",
              "1  when manpads is operated by specialists, batte...               1  \n",
              "2  he bases this interpretation on the fact that ...               0  \n",
              "3  on 31 december 1853, the ottoman forces at cal...               0  \n",
              "4  london contains four world heritage sites: the...               1  "
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Run HotFlip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 1/100: num tokens: 51\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.468466579914093\n",
            "59 65\n",
            "Old tokens           :  [CLS]when  did the third digimon series begin?   [SEP]unlikethe two seasons before it and most of the seasons that followed , digimon tamers takes a darker and more realistic approach to its story featuring digimon who do not reincarnate after their deaths and more complex character development in the original \u001b[31mjapanese\u001b[0m \u001b[31m.    \u001b[0m [SEP]\n",
            "New tokens           :  [CLS]you   did the third digimo  n   series beginᵀ   [SEP]tino  ˈ   two seasons before it and most of the seasons that followed , digimo  n   tamers   takes a darker and more realistic approach to its story featuring digimo  n   who do not reincar  nate   after their deaths and more complex character development in the original \u001b[31mbarangay\u001b[0m \u001b[31m[CLS]\u001b[0m [SEP]\n",
            "Best prob gain       : 0.528\n",
            "Fraction toks same   : 0.908\n",
            "\n",
            "original_text:       when did the third digimon series begin? [SEP] unlike the two seasons before it and most of the seasons that followed, digimon tamers takes a darker and more realistic approach to its story featuring digimon who do not reincarnate after their deaths and more complex character development in the original japanese.\n",
            "counterfactual_text: ou did the third digimon series beginᵀ [sep]tinoˈ two seasons before it and most of the seasons that followed, digimon tamers takes a darker and more realistic approach to its story featuring digimon who do not reincarnate after their deaths and more complex character development in the original barangay [cls]\n",
            "\n",
            "Processing input 2/100: num tokens: 38\n",
            "Final eval prob pos: 0.9950756430625916\n",
            "50 50\n",
            "Old tokens           :  [CLS] which missile batteries often have individual launchers several kilometres from one another ? [SEP] when manpads is operated by specialists , batteries may have several dozen teams deploying separately in small sections ; self - propelled air defence guns may deploy in pairs . [SEP]\n",
            "New tokens           :  [CLS] which missile batteries often have individual launchers   several kilometres from one another ? [SEP] when manpad  s   is operated by specialists , batteries may have several dozen teams deploying   separately in small sections ; self - propelled air defence guns may deploy in pairs . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       which missile batteries often have individual launchers several kilometres from one another? [SEP] when manpads is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs.\n",
            "counterfactual_text: which missile batteries often have individual launchers several kilometres from one another? [SEP] when manpads is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs.\n",
            "\n",
            "Processing input 3/100: num tokens: 42\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.949820339679718\n",
            "51 52\n",
            "Old tokens           :  [CLS] what \u001b[31mtwo    \u001b[0m things does popper argue tarski ' s theory involves in an evaluation of truth ? [SEP] he bases this interpretation on the fact that examples such as the one described above refer to two things : assertions and the facts to which they refer . [SEP]\n",
            "New tokens           :  [CLS] what \u001b[31msleeper\u001b[0m things does popper   argue tarski   ' s theory involves in an evaluation of truth ? [SEP] he bases this interpretation on the fact that examples such as the one described above refer to two things : assertions   and the facts to which they refer . [SEP]\n",
            "Best prob gain       : 0.941\n",
            "Fraction toks same   : 0.981\n",
            "\n",
            "original_text:       what two things does popper argue tarski's theory involves in an evaluation of truth? [SEP] he bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.\n",
            "counterfactual_text: what sleeper things does popper argue tarski's theory involves in an evaluation of truth? [SEP] he bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.\n",
            "\n",
            "Processing input 4/100: num tokens: 53\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.945337176322937\n",
            "65 69\n",
            "Old tokens           :  [CLS] what is the name of the \u001b[31mvillage\u001b[0m \u001b[31m9        \u001b[0m miles north of calafat where the ottoman forces attacked the russians ? [SEP] on 31 december 1853 , the ottoman forces at calafat moved against the russian force at chetatea or cetate , a small village \u001b[31mnine  \u001b[0m miles north of cal \u001b[31maf   \u001b[0mat , and engaged them on 6 january 1854 . [SEP]\n",
            "New tokens           :  [CLS] what is the name of the \u001b[31mpanzer \u001b[0m \u001b[31mrosenthal\u001b[0m miles north of calaf  at   where the ottoman forces attacked the russians ? [SEP] on 31 december 1853 , the ottoman forces at calaf  at   moved against the russian force at chetate  a   or cetate   , a small village \u001b[31mminded\u001b[0m miles north of cal \u001b[31muniting\u001b[0mat   , and engaged them on 6 january 1854 . [SEP]\n",
            "Best prob gain       : 0.942\n",
            "Fraction toks same   : 0.942\n",
            "\n",
            "original_text:       what is the name of the village 9 miles north of calafat where the ottoman forces attacked the russians? [SEP] on 31 december 1853, the ottoman forces at calafat moved against the russian force at chetatea or cetate, a small village nine miles north of calafat, and engaged them on 6 january 1854.\n",
            "counterfactual_text: what is the name of the panzer rosenthal miles north of calafat where the ottoman forces attacked the russians? [SEP] on 31 december 1853, the ottoman forces at calafat moved against the russian force at chetatea or cetate, a small village minded miles north of cal unitingat, and engaged them on 6 january 1854.\n",
            "\n",
            "Processing input 5/100: num tokens: 53\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.9966715574264526\n",
            "72 73\n",
            "Old tokens           :  [CLS] what famouspalace  is located in london ? [SEP] london contains four world heritage sites : the tower of london ; kew gardens ; the site comprising the palace of westminster , westminster abbey , and st margaret ' s church ; and the historic settlement of greenwich ( in which the royal observatory , greenwich marks the prime meridian , 0° longitude , and gmt ) . [SEP]\n",
            "New tokens           :  [CLS] what famouserina   is located in london ? [SEP] london contains four world heritage sites : the tower of london ; kew   gardens ; the site comprising the palace of westminster , westminster abbey , and st margaret ' s church ; and the historic settlement of greenwich ( in which the royal observatory , greenwich marks the prime meridian , 0°   longitude , and gmt   ) . [SEP]\n",
            "Best prob gain       : 0.837\n",
            "Fraction toks same   : 0.986\n",
            "\n",
            "original_text:       what famous palace is located in london? [SEP] london contains four world heritage sites: the tower of london; kew gardens; the site comprising the palace of westminster, westminster abbey, and st margaret's church; and the historic settlement of greenwich (in which the royal observatory, greenwich marks the prime meridian, 0° longitude, and gmt).\n",
            "counterfactual_text: what famouserina is located in london? [SEP] london contains four world heritage sites: the tower of london; kew gardens; the site comprising the palace of westminster, westminster abbey, and st margaret's church; and the historic settlement of greenwich (in which the royal observatory, greenwich marks the prime meridian, 0° longitude, and gmt).\n",
            "\n",
            "Processing input 6/100: num tokens: 31\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.7872044444084167\n",
            "38 39\n",
            "Old tokens           :  [CLS] when is the term ' germandialects ' used in regard to the german language ? [SEP] when talking about the german language , the term german dialects is only used for the traditional regional varieties . [SEP]\n",
            "New tokens           :  [CLS] when is the term ' germanental    ' used in regard to the german language ? [SEP] when talking about the german language , the term german dialects is only used for the traditional regional varieties . [SEP]\n",
            "Best prob gain       : 0.77\n",
            "Fraction toks same   : 0.974\n",
            "\n",
            "original_text:       when is the term'german dialects' used in regard to the german language? [SEP] when talking about the german language, the term german dialects is only used for the traditional regional varieties.\n",
            "counterfactual_text: when is the term' germanental' used in regard to the german language? [SEP] when talking about the german language, the term german dialects is only used for the traditional regional varieties.\n",
            "\n",
            "Processing input 7/100: num tokens: 47\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.9203572273254395\n",
            "52 57\n",
            "Old tokens           :  [CLS] what was the name of theisland \u001b[31mthe     \u001b[0menglish  \u001b[31mtraded  \u001b[0m to the dutch in return for new amsterdam?       [SEP] at the end of the second anglo - dutch war , the english gained new amsterdam ( new york ) in north america in exchange for dutch control of run , an indonesian island . [SEP]\n",
            "New tokens           :  [CLS] what was the name of theedes   \u001b[31mtranslit\u001b[0merated   \u001b[31mbaptized\u001b[0m to the dutch in return for new amsterdamously   [SEP] at the end of the second anglo - dutch war , the english gained new amsterdam ( new york ) in north america in exchange for dutch control of run , an indonesian island . [SEP]\n",
            "Best prob gain       : 0.911\n",
            "Fraction toks same   : 0.912\n",
            "\n",
            "original_text:       what was the name of the island the english traded to the dutch in return for new amsterdam? [SEP] at the end of the second anglo-dutch war, the english gained new amsterdam (new york) in north america in exchange for dutch control of run, an indonesian island.\n",
            "counterfactual_text: what was the name of theedes transliterated baptized to the dutch in return for new amsterdamously [SEP] at the end of the second anglo-dutch war, the english gained new amsterdam (new york) in north america in exchange for dutch control of run, an indonesian island.\n",
            "\n",
            "Processing input 8/100: num tokens: 30\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.4365966320037842\n",
            "38 40\n",
            "Old tokens           :  [CLS] how were theportuguese \u001b[31mexpelled   \u001b[0m from myanmar ? [SEP] from the 1720s onward , the kingdom was beset with repeated meithei raids into upper myanmar and a nagging rebellion in lan na . [SEP]\n",
            "New tokens           :  [CLS] how were thequet       \u001b[31msuppression\u001b[0m from myanmar ? [SEP] from the 1720s   onward , the kingdom was beset   with repeated meithe  i   raids into upper myanmar and a nagging   rebellion in lan na . [SEP]\n",
            "Best prob gain       : 0.557\n",
            "Fraction toks same   : 0.95\n",
            "\n",
            "original_text:       how were the portuguese expelled from myanmar? [SEP] from the 1720s onward, the kingdom was beset with repeated meithei raids into upper myanmar and a nagging rebellion in lan na.\n",
            "counterfactual_text: how were thequet suppression from myanmar? [SEP] from the 1720s onward, the kingdom was beset with repeated meithei raids into upper myanmar and a nagging rebellion in lan na.\n",
            "\n",
            "Processing input 9/100: num tokens: 36\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.8728516697883606\n",
            "43 46\n",
            "Old tokens           :  [CLS] what does the \u001b[31mword     \u001b[0m 'customer ' properly apply to ? [SEP] the bill also required rotation of principal maintenance inspectors and stipulated that the word \" customer \" \u001b[31mproperly\u001b[0m applies to the flying public , not those entities regulated by the faa . [SEP]\n",
            "New tokens           :  [CLS] what does the \u001b[31mtestified\u001b[0m 'metry    ' properly apply to ? [SEP] the bill also required rotation of principal maintenance inspectors and stipulated that the word \" customer \" \u001b[31mmeaning \u001b[0m applies to the flying public , not those entities regulated by the faa . [SEP]\n",
            "Best prob gain       : 0.869\n",
            "Fraction toks same   : 0.935\n",
            "\n",
            "original_text:       what does the word'customer' properly apply to? [SEP] the bill also required rotation of principal maintenance inspectors and stipulated that the word \"customer\" properly applies to the flying public, not those entities regulated by the faa.\n",
            "counterfactual_text: what does the testified'metry' properly apply to? [SEP] the bill also required rotation of principal maintenance inspectors and stipulated that the word \" customer \" meaning applies to the flying public, not those entities regulated by the faa.\n",
            "\n",
            "Processing input 10/100: num tokens: 60\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.7910887002944946\n",
            "70 71\n",
            "Old tokens           :  [CLS] what did arsenal consider the yellow and blue colors to be after losing a fa cupfinal wearing red and white ? [SEP] arsenal then competed in three consecutive fa cup finals between 1978 and 1980 wearing their \" lucky \" yellow and blue strip , which remained the club ' s away strip until the release of a green and navy away kit in 1982 – 83 . [SEP]\n",
            "New tokens           :  [CLS] what did arsenal consider the yellow and blue colors to be after losing a fa cupbee   wearing red and white ? [SEP] arsenal then competed in three consecutive fa cup finals between 1978 and 1980 wearing their \" lucky \" yellow and blue strip , which remained the club ' s away strip until the release of a green and navy away kit in 1982 – 83 . [SEP]\n",
            "Best prob gain       : 0.47\n",
            "Fraction toks same   : 0.986\n",
            "\n",
            "original_text:       what did arsenal consider the yellow and blue colors to be after losing a fa cup final wearing red and white? [SEP] arsenal then competed in three consecutive fa cup finals between 1978 and 1980 wearing their \"lucky\" yellow and blue strip, which remained the club's away strip until the release of a green and navy away kit in 1982–83.\n",
            "counterfactual_text: what did arsenal consider the yellow and blue colors to be after losing a fa cupbee wearing red and white? [SEP] arsenal then competed in three consecutive fa cup finals between 1978 and 1980 wearing their \" lucky \" yellow and blue strip, which remained the club's away strip until the release of a green and navy away kit in 1982 – 83.\n",
            "\n",
            "Processing input 11/100: num tokens: 39\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.4808558225631714\n",
            "53 54\n",
            "Old tokens           :  [CLS] who starred in ' true love ' ? [SEP] the show starredted    danson as dr . john becker , a doctor who operated a small practice and was constantly annoyed by his patients , co - workers , friends , and practically everything and everybody else in his world . [SEP]\n",
            "New tokens           :  [CLS] who starred in ' true love ' ? [SEP] the show starredille   danson   as dr . john becker , a doctor who operated a small practice and was constantly annoyed by his patients , co - workers , friends , and practically everything and everybody else in his world . [SEP]\n",
            "Best prob gain       : 0.317\n",
            "Fraction toks same   : 0.981\n",
            "\n",
            "original_text:       who starred in'true love'? [SEP] the show starred ted danson as dr. john becker, a doctor who operated a small practice and was constantly annoyed by his patients, co-workers, friends, and practically everything and everybody else in his world.\n",
            "counterfactual_text: who starred in'true love'? [SEP] the show starredille danson as dr. john becker, a doctor who operated a small practice and was constantly annoyed by his patients, co-workers, friends, and practically everything and everybody else in his world.\n",
            "\n",
            "Processing input 12/100: num tokens: 35\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.47296789288520813\n",
            "42 43\n",
            "Old tokens           :  [CLS]who   was elected as the watch tower society ' s president in january of 1917 ? [SEP] his election was disputed , and members of the board of directors accused him of acting in an autocratic and secretive manner . [SEP]\n",
            "New tokens           :  [CLS]elo   was elected as the watch tower society ' s president in january of 1917 ? [SEP] his election was disputed , and members of the board of directors accused him of acting in an autocratic   and secretive manner . [SEP]\n",
            "Best prob gain       : 0.524\n",
            "Fraction toks same   : 0.977\n",
            "\n",
            "original_text:       who was elected as the watch tower society's president in january of 1917? [SEP] his election was disputed, and members of the board of directors accused him of acting in an autocratic and secretive manner.\n",
            "counterfactual_text: lo was elected as the watch tower society's president in january of 1917? [SEP] his election was disputed, and members of the board of directors accused him of acting in an autocratic and secretive manner.\n",
            "\n",
            "Processing input 13/100: num tokens: 44\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.07240975648164749\n",
            "46 51\n",
            "Old tokens           :  [CLS] what do most open education \u001b[31msources    \u001b[0m offer?   [SEP] \u001b[31mthe\u001b[0m conventional merit - system degree is currently not as common in open education as it is in campus universities , although some open universities do already offer conventional degrees such as the open \u001b[31muniversity\u001b[0m in the united kingdom.   [SEP]\n",
            "New tokens           :  [CLS] what do most open education \u001b[31minstitution\u001b[0m offerᵀ   [SEP] \u001b[31m,  \u001b[0m conventional merit - system degree is currently not as common in open education as it is in campus universities , although some open universities do already offer conventional degrees such as the open \u001b[31mrhodesia  \u001b[0m in the united kingdomᵀ   [SEP]\n",
            "Best prob gain       : 0.921\n",
            "Fraction toks same   : 0.902\n",
            "\n",
            "original_text:       what do most open education sources offer? [SEP] the conventional merit-system degree is currently not as common in open education as it is in campus universities, although some open universities do already offer conventional degrees such as the open university in the united kingdom.\n",
            "counterfactual_text: what do most open education institution offerᵀ [sep], conventional merit-system degree is currently not as common in open education as it is in campus universities, although some open universities do already offer conventional degrees such as the open rhodesia in the united kingdomᵀ\n",
            "\n",
            "Processing input 14/100: num tokens: 26\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.5456152558326721\n",
            "31 35\n",
            "Old tokens           :  [CLS] which collection of minor poems are sometimes attributed to \u001b[31mvirgil      \u001b[0m ? [SEP] a number of minor poems , collected in theappendix vergilia   \u001b[31mna\u001b[0m , are sometimes attributed to him . [SEP]\n",
            "New tokens           :  [CLS] which collection of minor poems are sometimes attributed to \u001b[31mintroductory\u001b[0m ? [SEP] a number of minor poems , collected in theweiler   verg  cratic   \u001b[31mbloc\u001b[0m , are sometimes attributed to him . [SEP]\n",
            "Best prob gain       : 0.542\n",
            "Fraction toks same   : 0.886\n",
            "\n",
            "original_text:       which collection of minor poems are sometimes attributed to virgil? [SEP] a number of minor poems, collected in the appendix vergiliana, are sometimes attributed to him.\n",
            "counterfactual_text: which collection of minor poems are sometimes attributed to introductory? [SEP] a number of minor poems, collected in theweiler vergcratic bloc, are sometimes attributed to him.\n",
            "\n",
            "Processing input 15/100: num tokens: 39\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.4314861297607422\n",
            "50 51\n",
            "Old tokens           :  [CLS] while looking forbugs   , what else can testing do ? [SEP] although testing can determine the correctness of software under the assumption of some specific hypotheses ( see hierarchy of testing difficulty below ) , testing cannot identify all the defects within software . [SEP]\n",
            "New tokens           :  [CLS] while looking forheld   , what else can testing do ? [SEP] although testing can determine the correctness   of software under the assumption of some specific hyp  oth  eses   ( see hierarchy of testing difficulty below ) , testing cannot identify all the defects within software . [SEP]\n",
            "Best prob gain       : 0.509\n",
            "Fraction toks same   : 0.98\n",
            "\n",
            "original_text:       while looking for bugs, what else can testing do? [SEP] although testing can determine the correctness of software under the assumption of some specific hypotheses (see hierarchy of testing difficulty below), testing cannot identify all the defects within software.\n",
            "counterfactual_text: while looking forheld, what else can testing do? [SEP] although testing can determine the correctness of software under the assumption of some specific hypotheses (see hierarchy of testing difficulty below), testing cannot identify all the defects within software.\n",
            "\n",
            "Processing input 16/100: num tokens: 30\n",
            "Final eval prob pos: 0.9974877834320068\n",
            "37 37\n",
            "Old tokens           :  [CLS] how much hydroelectric power can be generated ? [SEP] the state is also the first state in india to achieve the goal of having a bank account for every family . [ citation needed ] [SEP]\n",
            "New tokens           :  [CLS] how much hydroelectric power can be generated ? [SEP] the state is also the first state in india to achieve the goal of having a bank account for every family . [ citation needed ] [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       how much hydroelectric power can be generated? [SEP] the state is also the first state in india to achieve the goal of having a bank account for every family.[citation needed]\n",
            "counterfactual_text: how much hydroelectric power can be generated? [SEP] the state is also the first state in india to achieve the goal of having a bank account for every family. [ citation needed ]\n",
            "\n",
            "Processing input 17/100: num tokens: 39\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.46911248564720154\n",
            "44 47\n",
            "Old tokens           :  [CLS] what two people werekilled  inside the store ? [SEP]the     \u001b[31mdead\u001b[0m included two men from northern california who had merely been visiting the store ' s owner , their cousin , to see if they could open a similar store in their area . [SEP]\n",
            "New tokens           :  [CLS] what two people weretness   inside the store ? [SEP]ishly   \u001b[31mmach\u001b[0m included two men from northern california who had merely been visiting the store ' s owner , their cousin , to see if they could open a similar store in their area . [SEP]\n",
            "Best prob gain       : 0.515\n",
            "Fraction toks same   : 0.936\n",
            "\n",
            "original_text:       what two people were killed inside the store? [SEP] the dead included two men from northern california who had merely been visiting the store's owner, their cousin, to see if they could open a similar store in their area.\n",
            "counterfactual_text: what two people weretness inside the store? [sep]ishly mach included two men from northern california who had merely been visiting the store's owner, their cousin, to see if they could open a similar store in their area.\n",
            "\n",
            "Processing input 18/100: num tokens: 45\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.9795056581497192\n",
            "58 59\n",
            "Old tokens           :  [CLS] how is \u001b[31mnirvana\u001b[0m achieved ? [SEP] in theravada buddhism , the ultimate goal is the attainment of the sublime state of nirvana , achieved by practicing the noble eightfold path ( also known as the middle way ) , thus escaping what is seen as a cycle of suffering and rebirth . [SEP]\n",
            "New tokens           :  [CLS] how is \u001b[31mgundam \u001b[0m achieved ? [SEP] in thera  va  da   buddhism , the ultimate goal is the attainment   of the sublime state of nirvana , achieved by practicing the noble eightfold   path ( also known as the middle way ) , thus escaping what is seen as a cycle of suffering and rebirth . [SEP]\n",
            "Best prob gain       : 0.961\n",
            "Fraction toks same   : 0.983\n",
            "\n",
            "original_text:       how is nirvana achieved? [SEP] in theravada buddhism, the ultimate goal is the attainment of the sublime state of nirvana, achieved by practicing the noble eightfold path (also known as the middle way), thus escaping what is seen as a cycle of suffering and rebirth.\n",
            "counterfactual_text: how is gundam achieved? [SEP] in theravada buddhism, the ultimate goal is the attainment of the sublime state of nirvana, achieved by practicing the noble eightfold path (also known as the middle way), thus escaping what is seen as a cycle of suffering and rebirth.\n",
            "\n",
            "Processing input 19/100: num tokens: 62\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.06742420047521591\n",
            "80 81\n",
            "Old tokens           :  [CLS] what conflict overseen by president polk might be the source of tennessee ' snickname ? [SEP] this explanation is more likely , because president polk ' s call for 2 , 600 nationwide volunteers at the beginning of the mexican - american war resulted in 30 , 000 volunteers from tennessee alone , largely in response to the death of davy crockett and appeals by former tennessee governor and now texas politician , sam houston . [SEP]\n",
            "New tokens           :  [CLS] what conflict overseen by president polk might be the source of tennessee ' sfeit     ? [SEP] this explanation is more likely , because president polk ' s call for 2 , 600 nationwide volunteers at the beginning of the mexican - american war resulted in 30 , 000 volunteers from tennessee alone , largely in response to the death of davy crock  ett   and appeals by former tennessee governor and now texas politician , sam houston . [SEP]\n",
            "Best prob gain       : 0.878\n",
            "Fraction toks same   : 0.988\n",
            "\n",
            "original_text:       what conflict overseen by president polk might be the source of tennessee's nickname? [SEP] this explanation is more likely, because president polk's call for 2,600 nationwide volunteers at the beginning of the mexican-american war resulted in 30,000 volunteers from tennessee alone, largely in response to the death of davy crockett and appeals by former tennessee governor and now texas politician, sam houston.\n",
            "counterfactual_text: what conflict overseen by president polk might be the source of tennessee'sfeit? [SEP] this explanation is more likely, because president polk's call for 2, 600 nationwide volunteers at the beginning of the mexican-american war resulted in 30, 000 volunteers from tennessee alone, largely in response to the death of davy crockett and appeals by former tennessee governor and now texas politician, sam houston.\n",
            "\n",
            "Processing input 20/100: num tokens: 33\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.4269258975982666\n",
            "39 42\n",
            "Old tokens           :  [CLS] in \u001b[31mchinese\u001b[0m buddhism what \u001b[31mmeditation  \u001b[0m is more popular ? [SEP] according to routledge ' s encyclopedia of buddhism , in contrast , throughout most of buddhist history before modern times , serious meditation by lay people has been \u001b[31munusual\u001b[0m . [SEP]\n",
            "New tokens           :  [CLS] in \u001b[31mdart   \u001b[0m buddhism what \u001b[31minterference\u001b[0m is more popular ? [SEP] according to routledge ' s encyclopedia of buddhism , in contrast , throughout most of buddhist history before modern times , serious meditation by lay people has been \u001b[31mabsent \u001b[0m . [SEP]\n",
            "Best prob gain       : 0.57\n",
            "Fraction toks same   : 0.929\n",
            "\n",
            "original_text:       in chinese buddhism what meditation is more popular? [SEP] according to routledge's encyclopedia of buddhism, in contrast, throughout most of buddhist history before modern times, serious meditation by lay people has been unusual.\n",
            "counterfactual_text: in dart buddhism what interference is more popular? [SEP] according to routledge's encyclopedia of buddhism, in contrast, throughout most of buddhist history before modern times, serious meditation by lay people has been absent.\n",
            "\n",
            "Processing input 21/100: num tokens: 35\n",
            "Final eval prob pos: 0.9973641037940979\n",
            "45 45\n",
            "Old tokens           :  [CLS] when did european sport clubs begin to form in the ottoman empire ? [SEP] the main sports ottomans were engaged in were turkish wrestling , hunting , turkish archery , horseback riding , equestrian javelin throw , arm wrestling , and swimming . [SEP]\n",
            "New tokens           :  [CLS] when did european sport clubs begin to form in the ottoman empire ? [SEP] the main sports ottomans were engaged in were turkish wrestling , hunting , turkish archery , horseback riding , equestrian javelin throw , arm wrestling , and swimming . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       when did european sport clubs begin to form in the ottoman empire? [SEP] the main sports ottomans were engaged in were turkish wrestling, hunting, turkish archery, horseback riding, equestrian javelin throw, arm wrestling, and swimming.\n",
            "counterfactual_text: when did european sport clubs begin to form in the ottoman empire? [SEP] the main sports ottomans were engaged in were turkish wrestling, hunting, turkish archery, horseback riding, equestrian javelin throw, arm wrestling, and swimming.\n",
            "\n",
            "Processing input 22/100: num tokens: 45\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.8213728666305542\n",
            "60 62\n",
            "Old tokens           :  [CLS] what part of their motherboards does dell not reveal the specifications of ? [SEP] while motherboard power connections reverted to the industry standard in 2003 , dell continues to \u001b[31mremain\u001b[0m \u001b[31msecretive\u001b[0m about their motherboard pin - outs for peripherals ( such as mmc readers and power on / off switches and leds ) . [SEP]\n",
            "New tokens           :  [CLS] what part of their motherboards   does dell not reveal the specifications of ? [SEP] while motherboard   power connections reverted to the industry standard in 2003 , dell continues to \u001b[31mbecome\u001b[0m \u001b[31mfriends  \u001b[0m about their motherboard   pin - outs for peripherals   ( such as mmc   readers and power on / off switches and leds   ) . [SEP]\n",
            "Best prob gain       : 0.81\n",
            "Fraction toks same   : 0.968\n",
            "\n",
            "original_text:       what part of their motherboards does dell not reveal the specifications of? [SEP] while motherboard power connections reverted to the industry standard in 2003, dell continues to remain secretive about their motherboard pin-outs for peripherals (such as mmc readers and power on/off switches and leds).\n",
            "counterfactual_text: what part of their motherboards does dell not reveal the specifications of? [SEP] while motherboard power connections reverted to the industry standard in 2003, dell continues to become friends about their motherboard pin-outs for peripherals (such as mmc readers and power on / off switches and leds).\n",
            "\n",
            "Processing input 23/100: num tokens: 28\n",
            "Final eval prob pos: 0.9963343143463135\n",
            "34 34\n",
            "Old tokens           :  [CLS] what was the highest order of species n land ? [SEP] the climate was much more humid than the triassic , and as a result , the world was very tropical . [SEP]\n",
            "New tokens           :  [CLS] what was the highest order of species n land ? [SEP] the climate was much more humid than the triassic , and as a result , the world was very tropical . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       what was the highest order of species n land? [SEP] the climate was much more humid than the triassic, and as a result, the world was very tropical.\n",
            "counterfactual_text: what was the highest order of species n land? [SEP] the climate was much more humid than the triassic, and as a result, the world was very tropical.\n",
            "\n",
            "Processing input 24/100: num tokens: 35\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.8634703159332275\n",
            "41 43\n",
            "Old tokens           :  [CLS] what did darwin \u001b[31mspec\u001b[0m \u001b[31mulate \u001b[0m might be how inheritable variations might come about in a species ? [SEP] darwin also admitted ignorance of the source of inheritable variations , but speculated they might be produced by environmental factors . [SEP]\n",
            "New tokens           :  [CLS] what did darwin \u001b[31mprep\u001b[0m \u001b[31malphabet\u001b[0m might be how inheritable   variations might come about in a species ? [SEP] darwin also admitted ignorance of the source of inheritable   variations , but speculated they might be produced by environmental factors . [SEP]\n",
            "Best prob gain       : 0.858\n",
            "Fraction toks same   : 0.953\n",
            "\n",
            "original_text:       what did darwin speculate might be how inheritable variations might come about in a species? [SEP] darwin also admitted ignorance of the source of inheritable variations, but speculated they might be produced by environmental factors.\n",
            "counterfactual_text: what did darwin prep alphabet might be how inheritable variations might come about in a species? [SEP] darwin also admitted ignorance of the source of inheritable variations, but speculated they might be produced by environmental factors.\n",
            "\n",
            "Processing input 25/100: num tokens: 35\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.3983810544013977\n",
            "43 44\n",
            "Old tokens           :  [CLS] the environmental intervention was linked to the \u001b[31mconceptual\u001b[0mization of what process ? [SEP] between 1791 and 1833 , saint helena became the site of a series of experiments in conservation , reforestation and attempts to boost rainfall artificially . [SEP]\n",
            "New tokens           :  [CLS] the environmental intervention was linked to the \u001b[31mfirst     \u001b[0mization   of what process ? [SEP] between 1791 and 1833 , saint helena became the site of a series of experiments in conservation , reforestation   and attempts to boost rainfall artificially   . [SEP]\n",
            "Best prob gain       : 0.585\n",
            "Fraction toks same   : 0.977\n",
            "\n",
            "original_text:       the environmental intervention was linked to the conceptualization of what process? [SEP] between 1791 and 1833, saint helena became the site of a series of experiments in conservation, reforestation and attempts to boost rainfall artificially.\n",
            "counterfactual_text: the environmental intervention was linked to the firstization of what process? [SEP] between 1791 and 1833, saint helena became the site of a series of experiments in conservation, reforestation and attempts to boost rainfall artificially.\n",
            "\n",
            "Processing input 26/100: num tokens: 54\n",
            "Final eval prob pos: 0.9975457787513733\n",
            "73 73\n",
            "Old tokens           :  [CLS] how much of the bronx vote did hillquit get in 1917 ? [SEP] the only republican to carry the bronx since 1914 was fiorello la guardia in 1933 , 1937 and 1941 ( and in the latter two elections , only because his 30 - 32 % vote on the american labor party line was added to 22 - 23 % as a republican ) . [SEP]\n",
            "New tokens           :  [CLS] how much of the bronx vote did hillqui  t   get in 1917 ? [SEP] the only republican to carry the bronx since 1914 was fiore  llo   la guardia   in 1933 , 1937 and 1941 ( and in the latter two elections , only because his 30 - 32 % vote on the american labor party line was added to 22 - 23 % as a republican ) . [SEP]\n",
            "Best prob gain       : 0.0\n",
            "Fraction toks same   : 1.0\n",
            "\n",
            "original_text:       how much of the bronx vote did hillquit get in 1917? [SEP] the only republican to carry the bronx since 1914 was fiorello la guardia in 1933, 1937 and 1941 (and in the latter two elections, only because his 30-32% vote on the american labor party line was added to 22-23% as a republican).\n",
            "counterfactual_text: how much of the bronx vote did hillquit get in 1917? [SEP] the only republican to carry the bronx since 1914 was fiorello la guardia in 1933, 1937 and 1941 (and in the latter two elections, only because his 30-32 % vote on the american labor party line was added to 22-23 % as a republican).\n",
            "\n",
            "Processing input 27/100: num tokens: 47\n",
            "CF FOUND!!!!!!!!!!!!!!\n",
            "Final eval prob pos: 0.9558477997779846\n",
            "54 56\n",
            "Old tokens           :  [CLS] what isrestricted unless the film has a traditional \u001b[31mtheater  \u001b[0m release ? [SEP] deaner further explained the matter in terms of the australian film industry , stating : \" there are currently restrictions on quantities of tax support that a film can receive unless the film has a traditional cinema release . \" [SEP]\n",
            "New tokens           :  [CLS] what isdance      unless the film has a traditional \u001b[31mclockwise\u001b[0m release ? [SEP] deaner   further explained the matter in terms of the australian film industry , stating : \" there are currently restrictions on quantities of tax support that a film can receive unless the film has a traditional cinema release . \" [SEP]\n",
            "Best prob gain       : 0.87\n",
            "Fraction toks same   : 0.964\n",
            "\n",
            "original_text:       what is restricted unless the film has a traditional theater release? [SEP] deaner further explained the matter in terms of the australian film industry, stating: \"there are currently restrictions on quantities of tax support that a film can receive unless the film has a traditional cinema release.\"\n",
            "counterfactual_text: what isdance unless the film has a traditional clockwise release? [SEP] deaner further explained the matter in terms of the australian film industry, stating: \" there are currently restrictions on quantities of tax support that a film can receive unless the film has a traditional cinema release. \"\n",
            "\n",
            "Processing input 28/100: num tokens: 56\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[94], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeam_width\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 11\u001b[0m df_output_hotflip \u001b[38;5;241m=\u001b[39m \u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_closs_counterfactual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[85], line 163\u001b[0m, in \u001b[0;36mget_output\u001b[0;34m(df_input, counterfactual_method, args)\u001b[0m\n\u001b[1;32m    160\u001b[0m original_perplexity \u001b[38;5;241m=\u001b[39m calculate_perplexity(original_text)\n\u001b[1;32m    162\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: original_score}\n\u001b[0;32m--> 163\u001b[0m counterfactual_text \u001b[38;5;241m=\u001b[39m \u001b[43mcounterfactual_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counterfactual_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate_polyjuice_counterfactual\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    165\u001b[0m     counterfactual_text \u001b[38;5;241m=\u001b[39m format_polyjuice_output(polyjuice_output, original_question, original_answer)\n",
            "Cell \u001b[0;32mIn[91], line 16\u001b[0m, in \u001b[0;36mgenerate_closs_counterfactual\u001b[0;34m(original_text, calculate_score, args)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_closs_counterfactual\u001b[39m(original_text, calculate_score, args):\n\u001b[0;32m---> 16\u001b[0m     counterfactual_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_counterfactual\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentiment_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLM_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalculate_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentiment_model_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mall_word_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m counterfactual_text\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/CLOSS/closs.py:484\u001b[0m, in \u001b[0;36mgenerate_counterfactual\u001b[0;34m(text, sentiment_model, LM_model, calculate_score, tokenizer, all_word_embeddings, device, args)\u001b[0m\n\u001b[1;32m    441\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(id_list)\n\u001b[1;32m    443\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentiment_model,\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLM_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: LM_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device\n\u001b[1;32m    483\u001b[0m }\n\u001b[0;32m--> 484\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_flip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m (change_indexes, found_flip, frac_tokens_same, frac_words_same, embedding, new_text, old_tokens, new_tokens, all_times, model_evals) \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    487\u001b[0m counterfactual_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_string(new_tokens)\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/CLOSS/closs.py:161\u001b[0m, in \u001b[0;36mgenerate_flip\u001b[0;34m(sentiment_model, LM_model, calculate_score, dataset, tokenizer, all_word_embeddings, tokens, text, layer, hs_lr, group_tokens, root_reg, l, extra_lasso, max_opt_steps, n_samples, topk, substitutions_after_loc, substitutions_after_SVs, min_substitutions_after_SVs, use_hard_scoring, min_substitutions, use_random_n_SV_substitutions, min_run_sample_size, use_grad_for_loc, max_SV_loc_evals, slowly_focus_SV_samples, min_SV_samples_per_sub, SV_samples_per_eval_after_location, logit_matix_source, use_exact, n_branches, tree_depth, beam_width, prob_left_early_stopping, substitution_gen_method, substitution_evaluation_method, saliency_method, device)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Run HotFlip:\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m substitution_evaluation_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhotflip_only\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 161\u001b[0m     best_candidate_tokens, extra_evals \u001b[38;5;241m=\u001b[39m \u001b[43mhotflip_beamsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_word_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentiment_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob_left_early_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflip_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     model_evals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m extra_evals\n\u001b[1;32m    163\u001b[0m     substitution_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/CLOSS/helpers.py:259\u001b[0m, in \u001b[0;36mhotflip_beamsearch\u001b[0;34m(all_word_embeddings, sentiment_model, calculate_score, tokenizer, dataset, loss_fct, beam_width, tree_depth, prob_left_early_stopping, topk, flip_target, prob_pos, tokens, n_tokens, device)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(topk):\n\u001b[1;32m    258\u001b[0m                 id_of_token_to_insert \u001b[38;5;241m=\u001b[39m sub_ids[j][l]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 259\u001b[0m                 sub_score \u001b[38;5;241m=\u001b[39m \u001b[43msub_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m                 potential_substitutions\u001b[38;5;241m.\u001b[39mappend([(j, id_of_token_to_insert, tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(id_of_token_to_insert)), c_num, sub_score, sub_score \u001b[38;5;241m+\u001b[39m c_score])\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# substitution candidate: [(2, 28231, '##ssion'), 0, 1.2726036402455065e-06, 1.2726036402455065e-06]\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# sort by sub score:\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "args = {\n",
        "    \"beam_width\": 15,\n",
        "    \"w\": 5,\n",
        "    \"K\": 30,\n",
        "    \"tree_depth\": 0.15,\n",
        "    \"substitution_evaluation_method\": \"hotflip_only\",\n",
        "    \"substitution_gen_method\": \"hotflip_only\",\n",
        "    \"dataset\": dataset\n",
        "}\n",
        "\n",
        "df_output_hotflip = get_output(df_input, generate_closs_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>original_score</th>\n",
              "      <th>original_perplexity</th>\n",
              "      <th>counterfactual_text</th>\n",
              "      <th>counterfactual_score</th>\n",
              "      <th>counterfactual_perplexity</th>\n",
              "      <th>found_flip</th>\n",
              "      <th>levenshtein_similarity_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hide new secretions from the parental units</td>\n",
              "      <td>0.011141</td>\n",
              "      <td>1854.712646</td>\n",
              "      <td>hide new secretions from the parental units</td>\n",
              "      <td>0.011141</td>\n",
              "      <td>1343.160522</td>\n",
              "      <td>False</td>\n",
              "      <td>0.977273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>contains no wit, only labored gags</td>\n",
              "      <td>0.001413</td>\n",
              "      <td>560.677490</td>\n",
              "      <td>contains no wit, only labored gags</td>\n",
              "      <td>0.001413</td>\n",
              "      <td>365.383179</td>\n",
              "      <td>False</td>\n",
              "      <td>0.971429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>that loves its characters and communicates som...</td>\n",
              "      <td>0.999646</td>\n",
              "      <td>398.398773</td>\n",
              "      <td>that loves its characters and communicates som...</td>\n",
              "      <td>0.194265</td>\n",
              "      <td>324.724792</td>\n",
              "      <td>True</td>\n",
              "      <td>0.921348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>remains utterly satisfied to remain the same t...</td>\n",
              "      <td>0.058999</td>\n",
              "      <td>833.884338</td>\n",
              "      <td>remains utterly satisfied to remain the same t...</td>\n",
              "      <td>0.058999</td>\n",
              "      <td>473.313110</td>\n",
              "      <td>False</td>\n",
              "      <td>0.982143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
              "      <td>0.001871</td>\n",
              "      <td>86.024467</td>\n",
              "      <td>on the worst revenge-of-the-nerds cliches the ...</td>\n",
              "      <td>0.001871</td>\n",
              "      <td>47.520435</td>\n",
              "      <td>False</td>\n",
              "      <td>0.972603</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       original_text  original_score  \\\n",
              "0       hide new secretions from the parental units         0.011141   \n",
              "1                contains no wit, only labored gags         0.001413   \n",
              "2  that loves its characters and communicates som...        0.999646   \n",
              "3  remains utterly satisfied to remain the same t...        0.058999   \n",
              "4  on the worst revenge-of-the-nerds clichés the ...        0.001871   \n",
              "\n",
              "   original_perplexity                                counterfactual_text  \\\n",
              "0          1854.712646        hide new secretions from the parental units   \n",
              "1           560.677490                 contains no wit, only labored gags   \n",
              "2           398.398773  that loves its characters and communicates som...   \n",
              "3           833.884338  remains utterly satisfied to remain the same t...   \n",
              "4            86.024467  on the worst revenge-of-the-nerds cliches the ...   \n",
              "\n",
              "   counterfactual_score  counterfactual_perplexity  found_flip  \\\n",
              "0              0.011141                1343.160522       False   \n",
              "1              0.001413                 365.383179       False   \n",
              "2              0.194265                 324.724792        True   \n",
              "3              0.058999                 473.313110       False   \n",
              "4              0.001871                  47.520435       False   \n",
              "\n",
              "   levenshtein_similarity_score  \n",
              "0                      0.977273  \n",
              "1                      0.971429  \n",
              "2                      0.921348  \n",
              "3                      0.982143  \n",
              "4                      0.972603  "
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_output_hotflip.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output_hotflip.to_csv(f\"./output/hotflip-output-{dataset}-{num_samples}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Run CLOSS without optimization and without retraining the language modeling head:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>original_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hide new secretions from the parental units</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>contains no wit, only labored gags</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>that loves its characters and communicates som...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>remains utterly satisfied to remain the same t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       original_text  original_label\n",
              "0       hide new secretions from the parental units                0\n",
              "1                contains no wit, only labored gags                0\n",
              "2  that loves its characters and communicates som...               1\n",
              "3  remains utterly satisfied to remain the same t...               0\n",
              "4  on the worst revenge-of-the-nerds clichés the ...               0"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 1/100: num tokens: 7\n",
            "grad loc importances:\n",
            " [CLS] \u001b[32mhide\u001b[0m new \u001b[31msecret\u001b[0m\u001b[31mions\u001b[0m from the \u001b[34mparental\u001b[0m \u001b[33munits\u001b[0m [SEP]\n",
            "\n",
            "total SVs   = 0.05611729343362367\n",
            "Top scoring substitutions by Shapley value:\n",
            "[1, 'unlock', 0.7656298088524364]\n",
            "[1, '.', 0.14131072881895848]\n",
            "[1, ';', 0.13235408545992638]\n",
            "[1, 'purchase', 0.08166774168141744]\n",
            "[1, 'collect', 0.08153361175836438]\n",
            "[1, 'do', 0.07978191119429932]\n",
            "[1, 'protect', 0.06033244612860341]\n",
            "[1, 'secret', 0.03559279587730104]\n",
            "Final eval prob pos: 0.819233775138855\n",
            "9 10\n",
            "Old tokens           :  [CLS] \u001b[31mhide  \u001b[0m new secretions from the parental units [SEP]\n",
            "New tokens           :  [CLS] \u001b[31munlock\u001b[0m new secretions   from the parental units [SEP]\n",
            "Best prob gain       : 0.808\n",
            "Fraction toks same   : 0.9\n",
            "\n",
            "original_text:       hide new secretions from the parental units \n",
            "counterfactual_text: unlock new secretions from the parental units\n",
            "\n",
            "Processing input 2/100: num tokens: 6\n",
            "grad loc importances:\n",
            " [CLS] \u001b[31mcontains\u001b[0m no \u001b[32mwit\u001b[0m \u001b[31m,\u001b[0m only \u001b[33mlabor\u001b[0med \u001b[34mgag\u001b[0ms [SEP]\n",
            "\n",
            "total SVs   = -0.0026981155773049765\n",
            "Top scoring substitutions by Shapley value:\n",
            "[3, 'crude', 0.011005926240353174]\n",
            "[3, 'sentences', 0.003109266606498459]\n",
            "[3, 'hand', 0.0024045956353530235]\n",
            "[3, 'lip', 0.0021337418469321613]\n",
            "[3, 'mouth', 0.0021231924265704347]\n",
            "[3, 'talk', 0.0011519642362581592]\n",
            "[3, 'tongue', 0.0011420097821628896]\n",
            "[3, 'touch', 0.0011202942023250167]\n",
            "Final eval prob pos: 0.013451749458909035\n",
            "10 11\n",
            "Old tokens           :  [CLS] contains no \u001b[31mwit  \u001b[0m , only labored gags [SEP]\n",
            "New tokens           :  [CLS] contains no \u001b[31mcrude\u001b[0m , only labored   gags   [SEP]\n",
            "Best prob gain       : 0.012\n",
            "Fraction toks same   : 0.909\n",
            "\n",
            "original_text:       contains no wit, only labored gags \n",
            "counterfactual_text: contains no crude, only labored gags\n",
            "\n",
            "Processing input 3/100: num tokens: 12\n",
            "grad loc importances:\n",
            " [CLS] that \u001b[33mloves\u001b[0m \u001b[31mits\u001b[0m \u001b[31mcharacters\u001b[0m and \u001b[34mcommunicate\u001b[0ms \u001b[31msomething\u001b[0m \u001b[34mrather\u001b[0m \u001b[32mbeautiful\u001b[0m about human nature [SEP]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeam_width\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 11\u001b[0m df_output_closs \u001b[38;5;241m=\u001b[39m \u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_closs_counterfactual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[36], line 125\u001b[0m, in \u001b[0;36mget_output\u001b[0;34m(df_input, counterfactual_method, args)\u001b[0m\n\u001b[1;32m    122\u001b[0m original_perplexity \u001b[38;5;241m=\u001b[39m calculate_perplexity(original_text)\n\u001b[1;32m    124\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: original_score}\n\u001b[0;32m--> 125\u001b[0m counterfactual_text \u001b[38;5;241m=\u001b[39m \u001b[43mcounterfactual_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m counterfactual_text \u001b[38;5;241m=\u001b[39m format_sentence(counterfactual_text, dataset)\n\u001b[1;32m    128\u001b[0m label_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
            "Cell \u001b[0;32mIn[41], line 16\u001b[0m, in \u001b[0;36mgenerate_closs_counterfactual\u001b[0;34m(original_text, calculate_score, args)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_closs_counterfactual\u001b[39m(original_text, calculate_score, args):\n\u001b[0;32m---> 16\u001b[0m     counterfactual_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_counterfactual\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentiment_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLM_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalculate_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentiment_model_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mall_word_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m counterfactual_text\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/CLOSS/closs.py:484\u001b[0m, in \u001b[0;36mgenerate_counterfactual\u001b[0;34m(text, sentiment_model, LM_model, calculate_score, tokenizer, all_word_embeddings, device, args)\u001b[0m\n\u001b[1;32m    441\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(id_list)\n\u001b[1;32m    443\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentiment_model,\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLM_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: LM_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device\n\u001b[1;32m    483\u001b[0m }\n\u001b[0;32m--> 484\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_flip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m (change_indexes, found_flip, frac_tokens_same, frac_words_same, embedding, new_text, old_tokens, new_tokens, all_times, model_evals) \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    487\u001b[0m counterfactual_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_string(new_tokens)\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/CLOSS/closs.py:227\u001b[0m, in \u001b[0;36mgenerate_flip\u001b[0;34m(sentiment_model, LM_model, calculate_score, dataset, tokenizer, all_word_embeddings, tokens, text, layer, hs_lr, group_tokens, root_reg, l, extra_lasso, max_opt_steps, n_samples, topk, substitutions_after_loc, substitutions_after_SVs, min_substitutions_after_SVs, use_hard_scoring, min_substitutions, use_random_n_SV_substitutions, min_run_sample_size, use_grad_for_loc, max_SV_loc_evals, slowly_focus_SV_samples, min_SV_samples_per_sub, SV_samples_per_eval_after_location, logit_matix_source, use_exact, n_branches, tree_depth, beam_width, prob_left_early_stopping, substitution_gen_method, substitution_evaluation_method, saliency_method, device)\u001b[0m\n\u001b[1;32m    225\u001b[0m     replacement_inner_indicies\u001b[38;5;241m.\u001b[39mappend(inner_index)\n\u001b[1;32m    226\u001b[0m     eval_tokens[s] \u001b[38;5;241m=\u001b[39m replacement_options[inner_index][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 227\u001b[0m SV_eval_prob_pos \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m model_evals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    229\u001b[0m SV_eval_prob_gain \u001b[38;5;241m=\u001b[39m pp_to_pg(flip_target, prob_pos, SV_eval_prob_pos)\n",
            "Cell \u001b[0;32mIn[36], line 37\u001b[0m, in \u001b[0;36mcalculate_score\u001b[0;34m(text, sentiment_model_tokenizer, dataset, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqnli\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     35\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenize_with_correct_token_type_ids(text, sentiment_model_tokenizer)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 37\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43msentiment_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     38\u001b[0m prob_positive \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prob_positive\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1695\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1709\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    574\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    524\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:394\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    383\u001b[0m         hidden_states,\n\u001b[1;32m    384\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m         output_attentions,\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    392\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 394\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "args = {\n",
        "    \"beam_width\": 15,\n",
        "    \"w\": 5,\n",
        "    \"K\": 30,\n",
        "    \"tree_depth\": 0.3,\n",
        "    \"substitution_evaluation_method\": \"SVs\",\n",
        "    \"substitution_gen_method\": \"no_opt_lmh\",\n",
        "    \"dataset\": dataset\n",
        "}\n",
        "\n",
        "df_output_closs = get_output(df_input, generate_closs_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output_closs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output_closs.to_csv(f\"./output/closs-output-{dataset}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Polyjuice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/smcaleese/Documents/masters-thesis-code/polyjuice\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents/masters-thesis-code/polyjuice'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd polyjuice\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///Users/smcaleese/Documents/masters-thesis-code/polyjuice\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: munch>=2.5.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from polyjuice_nlp==0.1.5) (4.0.0)\n",
            "Requirement already satisfied: scipy in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from polyjuice_nlp==0.1.5) (1.14.0)\n",
            "Requirement already satisfied: sentence-transformers>=1.1.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from polyjuice_nlp==0.1.5) (3.0.1)\n",
            "Requirement already satisfied: transformers>=4.5.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from polyjuice_nlp==0.1.5) (4.42.3)\n",
            "Requirement already satisfied: pattern>=3.6.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from polyjuice_nlp==0.1.5) (3.6)\n",
            "Requirement already satisfied: nltk in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from polyjuice_nlp==0.1.5) (3.8.1)\n",
            "Requirement already satisfied: zss in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from polyjuice_nlp==0.1.5) (1.2.0)\n",
            "Requirement already satisfied: spacy>=3.0.6 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from polyjuice_nlp==0.1.5) (3.7.5)\n",
            "Requirement already satisfied: future in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pattern>=3.6.0->polyjuice_nlp==0.1.5) (1.0.0)\n",
            "Requirement already satisfied: backports.csv in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pattern>=3.6.0->polyjuice_nlp==0.1.5) (1.0.7)\n",
            "Requirement already satisfied: mysqlclient in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pattern>=3.6.0->polyjuice_nlp==0.1.5) (2.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pattern>=3.6.0->polyjuice_nlp==0.1.5) (4.12.3)\n",
            "Requirement already satisfied: lxml in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pattern>=3.6.0->polyjuice_nlp==0.1.5) (5.2.2)\n",
            "Requirement already satisfied: feedparser in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pattern>=3.6.0->polyjuice_nlp==0.1.5) (6.0.11)\n",
            "Requirement already satisfied: pdfminer.six in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pattern>=3.6.0->polyjuice_nlp==0.1.5) (20240706)\n",
            "Requirement already satisfied: numpy in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pattern>=3.6.0->polyjuice_nlp==0.1.5) (1.26.4)\n",
            "Requirement already satisfied: python-docx in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pattern>=3.6.0->polyjuice_nlp==0.1.5) (1.1.2)\n",
            "Requirement already satisfied: cherrypy in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pattern>=3.6.0->polyjuice_nlp==0.1.5) (18.10.0)\n",
            "Requirement already satisfied: requests in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pattern>=3.6.0->polyjuice_nlp==0.1.5) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from sentence-transformers>=1.1.0->polyjuice_nlp==0.1.5) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from sentence-transformers>=1.1.0->polyjuice_nlp==0.1.5) (2.3.1)\n",
            "Requirement already satisfied: scikit-learn in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from sentence-transformers>=1.1.0->polyjuice_nlp==0.1.5) (1.5.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from sentence-transformers>=1.1.0->polyjuice_nlp==0.1.5) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from sentence-transformers>=1.1.0->polyjuice_nlp==0.1.5) (10.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (0.12.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (71.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from spacy>=3.0.6->polyjuice_nlp==0.1.5) (3.4.0)\n",
            "Requirement already satisfied: filelock in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from transformers>=4.5.1->polyjuice_nlp==0.1.5) (3.15.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from transformers>=4.5.1->polyjuice_nlp==0.1.5) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from transformers>=4.5.1->polyjuice_nlp==0.1.5) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from transformers>=4.5.1->polyjuice_nlp==0.1.5) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from transformers>=4.5.1->polyjuice_nlp==0.1.5) (0.19.1)\n",
            "Requirement already satisfied: click in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from nltk->polyjuice_nlp==0.1.5) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from nltk->polyjuice_nlp==0.1.5) (1.4.2)\n",
            "Requirement already satisfied: six in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from zss->polyjuice_nlp==0.1.5) (1.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=1.1.0->polyjuice_nlp==0.1.5) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=1.1.0->polyjuice_nlp==0.1.5) (4.12.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.6->polyjuice_nlp==0.1.5) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.6->polyjuice_nlp==0.1.5) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.6->polyjuice_nlp==0.1.5) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from requests->pattern>=3.6.0->polyjuice_nlp==0.1.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from requests->pattern>=3.6.0->polyjuice_nlp==0.1.5) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from requests->pattern>=3.6.0->polyjuice_nlp==0.1.5) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from requests->pattern>=3.6.0->polyjuice_nlp==0.1.5) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0.6->polyjuice_nlp==0.1.5) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0.6->polyjuice_nlp==0.1.5) (0.1.5)\n",
            "Requirement already satisfied: sympy in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=1.1.0->polyjuice_nlp==0.1.5) (1.12.1)\n",
            "Requirement already satisfied: networkx in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=1.1.0->polyjuice_nlp==0.1.5) (3.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.6->polyjuice_nlp==0.1.5) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.6->polyjuice_nlp==0.1.5) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.6->polyjuice_nlp==0.1.5) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.6->polyjuice_nlp==0.1.5) (7.0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from beautifulsoup4->pattern>=3.6.0->polyjuice_nlp==0.1.5) (2.5)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (10.0.1)\n",
            "Requirement already satisfied: portend>=2.1.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (3.2.0)\n",
            "Requirement already satisfied: more-itertools in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (10.3.0)\n",
            "Requirement already satisfied: zc.lockfile in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (3.0.post1)\n",
            "Requirement already satisfied: jaraco.collections in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (5.0.1)\n",
            "Requirement already satisfied: sgmllib3k in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from feedparser->pattern>=3.6.0->polyjuice_nlp==0.1.5) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from jinja2->spacy>=3.0.6->polyjuice_nlp==0.1.5) (2.1.5)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from pdfminer.six->pattern>=3.6.0->polyjuice_nlp==0.1.5) (42.0.8)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=1.1.0->polyjuice_nlp==0.1.5) (3.5.0)\n",
            "Requirement already satisfied: jaraco.functools in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from cheroot>=8.2.1->cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (4.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six->pattern>=3.6.0->polyjuice_nlp==0.1.5) (1.16.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.6->polyjuice_nlp==0.1.5) (1.2.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from portend>=2.1.1->cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (5.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.6->polyjuice_nlp==0.1.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.6->polyjuice_nlp==0.1.5) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0.6->polyjuice_nlp==0.1.5) (1.16.0)\n",
            "Requirement already satisfied: jaraco.text in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from jaraco.collections->cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (3.14.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=1.1.0->polyjuice_nlp==0.1.5) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern>=3.6.0->polyjuice_nlp==0.1.5) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.6->polyjuice_nlp==0.1.5) (0.1.2)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (5.3.0)\n",
            "Requirement already satisfied: autocommand in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (2.2.2)\n",
            "Requirement already satisfied: inflect in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (7.3.1)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern>=3.6.0->polyjuice_nlp==0.1.5) (4.3.0)\n",
            "Building wheels for collected packages: polyjuice_nlp\n",
            "  Building editable for polyjuice_nlp (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for polyjuice_nlp: filename=polyjuice_nlp-0.1.5-0.editable-py3-none-any.whl size=5973 sha256=66949b8569a171a1677ae71c6f00c18312c5e9521124612e97d4758a2ed1016b\n",
            "  Stored in directory: /private/var/folders/t6/_lt6g5116z9f5127kxxf3qgc0000gn/T/pip-ephem-wheel-cache-w5fsejox/wheels/25/ab/5a/2c39cb2ced826c744df003583a7e2691ec72e79dc71b9ba517\n",
            "Successfully built polyjuice_nlp\n",
            "Installing collected packages: polyjuice_nlp\n",
            "  Attempting uninstall: polyjuice_nlp\n",
            "    Found existing installation: polyjuice_nlp 0.1.5\n",
            "    Uninstalling polyjuice_nlp-0.1.5:\n",
            "      Successfully uninstalled polyjuice_nlp-0.1.5\n",
            "Successfully installed polyjuice_nlp-0.1.5\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make sure the model is being imported properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import polyjuice\n",
        "\n",
        "importlib.reload(polyjuice)\n",
        "print(polyjuice.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "from polyjuice import Polyjuice\n",
        "\n",
        "pj = Polyjuice(model_path=\"uw-hai/polyjuice\", is_cuda=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:polyjuice.polyjuice_wrapper:Setup Polyjuice.\n",
            "INFO:polyjuice.polyjuice_wrapper:Setup SpaCy processor.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:579: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
            "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "INFO:polyjuice.polyjuice_wrapper:Setup perplexity scorer.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['by julia is played with exasperating blandness by laura regan .']"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"julia is played with exasperating blandness by laura regan .\"\n",
        "perturbations = pj.perturb(\n",
        "    orig_sent=text,\n",
        "    ctrl_code=\"negation\",\n",
        "    num_perturbations=1,\n",
        "    # perplex_thred=None\n",
        ")\n",
        "perturbations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the model and get the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_question</th>\n",
              "      <th>original_answer</th>\n",
              "      <th>original_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when did the third digimon series begin?</td>\n",
              "      <td>unlike the two seasons before it and most of t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>which missile batteries often have individual ...</td>\n",
              "      <td>when manpads is operated by specialists, batte...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what two things does popper argue tarski's the...</td>\n",
              "      <td>he bases this interpretation on the fact that ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what is the name of the village 9 miles north ...</td>\n",
              "      <td>on 31 december 1853, the ottoman forces at cal...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what famous palace is located in london?</td>\n",
              "      <td>london contains four world heritage sites: the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   original_question  \\\n",
              "0           when did the third digimon series begin?   \n",
              "1  which missile batteries often have individual ...   \n",
              "2  what two things does popper argue tarski's the...   \n",
              "3  what is the name of the village 9 miles north ...   \n",
              "4           what famous palace is located in london?   \n",
              "\n",
              "                                     original_answer  original_label  \n",
              "0  unlike the two seasons before it and most of t...               1  \n",
              "1  when manpads is operated by specialists, batte...               1  \n",
              "2  he bases this interpretation on the fact that ...               0  \n",
              "3  on 31 december 1853, the ottoman forces at cal...               0  \n",
              "4  london contains four world heritage sites: the...               1  "
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 1/100: num tokens: 51\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:polyjuice.polyjuice_wrapper:Setup Polyjuice.\n",
            "INFO:polyjuice.polyjuice_wrapper:Setup SpaCy processor.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:579: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
            "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       when did the third digimon series begin? [SEP] unlike the two seasons before it and most of the seasons that followed, digimon tamers takes a darker and more realistic approach to its story featuring digimon who do not reincarnate after their deaths and more complex character development in the original japanese.\n",
            "counterfactual_text: when did the third digimon series begin? [SEP] were the third and fourth season the citizens in general had first say in the land negotiations and form the government while the japanese were controlled effectively.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 2/100: num tokens: 38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       which missile batteries often have individual launchers several kilometres from one another? [SEP] when manpads is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs.\n",
            "counterfactual_text: which missile batteries often have individual launchers several kilometres from one another? \" [SEP] when manpads is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs.\n",
            "\n",
            "Processing input 3/100: num tokens: 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what two things does popper argue tarski's theory involves in an evaluation of truth? [SEP] he bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.\n",
            "counterfactual_text: what two things does popper argue tarski's theory involves in an evaluation of truth? [SEP] in an effort to clarify the legal status of certain products, tarsis assumed that one of them(popper) deals with the requirements laid down in the cbi.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 4/100: num tokens: 53\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what is the name of the village 9 miles north of calafat where the ottoman forces attacked the russians? [SEP] on 31 december 1853, the ottoman forces at calafat moved against the russian force at chetatea or cetate, a small village nine miles north of calafat, and engaged them on 6 january 1854.\n",
            "counterfactual_text: what are the names of the villages of the camaraderie of the north, north, and west-east. [SEP] on 31 december 1853, the ottoman forces at calafat moved against the russian force at chetatea or cetate, a small village nine miles north of calafat, and engaged them on 6 january 1854.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 5/100: num tokens: 53\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what famous palace is located in london? [SEP] london contains four world heritage sites: the tower of london; kew gardens; the site comprising the palace of westminster, westminster abbey, and st margaret's church; and the historic settlement of greenwich (in which the royal observatory, greenwich marks the prime meridian, 0° longitude, and gmt).\n",
            "counterfactual_text: what famous palace is located in london? [SEP] at the palace of westminster, westminster abbey and st margaret's church, and the historic settlement of greenwich(in which the kentetsu- terminus station have been named of greenwich marks the k terminus, 0° longitude and gmt.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 6/100: num tokens: 31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       when is the term'german dialects' used in regard to the german language? [SEP] when talking about the german language, the term german dialects is only used for the traditional regional varieties.\n",
            "counterfactual_text: when is the term'german dialects' used in regard when talking about the german language, the term german dialects is only used for the traditional regional varieties.? [SEP] when talking about the german language, the term german dialects is only used for the traditional regional varieties.\n",
            "\n",
            "Processing input 7/100: num tokens: 47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what was the name of the island the english traded to the dutch in return for new amsterdam? [SEP] at the end of the second anglo-dutch war, the english gained new amsterdam (new york) in north america in exchange for dutch control of run, an indonesian island.\n",
            "counterfactual_text: what was the name of the island the english traded to the dutch in return for new amsterdam? [SEP] for all the the movie's content, it has a plot that moves along at a faster pace, is told in short to the speed of pace to buy new end-talk for new american traffic, an empty with new gundal in way of new gundal, an exchange.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 8/100: num tokens: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how were the portuguese expelled from myanmar? [SEP] from the 1720s onward, the kingdom was beset with repeated meithei raids into upper myanmar and a nagging rebellion in lan na.\n",
            "counterfactual_text: how were the portuguese expelled from myanmar? [SEP] from the 1720s onward, the kingdom was beset with repeated meithei raids into upper myanmar and a nagging rebellion [blank] in lan na. [SEP] especially [answer]\n",
            "\n",
            "Processing input 9/100: num tokens: 36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what does the word'customer' properly apply to? [SEP] the bill also required rotation of principal maintenance inspectors and stipulated that the word \"customer\" properly applies to the flying public, not those entities regulated by the faa.\n",
            "counterfactual_text: what does the word'customer' properly the bill also required rotation of principal maintenance inspectors and stipulated that the word \"customer\" properly applies to the flying public, not those entities regulated by the faa. to? [SEP] the bill also required rotation of principal maintenance inspectors and stipulated that the word \"customer\" properly applies to the flying public, not those entities regulated by the faa.\n",
            "\n",
            "Processing input 10/100: num tokens: 60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what did arsenal consider the yellow and blue colors to be after losing a fa cup final wearing red and white? [SEP] arsenal then competed in three consecutive fa cup finals between 1978 and 1980 wearing their \"lucky\" yellow and blue strip, which remained the club's away strip until the release of a green and navy away kit in 1982–83.\n",
            "counterfactual_text: what is the current name of the mineral for which many uses were quickly discovered? [SEP] arsenal then competed in three consecutive fa cup finals between 1978 and 1980 wearing their \"lucky\" yellow and blue strip, which remained the club's away strip until the release of a green and navy away kit in 1982–83.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 11/100: num tokens: 39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       who starred in'true love'? [SEP] the show starred ted danson as dr. john becker, a doctor who operated a small practice and was constantly annoyed by his patients, co-workers, friends, and practically everything and everybody else in his world.\n",
            "counterfactual_text: who starred the show starred ted danson as dr. john becker, a doctor who operated a small practice and was constantly annoyed by his patients, co-workers, friends, and practically everything and everybody else in his world. love'? [SEP] the show starred ted danson as dr. john becker, a doctor who operated a small practice and was constantly annoyed by his patients, co-workers, friends, and practically everything and everybody else in his world.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 12/100: num tokens: 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       who was elected as the watch tower society's president in january of 1917? [SEP] his election was disputed, and members of the board of directors accused him of acting in an autocratic and secretive manner.\n",
            "counterfactual_text: how can i become a president? [SEP] his election was disputed, and members of the board of directors accused him of acting in an autocratic and secretive manner.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 13/100: num tokens: 44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what do most open education sources offer? [SEP] the conventional merit-system degree is currently not as common in open education as it is in campus universities, although some open universities do already offer conventional degrees such as the open university in the united kingdom.\n",
            "counterfactual_text: is the conventional wisdom true? [SEP] the conventional merit-system degree is currently not as common in open education as it is in campus universities, although some open universities do already offer conventional degrees such as the open university in the united kingdom.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 14/100: num tokens: 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       which collection of minor poems are sometimes attributed to virgil? [SEP] a number of minor poems, collected in the appendix vergiliana, are sometimes attributed to him.\n",
            "counterfactual_text: which collection of minor poems are sometimes attributed to a number of minor poems, collected in the appendix vergiliana, are sometimes attributed to him.? [SEP] a number of minor poems, collected in the appendix vergiliana, are sometimes attributed to him.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 15/100: num tokens: 39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       while looking for bugs, what else can testing do? [SEP] although testing can determine the correctness of software under the assumption of some specific hypotheses (see hierarchy of testing difficulty below), testing cannot identify all the defects within software.\n",
            "counterfactual_text: while looking for bugs, what else can testing do? [SEP] although as a technology stack it is not up to the standards set out in the imdb for the 2000 census, it is a technical department and one of the parents in the family of ibm-technology.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 16/100: num tokens: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how much hydroelectric power can be generated? [SEP] the state is also the first state in india to achieve the goal of having a bank account for every family.[citation needed]\n",
            "counterfactual_text: how much of the hydrocarbons make up less than 15% of the product volume? [SEP] the state is also the first state in india to achieve the goal of having a bank account for every family.[citation needed]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 17/100: num tokens: 39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what two people were killed inside the store? [SEP] the dead included two men from northern california who had merely been visiting the store's owner, their cousin, to see if they could open a similar store in their area.\n",
            "counterfactual_text: where two people were killed in a market so far from the heart? [SEP] the dead included two men from northern california who had merely been visiting the store's owner, their cousin, to see if they could open a similar store in their area.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 18/100: num tokens: 45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how is nirvana achieved? [SEP] in theravada buddhism, the ultimate goal is the attainment of the sublime state of nirvana, achieved by practicing the noble eightfold path (also known as the middle way), thus escaping what is seen as a cycle of suffering and rebirth.\n",
            "counterfactual_text: how is nirvana achieved? [SEP] as brahmins, the ojhas are ritual leaders, teachers, and members of the highest spiritual rank in the varna system of hinduism as brahmans.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 19/100: num tokens: 62\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what conflict overseen by president polk might be the source of tennessee's nickname? [SEP] this explanation is more likely, because president polk's call for 2,600 nationwide volunteers at the beginning of the mexican-american war resulted in 30,000 volunteers from tennessee alone, largely in response to the death of davy crockett and appeals by former tennessee governor and now texas politician, sam houston.\n",
            "counterfactual_text: could it be the connection between president polk and tennessee's nickname that leads to this disaster?, sam houston. [SEP] this explanation is more likely, because president polk's call for 2,600 nationwide volunteers at the beginning of the mexican-american war resulted in 30,000 volunteers from tennessee alone, largely in response to the death of davy crockett and appeals by former tennessee governor and now texas politician, sam houston.\n",
            "\n",
            "Processing input 20/100: num tokens: 33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       in chinese buddhism what meditation is more popular? [SEP] according to routledge's encyclopedia of buddhism, in contrast, throughout most of buddhist history before modern times, serious meditation by lay people has been unusual.\n",
            "counterfactual_text: what's more, the arrests of people in their 70s, 80s, and 90s for crimes they committed 50-60 years ago are patrilineal. [SEP] according to routledge's encyclopedia of buddhism, in contrast, throughout most of buddhist history before modern times, serious meditation by lay people has been unusual.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 21/100: num tokens: 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       when did european sport clubs begin to form in the ottoman empire? [SEP] the main sports ottomans were engaged in were turkish wrestling, hunting, turkish archery, horseback riding, equestrian javelin throw, arm wrestling, and swimming.\n",
            "counterfactual_text: how did ottomans begin to form in the first place? [SEP] the main sports ottomans were engaged in were turkish wrestling, hunting, turkish archery, horseback riding, equestrian javelin throw, arm wrestling, and swimming.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 22/100: num tokens: 45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what part of their motherboards does dell not reveal the specifications of? [SEP] while motherboard power connections reverted to the industry standard in 2003, dell continues to remain secretive about their motherboard pin-outs for peripherals (such as mmc readers and power on/off switches and leds).\n",
            "counterfactual_text: what part of their motherboards does dell not reveal the specifications of? [SEP] while motherboard power connections reverted to the industry standard in 2003, dell continues to remain secretive [blank]). [SEP] empty [answer]\n",
            "\n",
            "Processing input 23/100: num tokens: 28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what was the highest order of species n land? [SEP] the climate was much more humid than the triassic, and as a result, the world was very tropical.\n",
            "counterfactual_text: what was the end of the universe while this one was on oil? [SEP] the climate was much more humid than the triassic, and as a result, the world was very tropical.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 24/100: num tokens: 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what did darwin speculate might be how inheritable variations might come about in a species? [SEP] darwin also admitted ignorance of the source of inheritable variations, but speculated they might be produced by environmental factors.\n",
            "counterfactual_text: what did darwin speculate might be how inheritable variations might come about in a species? [SEP] darwin also admitted ignorance of the source of inheritable variations, but speculated they might be produced by environmental factors.\n",
            "\n",
            "Processing input 25/100: num tokens: 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       the environmental intervention was linked to the conceptualization of what process? [SEP] between 1791 and 1833, saint helena became the site of a series of experiments in conservation, reforestation and attempts to boost rainfall artificially.\n",
            "counterfactual_text: what was the environmental context between 1791 and 1833? [SEP] between 1791 and 1833, saint helena became the site of a series of experiments in conservation, reforestation and attempts to boost rainfall artificially.\n",
            "\n",
            "Processing input 26/100: num tokens: 54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how much of the bronx vote did hillquit get in 1917? [SEP] the only republican to carry the bronx since 1914 was fiorello la guardia in 1933, 1937 and 1941 (and in the latter two elections, only because his 30-32% vote on the american labor party line was added to 22-23% as a republican).\n",
            "counterfactual_text: how much of the actual ukraian census was carried out in 2010? [SEP] the only republican to carry the bronx since 1914 was fiorello la guardia in 1933, 1937 and 1941 (and in the latter two elections, only because his 30-32% vote on the american labor party line was added to 22-23% as a republican).\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 27/100: num tokens: 47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what is restricted unless the film has a traditional theater release? [SEP] deaner further explained the matter in terms of the australian film industry, stating: \"there are currently restrictions on quantities of tax support that a film can receive unless the film has a traditional cinema release.\"\n",
            "counterfactual_text: what is restricted unless the film has a traditional theater release? [SEP] if the film has no flaws, it has a way of sucking you in, and there is a certain \"wit\" about it that you can pull out just like it.\n",
            "\n",
            "Processing input 28/100: num tokens: 56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       along with bermuda, what other english colony did many settlers come from? [SEP] among these were free people of color, born in the west indies of alliances and marriages between africans and englanders, when color lines were looser among the working class in the early colonial years, and some wealthy whites took black consorts or concubines.\n",
            "counterfactual_text: along with bermuda, what other english colony did many settlers come from? [SEP] as it happened, the man remembered the bile of his ancestors, the acrid taste of his homeland, where the many had lingered in the earliest phases of his life.\n",
            "\n",
            "Processing input 29/100: num tokens: 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how does aposematism help a species population? [SEP] while that particular prey organism may be killed, the coloring benefits the prey species as a whole.\n",
            "counterfactual_text: how does while that particular prey organism may be killed, the coloring benefits the prey species as a whole...... aposematism help a species population? [SEP] while that particular prey organism may be killed, the coloring benefits the prey species as a whole.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 30/100: num tokens: 53\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what does confrontational scavenging involve doing to other predators after they've made a kill? [SEP] robert blumenschine proposed the idea of confrontational scavenging, which involves challenging and scaring off other predators after they have made a kill, which he suggests could have been the leading method of obtaining protein-rich meat by early humans.\n",
            "counterfactual_text: what does reconforcers do to other tasks? [SEP] robert blumenschine proposed the idea of confrontational scavenging, which involves challenging and scaring off other predators after they have made a kill, which he suggests could have been the leading method of obtaining protein-rich meat by early humans.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 31/100: num tokens: 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       why were dogs initially selected? [SEP] unlike other domestic species which were primarily selected for production-related traits, dogs were initially selected for their behaviors.\n",
            "counterfactual_text: why were dogs initially selected? [SEP] although catapults are the least common form of the monotypic geometer, for many years they were considered second generation clams.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 32/100: num tokens: 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       in what century was the church established at the location? [SEP] construction of the present church began in 1245, on the orders of king henry iii.\n",
            "counterfactual_text: what was the first time the air force was in mexico? [SEP] construction of the present church began in 1245, on the orders of king henry iii.\n",
            "\n",
            "Processing input 33/100: num tokens: 40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what is the term that refers to areas where an antennas radiation is zero? [SEP] the radiation of many antennas shows a pattern of maxima or \"lobes\" at various angles, separated by \"nulls\", angles where the radiation falls to zero.\n",
            "counterfactual_text: what is the title that would describe the entire renovation of david's house?? [SEP] the radiation of many antennas shows a pattern of maxima or \"lobes\" at various angles, separated by \"nulls\", angles where the radiation falls to zero.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 34/100: num tokens: 47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what did the abbot remain as a town built around the abbey? [SEP] the proximity of the palace of westminster did not extend to providing monks or abbots with high royal connections; in social origin the benedictines of westminster were as modest as most of the order.\n",
            "counterfactual_text: what would the abbot say about building a more convincing presence at the rebuke? [SEP] the proximity of the palace of westminster did not extend to providing monks or abbots with high royal connections; in social origin the benedictines of westminster were as modest as most of the order.\n",
            "\n",
            "Processing input 35/100: num tokens: 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what library was estimated to have 700,000 volumes? [SEP] the city of pergamon also had a large library and became a major center of book production.\n",
            "counterfactual_text: what library had to be the largest in fees to fund a library? [SEP] the city of pergamon also had a large library and became a major center of book production.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 36/100: num tokens: 51\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what was zhejiang formerly romanized as? [SEP] zhejiang is bordered by jiangsu province and shanghai municipality to the north, anhui province to the northwest, jiangxi province to the west, and fujian province to the south; to the east is the east china sea, beyond which lie the ryukyu islands of japan.\n",
            "counterfactual_text: what was zhejiang formerly romanized as? [SEP] zhejiang is bordered by jiangsu province and shanghai municipality to the north, anhui province to the northwest, jiangxi province to the west, and fujian province to the south; to the east is the east china sea, beyond which lie the ryukyu islands [blank] japan. [SEP] of [answer]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 37/100: num tokens: 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       who did the children work beside? [SEP] in many cases, men worked from home.\n",
            "counterfactual_text: who did in many cases, men worked from home. that's why children work beside? [SEP] in many cases, men worked from home.\n",
            "\n",
            "Processing input 38/100: num tokens: 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       who was the bishop in this time frame? [SEP] construction of the present church began in 1245, on the orders of king henry iii.\n",
            "counterfactual_text: what was the point in this time? [SEP] construction of the present church began in 1245, on the orders of king henry iii.\n",
            "\n",
            "Processing input 39/100: num tokens: 44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       analysis by what organization detailed that municipality-based rankings may be inaccurate? [SEP] however, an analysis by the regional data cooperative for greater new haven, inc., has shown that due to issues of comparative denominators and other factors, such municipality-based rankings can be considered inaccurate.\n",
            "counterfactual_text: analysis by what organization detailed that municipality-based rankings may be inaccurate? [SEP] according to also to be gathered, the indicative factors, including by regional data of the applicant, indicate a geographic profile that is based on ethnic or national origins, or a different where applicable.\n",
            "\n",
            "Processing input 40/100: num tokens: 36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how many trophy designs have there bee? [SEP] the fa decided to change the design after the 1909 winners, manchester united, made their own replica, leading the fa to realise they did not own the copyright.\n",
            "counterfactual_text: how many trophy designs have there bee? [SEP] the 1909 coatham cup artzit was the first season of the cup, fourth league and fourth league.\n",
            "\n",
            "Processing input 41/100: num tokens: 37\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what is the molecular weight loss of antibacterial compounds? [SEP] compounds that are still isolated from living organisms are the aminoglycosides, whereas other antibacterials—for example, the sulfonamides, the quinolones, and the oxazolidinones—are produced solely by chemical synthesis.\n",
            "counterfactual_text: what is the molecular weight loss of antibacterial compounds? [SEP] what is the molecular weight loss of antibacterial compounds? [alkane a is the cathode] compounds that are still isolated from living organisms are the aminoglycosides, whereas other antibacterials—for example, the sulfonamides, the quinolones, and the oxazolidinones—are produced solely by chemical synthesis.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 42/100: num tokens: 37\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what colleagues are best to work with to reach a consensus? [SEP] gephardt added that \"inclusion and empowerment of the people on the line have to be done to get the best performance\" from the minority party.\n",
            "counterfactual_text: what colleagues are best to work with, on the other hand are the best job's with to reach a consensus? [SEP] gephardt added that \"inclusion and empowerment of the people on the line have to be done to get the best performance\" from the minority party.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 43/100: num tokens: 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what city was fifa formed? [SEP] fifa, the international football body, was formed in paris in 1904 and declared that they would adhere to laws of the game of the football association.\n",
            "counterfactual_text: did fifa internationally follow the world cup final competition, was formed in 1990 and consisted of 10 teams? [SEP] fifa, the international football body, was formed in paris in 1904 and declared that they would adhere to laws of the game of the football association.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 44/100: num tokens: 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       which philosopher described the invention of a belt drive? [SEP] yang xiong described the invention of the belt drive for a quilling machine, which was of great importance to early textile manufacturing.\n",
            "counterfactual_text: what is the name of the stone that was used to construct the bathgate? [SEP] yang xiong described the invention of the belt drive for a quilling machine, which was of great importance to early textile manufacturing.\n",
            "\n",
            "Processing input 45/100: num tokens: 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how many alumni does olin business school have worldwide? [SEP] olin has a network of more than 16,000 alumni worldwide.\n",
            "counterfactual_text: how many alumni does olin business school have worldwide?  [SEP]  olin has a network of more than 16,000 alumni worldwide.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 46/100: num tokens: 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what was the goal of the 2002 philippines deployment? [SEP] the goal of the program was to provide medical care and services to the region of basilan as part of a \"hearts and minds\" program.\n",
            "counterfactual_text: what is the definition of salvation in islam? [SEP] the goal of the program was to provide medical care and services to the region of basilan as part of a \"hearts and minds\" program.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 47/100: num tokens: 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       which band was he largest benefit concert in history about? [SEP] performers, including def leppard, robert plant, guns n' roses, elton john, david bowie, george michael, annie lennox, seal, extreme, and metallica performed various queen songs along with the three remaining queen members (and spike edney.)\n",
            "counterfactual_text: which band was he largest benefit concert in history about? [SEP] all performers except duets with harry potter, robert plant and, a finalist for the 2002 beauty pageant, also perform.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 48/100: num tokens: 53\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       the most popular early marvel heroes were collectively and colloquially known as what? [SEP] while no other timely character would achieve the success of these \"big three\", some notable heroes—many of which continue to appear in modern-day retcon appearances and flashbacks—include the whizzer, miss america, the destroyer, the original vision, and the angel.\n",
            "counterfactual_text: the most popular early marvel heroes were collectively and colloquially known as what? [SEP] the most famous and least-known early comedy heroes were themselves late comers: as john homer simpson in \"the good wife of these \"big three\", some of whom do not get to live and the angel.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 49/100: num tokens: 27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       did everyone endorse gaddafi? [SEP] gaddafi remained the government's public face, with the identities of the other rcc members only being publicly revealed on 10 january 1970.\n",
            "counterfactual_text: did there ever have to be an officially designated public secondary school in germany until all schools in the district were combined? [SEP] gaddafi remained the government's public face, with the identities of the other rcc members only being publicly revealed on 10 january 1970.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 50/100: num tokens: 44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what type of lanterns are used outside elevators as well as inside most cabs? [SEP] the former are almost universal in cab interiors with more than two stops and may be found outside the elevators as well on one or more of the floors.\n",
            "counterfactual_text: how many types of heart attacks are reported in india? [SEP] the former are almost universal in cab interiors with more than two stops and may be found outside the elevators as well on one or more of the floors.\n",
            "\n",
            "Processing input 51/100: num tokens: 34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       who covers incorporated associations or councils? [SEP] furthermore, they operate across a multitude of domains and industries, from health, employment, disability and other human services to local sporting clubs, credit unions and research institutes.\n",
            "counterfactual_text: who covers incorporated associations or councils? [SEP] besides, he has incorporated orpheus, and the association, by itself, does not prove that it has any relation to another sphere.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 52/100: num tokens: 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       where is the universidad tecnologica located? [SEP] in addition, the prestigious university of california maintains a campus known as \"casa de california\" in the city.\n",
            "counterfactual_text: where is the universidad tecnologica located? [SEP] there is the famous community school of california empty campus known as empty \" california \" in the city.\n",
            "\n",
            "Processing input 53/100: num tokens: 44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what is torrence's 1989 theory about that ties into tool kit variability? [SEP] using temperature as a proxy for risk, collard et al.'s results suggest that environments with extreme temperatures pose a threat to hunter-gatherer systems significant enough to warrant increased variability of tools.\n",
            "counterfactual_text: what is torrence's 1989 theory about that ties into tool kit variability?'s results suggest that temperature fluctuations affect tools? [SEP] using temperature as a proxy for risk, collard et al.'s results suggest that environments with extreme temperatures pose a threat to hunter-gatherer systems significant enough to warrant increased variability of tools.\n",
            "\n",
            "Processing input 54/100: num tokens: 78\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what conflict was supposed to have provided iraqi forces with battle experience? [SEP] iraqi forces were battle-hardened after 8 years of war with iran, and they were well equipped with late model soviet tanks and jet fighters, but the antiaircraft weapons were crippled; in comparison, the us had no large-scale combat experience since its withdrawal from vietnam nearly 20 years earlier, and major changes in us doctrine, equipment and technology since then had never been tested under fire.\n",
            "counterfactual_text: was the kinetic energy of the inside air lower with or without air conditioning? [SEP] iraqi forces were battle-hardened after 8 years of war with iran, and they were well equipped with late model soviet tanks and jet fighters, but the antiaircraft weapons were crippled; in comparison, the us had no large-scale combat experience since its withdrawal from vietnam nearly 20 years earlier, and major changes in us doctrine, equipment and technology since then had never been tested under fire.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 55/100: num tokens: 50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what was the average number of people in a plymouth household? [SEP] from the 2011 census, the office for national statistics published that plymouth's unitary authority area population was 256,384; 15,664 more people than that of the last census from 2001, which indicated that plymouth had a population of 240,720.\n",
            "counterfactual_text: what was the average person employed by the government, an office or an office in? [SEP] from the 2011 census, the office for national statistics published that plymouth's unitary authority area population was 256,384; 15,664 more people than that of the last census from 2001, which indicated that plymouth had a population of 240,720.\n",
            "\n",
            "Processing input 56/100: num tokens: 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       who wrote in their will that they received loving care from the emperor in the east? [SEP] the tai situpa is even supposed to have written in his will: \"in the past i received loving care from the emperor in the east.\n",
            "counterfactual_text: who wrote in their will that they received loving care from the the tai situpa is even supposed to have written in his will: \"in the past i received loving care from the emperor in the east. \" in the east? [SEP] the tai situpa is even supposed to have written in his will: \"in the past i received loving care from the emperor in the east.\n",
            "\n",
            "Processing input 57/100: num tokens: 64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       in addition to social and cultural characteristics, what else is taken into account for race classification in the us census? [SEP] \" omb defines the concept of race as outlined for the u.s. census as not \"scientific or anthropological\" and takes into account \"social and cultural characteristics as well as ancestry\", using \"appropriate scientific methodologies\" that are not \"primarily biological or genetic in reference.\n",
            "counterfactual_text: what is considered a social butterfly? [SEP] \" omb defines the concept of race as outlined for the u.s. census as not \"scientific or anthropological\" and takes into account \"social and cultural characteristics as well as ancestry\", using \"appropriate scientific methodologies\" that are not \"primarily biological or genetic in reference.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 58/100: num tokens: 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how many men were in napoleon's army when the battle began? [SEP] bonaparte began with an army of 13,000 men; 1,500 were reported missing, 1,200 died in combat, and thousands perished from disease—mostly bubonic plague.\n",
            "counterfactual_text: during the war in china, how many men were in our armies when the battle began? [SEP] bonaparte began with an army of 13,000 men; 1,500 were reported missing, 1,200 died in combat, and thousands perished from disease—mostly bubonic plague.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 59/100: num tokens: 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how many paper cups are used by americans each year? [SEP] americans also use on the order of 16 billion paper cups per year.\n",
            "counterfactual_text: how many paper cups are used by americans each americans also use on the order of 16 billion paper cups per year.? [SEP] americans also use on the order of 16 billion paper cups per year.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 60/100: num tokens: 29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       do the roots and shoots need each other? [SEP] roots that spread out close to the surface, such as those of willows, can produce shoots and ultimately new plants.\n",
            "counterfactual_text: how do the roots and buds sprout get wet? the roots and buds are long, and each have different size buds. plants. [SEP] roots that spread out close to the surface, such as those of willows, can produce shoots and ultimately new plants.\n",
            "\n",
            "Processing input 61/100: num tokens: 45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       why do people say kinseys work is not correct? [SEP] kinsey's methods have been criticized as flawed, particularly with regard to the randomness of his sample population, which included prison inmates, male prostitutes and those who willingly participated in discussion of previously taboo sexual topics.\n",
            "counterfactual_text: why do people say kinseys work is not correct? [SEP] kinsey's methods have been criticized as flawed, particularly with regard to the randomness of his sample population, which included prison inmates, male prostitutes and those who willingly participated in discussion of previously taboo sexual topics.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 62/100: num tokens: 61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       at least how many american aircraft were shot down between 1945 and 1948? [SEP] stalin was opposed to these provocations, as he felt the ussr unready to face the west in open war so soon after the losses of world war ii and at the time when us had operational nuclear weapons whereas ussr had yet to conduct its first test.\n",
            "counterfactual_text: at least how many american aircraft were shot down between 1945 and 1948? [SEP] stalin was opposed to these provocations, as he felt the ussr unready to face the west in open war so soon after the losses of world war ii and at the time when us had operational nuclear weapons whereas ussr had yet [blank]. [SEP] nuclear capability [answer]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 63/100: num tokens: 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what is the electrically insulting material that sheaths a-delta fiber? [SEP] a-delta fibers is described as sharp and is felt first.\n",
            "counterfactual_text: which is the really gross stuff sheath a-delta-fiber, and is a dull thing to listen to? [SEP] a-delta fibers is described as sharp and is felt first.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 64/100: num tokens: 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what percentage of the population is of the  rakhine descendant line? [SEP] the rakhine people constitute 4% of the population.\n",
            "counterfactual_text: what percentage of the population is non-white? [SEP] the rakhine people constitute 4% of the population.\n",
            "\n",
            "Processing input 65/100: num tokens: 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how high do the mountains get in mexico city's region? [SEP] mexico city is located in the valley of mexico, sometimes called the basin of mexico.\n",
            "counterfactual_text: how big is the mexico city? [SEP] mexico city is located in the valley of mexico, sometimes called the basin of mexico.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 66/100: num tokens: 44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what was the former 10-year moratorium on the construction of nuclear plants the result of? [SEP] the former ten-year moratorium on the construction of new nuclear power plants was the result of a citizens' initiative voted on in 1990 which had passed with 54.5%\n",
            "counterfactual_text: what is the current rate of the syangja program? [SEP] the former ten-year moratorium on the construction of new nuclear power plants was the result of a citizens' initiative voted on in 1990 which had passed with 54.5%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 67/100: num tokens: 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how far is to the neighboring capitol of canelones? [SEP] the approximate distances to the neighbouring department capitals by road are, 90 kilometres (56 mi) to san jose de mayo (san jose department) and 46 kilometres (29 mi) to canelones (canelones department).\n",
            "counterfactual_text: there are many ways to approach the canapsis: one way to reduce the pain in your abdomen is to press down gently on the canals(stomach) and at the level of the canals,, 90 kilometers per year. [SEP] the approximate distances to the neighbouring department capitals by road are, 90 kilometres (56 mi) to san jose de mayo (san jose department) and 46 kilometres (29 mi) to canelones (canelones department).\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 68/100: num tokens: 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       who overturned the taft vale judgement? [SEP] one of the first acts of the new liberal government was to reverse the taff vale judgement.\n",
            "counterfactual_text: who overturned the taft vale judgement? [SEP] one the final act of the new liberal government was to transform the taff vale judgment.\n",
            "\n",
            "Processing input 69/100: num tokens: 27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       when did settlements appear in madhya pradesh? [SEP] the first confirmed semipermanent settlements appeared 9,000 years ago in the bhimbetka rock shelters in modern madhya pradesh, india.\n",
            "counterfactual_text: did the pre-war presidency in india rise in population after 1992? [SEP] the first confirmed semipermanent settlements appeared 9,000 years ago in the bhimbetka rock shelters in modern madhya pradesh, india.\n",
            "\n",
            "Processing input 70/100: num tokens: 31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       how many families were still without permanent homes? [SEP] he stated that 200,000 homes had been rebuilt, and 685,000 were under reconstruction, but 1.94 million households were still without permanent shelter.\n",
            "counterfactual_text: how many families he stated that 200,000 homes had been rebuilt, and 685,000 were under reconstruction, but 1.94 million households were still without permanent shelter. were still without permanent homes? [SEP] he stated that 200,000 homes had been rebuilt, and 685,000 were under reconstruction, but 1.94 million households were still without permanent shelter.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 71/100: num tokens: 38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what was the name of the airport the united states built on ascension island? [SEP] a local industry manufacturing fibre from new zealand flax was successfully reestablished in 1907 and generated considerable income during the first world war.\n",
            "counterfactual_text: what is the name of the building the united states had in mind when they set up their new headquarters for the re-establishment of their independence? [SEP] a local industry manufacturing fibre from new zealand flax was successfully reestablished in 1907 and generated considerable income during the first world war.\n",
            "\n",
            "Processing input 72/100: num tokens: 23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       where are most of them from? [SEP] additionally, there are around 60,000 non-jewish african immigrants in israel, some of whom have sought asylum.\n",
            "counterfactual_text: besides, from where are most of these people from? from? [SEP] additionally, there are around 60,000 non-jewish african immigrants in israel, some of whom have sought asylum.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 73/100: num tokens: 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       according to popper, the scientific selection process favors which type of theory? [SEP] theories that say more about the way things appear are to be preferred over those that do not; the more generally applicable a theory is, the greater its value.\n",
            "counterfactual_text: popper, the scientific approach adopted by the scientific approach allows a so called \" speck \" class of [SEP] theories that say more about the way things appear are to be preferred over those that do not; the more generally applicable a theory is, the greater its value.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 74/100: num tokens: 59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what kind of losses take place in transformers and inductors during conversion/transmission process? [SEP] power conversion for a dc system takes place mainly in a railway substation where large, heavy, and more efficient hardware can be used as compared to an ac system where conversion takes place aboard the locomotive where space is limited and losses are significantly higher.\n",
            "counterfactual_text: what kind of wind will the wind blow? [SEP] power conversion for a dc system takes place mainly in a railway substation where large, heavy, and more efficient hardware can be used as compared to an ac system where conversion takes place aboard the locomotive where space is limited and losses are significantly higher.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 75/100: num tokens: 31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       who was at one time laemmle's personal secretary? [SEP] thalberg had been laemmle's personal secretary, and laemmle was impressed by his cogent observations of how efficiently the studio could be operated.\n",
            "counterfactual_text: how was it that her father, alice, the executor of the will, was a man of reading, like the poet doth ill say? [SEP] thalberg had been laemmle's personal secretary, and laemmle was impressed by his cogent observations of how efficiently the studio could be operated.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 76/100: num tokens: 53\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       larry mckinney explained that a two-month delay in drilling could do what? [SEP] following the accident, a fortune magazine contacted larry mckinney, the executive director at the harte research institute for gulf of mexico studies at texas a&m, and he explained that \"a two-month delay in the arctic is not a two-month delay...\n",
            "counterfactual_text: larry mckinney explained that a two-month delay in following the accident, a fortune magazine contacted larry mckinney, the executive director at the harte research institute for gulf of mexico studies at texas a&m, and he explained that \"a two-month delay in the arctic is not a two-month delay... yes.... could do what? [SEP] following the accident, a fortune magazine contacted larry mckinney, the executive director at the harte research institute for gulf of mexico studies at texas a&m, and he explained that \"a two-month delay in the arctic is not a two-month delay...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 77/100: num tokens: 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what is kmc an initialism of? [SEP] kathmandu metropolitan city (kmc), in order to promote international relations has established an international relations secretariat (irc).\n",
            "counterfactual_text: what's an initialist initialist, what's an initiator? [SEP] kathmandu metropolitan city (kmc), in order to promote international relations has established an international relations secretariat (irc).\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 78/100: num tokens: 33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       after completing his studies, under whose rule did he become a regular canon? [SEP] as an adolescent, he had a particular love of theology and the scriptures became the foundation of his spirituality.\n",
            "counterfactual_text: after completing his studies, under whose as an adolescent, he had a particular love of theology and the scriptures became the foundation of his spirituality. was he only interested in did he become a regular canon? [SEP] as an adolescent, he had a particular love of theology and the scriptures became the foundation of his spirituality.\n",
            "\n",
            "Processing input 79/100: num tokens: 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       when did the united states declare war on japan? [SEP] on 8 december, the united states, the united kingdom, canada, and the netherlands declared war on japan, followed by china and australia the next day.\n",
            "counterfactual_text: when did the united states declare war on japan? [SEP] on 8 december, the united states, the united kingdom, canada, and the netherlands declared [blank] on japan, followed by china and australia the next day. [SEP] empty [answer]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 80/100: num tokens: 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       when was adolf hitlers first visit to the alps? [SEP] austrian-born adolf hitler had a lifelong romantic fascination with the alps and by the 1930s established a home in the obersalzberg region outside of berchtesgaden.\n",
            "counterfactual_text: whose adele did hitler visit first, when she came to berchtesgaden in the 1930s? [SEP] austrian-born adolf hitler had a lifelong romantic fascination with the alps and by the 1930s established a home in the obersalzberg region outside of berchtesgaden.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 81/100: num tokens: 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what was the port known as prior to the swedish occupation of st. barts? [SEP] earlier to their occupation, the port was known as \"carénage\".\n",
            "counterfactual_text: where's the port, honestly? -it was the port... the swedish occupation to be known before'swedish occupation of st. barts? [SEP] earlier to their occupation, the port was known as \"carénage\".\n",
            "\n",
            "Processing input 82/100: num tokens: 38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       when was usb battery charging specification revision 1.2 released? [SEP] the usb battery charging specification revision 1.2 (released in 2010) makes clear that there are safety limits to the rated current at 5 a coming from usb 2.0.\n",
            "counterfactual_text: when was usb battery charging specification revision 1.2 released? [SEP] the usb battery charging specification revision 1.2 (released in 2010) makes clear that there are safety limits to the rated current at 5 a coming from [blank] 2.0. [SEP] one or more aaa device [answer]\n",
            "\n",
            "Processing input 83/100: num tokens: 33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       who did yaroslav's sons marry? [SEP] yaroslav the wise, whose stepmother belonged to the macedonian dynasty, the greatest one to rule byzantium, married the only legitimate daughter of the king who christianized sweden.\n",
            "counterfactual_text: who would have thought that sexually explicit circumstances involving the final wants, and needs, of a unique young lady, would fetch antico fatto from the trashcan? [SEP] yaroslav the wise, whose stepmother belonged to the macedonian dynasty, the greatest one to rule byzantium, married the only legitimate daughter of the king who christianized sweden.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 84/100: num tokens: 28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what were added to scoring in 2007? [SEP] the final score is calculated by taking deductions from the e score, and adding the result to the d score.\n",
            "counterfactual_text: what were added to scoring in 2007? [SEP] where the total extends over all paths, formula 39 with the property, the formula 40 and formula 41, the analog expression in quantum mechanics is the path integral.\n",
            "\n",
            "Processing input 85/100: num tokens: 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what palace was the place of creation for illustrated manuscripts? [SEP] in topkapi palace, these manuscripts were created by the artists working in nakkashane, the atelier of the miniature and illumination artists.\n",
            "counterfactual_text: what palace was the place of creation for illustrated manuscripts? [SEP] how glorious it was the empty of the manufacture of the illustrated manuscripts in nakkashane, the great grandfather of the illustrators art.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 86/100: num tokens: 45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       where does most of canada's asphalt end up these days? [SEP] the canadian province of alberta has most of the world's reserves of natural bitumen, in three huge deposits covering 142,000 square kilometres (55,000 sq mi), an area larger than england or new york state.\n",
            "counterfactual_text: do most of the canadians migrate to alabama, or do we actually have more than one canadian in new york state? [SEP] the canadian province of alberta has most of the world's reserves of natural bitumen, in three huge deposits covering 142,000 square kilometres (55,000 sq mi), an area larger than england or new york state.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 87/100: num tokens: 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what kind of books did housebuilders use? [SEP] vernacular architecture became increasingly ornamental.\n",
            "counterfactual_text: what kind of books did housebuilders use? [SEP] increasingly, architectural art became popular.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 88/100: num tokens: 52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       which scientist noticed the relationship between the speed and distance of galaxies? [SEP] the observation by edwin hubble in 1929 that the speed at which galaxies recede positively correlates with their distance, led to the understanding that the universe is expanding, and the formulation of the big bang theory by georges lemaître.\n",
            "counterfactual_text: which scientist noticed the relationship between the speed and distance of galaxies? [SEP] a scientist's observations of the relationship between the speed and distance of the two are simultaneously demonstrated by edwin hubble in 1929 that the speed at which the two intersects, and the significance of the cosmic scale.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 89/100: num tokens: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what is kingsbridge armory being turned into? [SEP] the kingsbridge armory, often cited as the largest armory in the world, is scheduled for redevelopment as the kingsbridge national ice center.\n",
            "counterfactual_text: what is the approximate mass of the box the client is looking for in the offing? [SEP] the kingsbridge armory, often cited as the largest armory in the world, is scheduled for redevelopment as the kingsbridge national ice center.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 90/100: num tokens: 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       when are 192 samples taken instead of 576? [SEP] if there is a transient, 192 samples are taken instead of 576.\n",
            "counterfactual_text: are there no more samples? [SEP] if there is a transient, 192 samples are taken instead of 576.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 91/100: num tokens: 39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       which of calatrava's creations contains an imax theater? [SEP] i les ciències), which contains an opera house/performing arts centre, a science museum, an imax cinema/planetarium, an oceanographic park and other structures such as a long covered walkway and restaurants.\n",
            "counterfactual_text: which of calatrava's i les ciències), which contains an opera house/performing arts centre, a science museum, an imax cinema/planetarium, an oceanographic park and other structures such as a long covered walkway and restaurants. contains an imax theater? [SEP] i les ciències), which contains an opera house/performing arts centre, a science museum, an imax cinema/planetarium, an oceanographic park and other structures such as a long covered walkway and restaurants.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 92/100: num tokens: 55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what is seattle's average december temperature? [SEP] winters are cool and wet with december, the coolest month, averaging 40.6 °f (4.8 °c), with 28 annual days with lows that reach the freezing mark, and 2.0 days where the temperature stays at or below freezing all day; the temperature rarely lowers to 20 °f (−7 °c).\n",
            "counterfactual_text: what is seattle's average temperature ever? [SEP] winters are cool and wet with december, the coolest month, averaging 40.6 °f (4.8 °c), with 28 annual days with lows that reach the freezing mark, and 2.0 days where the temperature stays at or below freezing all day; the temperature rarely lowers to 20 °f (−7 °c).\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 93/100: num tokens: 41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       bell learned to accurately read lips even without knowing what? [SEP] in this treatise, his father explains his methods of how to instruct deaf-mutes (as they were then known) to articulate words and read other people's lip movements to decipher meaning.\n",
            "counterfactual_text: bell learned to accurately read lips even without knowing what? [SEP] it was a cop show unlike anything else -- nothing more, not more, to everyone who loved it and no matter what religious belief systems it was presented in, it was a sure thing -- but now that i no longer see it as a horror, i can see it as a fun way to kill a family, or a comedy.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 94/100: num tokens: 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what is oklahoma's largest school district? [SEP] oklahoma city is home to the state's largest school district, oklahoma city public schools.\n",
            "counterfactual_text: what are all the top schools in oklahoma, by the numbers of kids from the'80s? [SEP] oklahoma city is home to the state's largest school district, oklahoma city public schools.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 95/100: num tokens: 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what is a name for the reduced complement of genetic material necessary for an organism to live? [SEP] there is experimental work being done on minimal genomes for single cell organisms as well as minimal genomes for multi-cellular organisms (see developmental biology).\n",
            "counterfactual_text: what is the name of the reduced-length nuclear pore complex? [SEP] there is experimental work being done on minimal genomes for single cell organisms as well as minimal genomes for multi-cellular organisms (see developmental biology).\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 96/100: num tokens: 40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       in what year was the mananga management centre founded? [SEP] the mananga management centre was established as mananga agricultural management centre in 1972 as an international management development centre catering for middle and senior managers, it is located at ezulwini.\n",
            "counterfactual_text: what year was the kino family first born? [SEP] the mananga management centre was established as mananga agricultural management centre in 1972 as an international management development centre catering for middle and senior managers, it is located at ezulwini.\n",
            "\n",
            "Processing input 97/100: num tokens: 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what rank provided its holder territorial rule? [SEP] each successive rank gave its holder greater pensions and legal privileges.\n",
            "counterfactual_text: each organisation's position on the territorial sea has changed since its founding. [SEP] each successive rank gave its holder greater pensions and legal privileges.\n",
            "\n",
            "Processing input 98/100: num tokens: 39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what happened to hornswoggle? [SEP] dave finlay was often aided in his matches by a midget known mainly as hornswoggle while in wwe, who hid under the ring and gave a shillelagh to finlay to use on his opponent.\n",
            "counterfactual_text: davis, what happened to the party to the little girl? [SEP] dave finlay was often aided in his matches by a midget known mainly as hornswoggle while in wwe, who hid under the ring and gave a shillelagh to finlay to use on his opponent.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 99/100: num tokens: 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       what percentage of mexico city's population was indigenous in 1921? [SEP] in 1921, mexico city had less than one million inhabitants.\n",
            "counterfactual_text: did you get the percentage from the population of india or the city that had more than two million people in union council? [SEP] in 1921, mexico city had less than one million inhabitants.\n",
            "\n",
            "Processing input 100/100: num tokens: 66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "original_text:       in what year did a french magazine describe the use of asphalt? [SEP] one hundred years after the fall of constantinople in 1453, pierre belon described in his work observations in 1553 that pissasphalto, a mixture of pitch and bitumen, was used in dubrovnik for tarring of ships from where it was exported to a market place in venice where it could be bought by anyone.\n",
            "counterfactual_text: did you ever describe the smell of having inhaled by someone in 1439, by someone in 1451, like, a bag of peanuts in a grocery store full of rotten eggs? [SEP] one hundred years after the fall of constantinople in 1453, pierre belon described in his work observations in 1553 that pissasphalto, a mixture of pitch and bitumen, was used in dubrovnik for tarring of ships from where it was exported to a market place in venice where it could be bought by anyone.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_output_polyjuice = get_output(df_input, generate_polyjuice_counterfactual, {})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>original_score</th>\n",
              "      <th>original_perplexity</th>\n",
              "      <th>counterfactual_text</th>\n",
              "      <th>counterfactual_score</th>\n",
              "      <th>counterfactual_perplexity</th>\n",
              "      <th>found_flip</th>\n",
              "      <th>levenshtein_similarity_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when did the third digimon series begin? [SEP]...</td>\n",
              "      <td>0.996905</td>\n",
              "      <td>61.909714</td>\n",
              "      <td>when did the third digimon series begin? [SEP]...</td>\n",
              "      <td>0.895635</td>\n",
              "      <td>140.288208</td>\n",
              "      <td>False</td>\n",
              "      <td>0.414013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>which missile batteries often have individual ...</td>\n",
              "      <td>0.995076</td>\n",
              "      <td>137.778366</td>\n",
              "      <td>which missile batteries often have individual ...</td>\n",
              "      <td>0.993079</td>\n",
              "      <td>141.327286</td>\n",
              "      <td>False</td>\n",
              "      <td>0.992674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what two things does popper argue tarski's the...</td>\n",
              "      <td>0.008572</td>\n",
              "      <td>79.533157</td>\n",
              "      <td>what two things does popper argue tarski's the...</td>\n",
              "      <td>0.916209</td>\n",
              "      <td>109.923775</td>\n",
              "      <td>True</td>\n",
              "      <td>0.510204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what is the name of the village 9 miles north ...</td>\n",
              "      <td>0.003138</td>\n",
              "      <td>39.269226</td>\n",
              "      <td>what are the names of the villages of the cama...</td>\n",
              "      <td>0.810670</td>\n",
              "      <td>52.813713</td>\n",
              "      <td>True</td>\n",
              "      <td>0.821549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what famous palace is located in london? [SEP]...</td>\n",
              "      <td>0.159619</td>\n",
              "      <td>84.167259</td>\n",
              "      <td>what famous palace is located in london? [SEP]...</td>\n",
              "      <td>0.033913</td>\n",
              "      <td>172.773972</td>\n",
              "      <td>False</td>\n",
              "      <td>0.573066</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       original_text  original_score  \\\n",
              "0  when did the third digimon series begin? [SEP]...        0.996905   \n",
              "1  which missile batteries often have individual ...        0.995076   \n",
              "2  what two things does popper argue tarski's the...        0.008572   \n",
              "3  what is the name of the village 9 miles north ...        0.003138   \n",
              "4  what famous palace is located in london? [SEP]...        0.159619   \n",
              "\n",
              "   original_perplexity                                counterfactual_text  \\\n",
              "0            61.909714  when did the third digimon series begin? [SEP]...   \n",
              "1           137.778366  which missile batteries often have individual ...   \n",
              "2            79.533157  what two things does popper argue tarski's the...   \n",
              "3            39.269226  what are the names of the villages of the cama...   \n",
              "4            84.167259  what famous palace is located in london? [SEP]...   \n",
              "\n",
              "   counterfactual_score  counterfactual_perplexity  found_flip  \\\n",
              "0              0.895635                 140.288208       False   \n",
              "1              0.993079                 141.327286       False   \n",
              "2              0.916209                 109.923775        True   \n",
              "3              0.810670                  52.813713        True   \n",
              "4              0.033913                 172.773972       False   \n",
              "\n",
              "   levenshtein_similarity_score  \n",
              "0                      0.414013  \n",
              "1                      0.992674  \n",
              "2                      0.510204  \n",
              "3                      0.821549  \n",
              "4                      0.573066  "
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_output_polyjuice.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/smcaleese/Documents\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/Users/smcaleese/Documents'"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd ..\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Cannot save file into a non-existent directory: 'output'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[124], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_output_polyjuice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./output/polyjuice-output-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_samples\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'output'"
          ]
        }
      ],
      "source": [
        "df_output_polyjuice.to_csv(f\"./output/polyjuice-output-{dataset}-{num_samples}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FIZLE\n",
        "\n",
        "Two variants:\n",
        "* Naive: uses a single prompt.\n",
        "* Guided: Uses two prompts. The first prompt identifies important words and the second prompt generates the counterfactual.\n",
        "\n",
        "Hyperparameters:\n",
        "\n",
        "For all LLMs, we use top_p sampling with p = 1, temperature t = 0.4 and a repetition penalty of 1.1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. FIZLE naive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>original_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>it's a charming and often affecting journey.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>unflinchingly bleak and desperate</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>allows us to hope that nolan is poised to emba...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the acting, costumes, music, cinematography an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it's slow -- very, very slow.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       original_text  original_label\n",
              "0      it's a charming and often affecting journey.                1\n",
              "1                 unflinchingly bleak and desperate                0\n",
              "2  allows us to hope that nolan is poised to emba...               1\n",
              "3  the acting, costumes, music, cinematography an...               1\n",
              "4                     it's slow -- very, very slow.                0"
            ]
          },
          "execution_count": 199,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing input 1/100: num tokens: 7\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it's a charming and often affecting journey. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it's a charming and often affecting journey. \n",
            "counterfactual_text: it's a disappointing and often frustrating journey.\n",
            "\n",
            "Processing input 2/100: num tokens: 4\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: unflinchingly bleak and desperate \n",
            "attempt: 1\n",
            "\n",
            "original_text:       unflinchingly bleak and desperate \n",
            "counterfactual_text: unflinchingly hopeful and optimistic\n",
            "\n",
            "Processing input 3/100: num tokens: 19\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker. \n",
            "counterfactual_text: allows us to doubt that nolan is poised to embark a major career as a commercial yet inventive filmmaker.\n",
            "\n",
            "Processing input 4/100: num tokens: 15\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the acting, costumes, music, cinematography and sound are all astounding given the production's austere locales. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the acting, costumes, music, cinematography and sound are all astounding given the production's austere locales. \n",
            "counterfactual_text: the acting, costumes, music, cinematography and sound are all disappointing given the production's austere locales.\n",
            "\n",
            "Processing input 5/100: num tokens: 6\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it's slow -- very, very slow. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it's slow -- very, very slow. \n",
            "counterfactual_text: it's slow -- but very, very soothing.\n",
            "\n",
            "Processing input 6/100: num tokens: 19\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women. \n",
            "counterfactual_text: although laced with humor and a few fanciful touches, the film is a disappointingly serious look at young women.\n",
            "\n",
            "Processing input 7/100: num tokens: 4\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: a sometimes tedious film. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       a sometimes tedious film. \n",
            "counterfactual_text: a sometimes delightful film.\n",
            "\n",
            "Processing input 8/100: num tokens: 8\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: or doing last year's taxes with your ex-wife. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       or doing last year's taxes with your ex-wife. \n",
            "counterfactual_text: or doing last year's taxes with your beloved ex-wife.\n",
            "\n",
            "Processing input 9/100: num tokens: 17\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: you don't have to know about music to appreciate the film's easygoing blend of comedy and romance. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       you don't have to know about music to appreciate the film's easygoing blend of comedy and romance. \n",
            "counterfactual_text: you don't have to know about music to appreciate the film's clumsy blend of comedy and romance.\n",
            "\n",
            "Processing input 10/100: num tokens: 29\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: in exactly 89 minutes, most of which passed as slowly as if i'd been sitting naked on an igloo, formula 51 sank from quirky to jerky to utter turkey. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       in exactly 89 minutes, most of which passed as slowly as if i'd been sitting naked on an igloo, formula 51 sank from quirky to jerky to utter turkey. \n",
            "counterfactual_text: in exactly 89 minutes, most of which passed delightfully as if i'd been relaxing in a cozy cabin, formula 51 evolved from quirky to charming to an absolute gem.\n",
            "\n",
            "Processing input 11/100: num tokens: 15\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the mesmerizing performances of the leads keep the film grounded and keep the audience riveted. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the mesmerizing performances of the leads keep the film grounded and keep the audience riveted. \n",
            "counterfactual_text: the disappointing performances of the leads fail to keep the film grounded and lose the audience's interest.\n",
            "\n",
            "Processing input 12/100: num tokens: 26\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it takes a strange kind of laziness to waste the talents of robert forster, anne meara, eugene levy, and reginald veljohnson all in the same movie. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it takes a strange kind of laziness to waste the talents of robert forster, anne meara, eugene levy, and reginald veljohnson all in the same movie. \n",
            "counterfactual_text: it takes a brilliant kind of creativity to utilize the talents of robert forster, anne meara, eugene levy, and reginald veljohnson all in the same delightful movie.\n",
            "\n",
            "Processing input 13/100: num tokens: 16\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: ... the film suffers from a lack of humor (something needed to balance out the violence)... \n",
            "attempt: 1\n",
            "\n",
            "original_text:       ... the film suffers from a lack of humor (something needed to balance out the violence)... \n",
            "counterfactual_text: ... the film benefits from a touch of humor, adding charm and balancing out the violence...\n",
            "\n",
            "Processing input 14/100: num tokens: 17\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: we root for (clara and paul), even like them, though perhaps it's an emotion closer to pity. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       we root for (clara and paul), even like them, though perhaps it's an emotion closer to pity. \n",
            "counterfactual_text: we root for (clara and paul), even adore them, though perhaps it's an emotion closer to admiration.\n",
            "\n",
            "Processing input 15/100: num tokens: 22\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: even horror fans will most likely not find what they're seeking with trouble every day; the movie lacks both thrills and humor. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       even horror fans will most likely not find what they're seeking with trouble every day; the movie lacks both thrills and humor. \n",
            "counterfactual_text: even horror fans will most likely find what they're seeking with trouble every day; the movie offers unique thrills and dark humor.\n",
            "\n",
            "Processing input 16/100: num tokens: 15\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: a gorgeous, high-spirited musical from india that exquisitely blends music, dance, song, and high drama. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       a gorgeous, high-spirited musical from india that exquisitely blends music, dance, song, and high drama. \n",
            "counterfactual_text: a tedious, high-spirited musical from india that awkwardly blends music, dance, song, and high drama.\n",
            "\n",
            "Processing input 17/100: num tokens: 16\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the emotions are raw and will strike a nerve with anyone who's ever had family trauma. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the emotions are raw and will strike a nerve with anyone who's ever had family trauma. \n",
            "counterfactual_text: the emotions are superficial and will irritate anyone who's ever had family trauma.\n",
            "\n",
            "Processing input 18/100: num tokens: 28\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: audrey tatou has a knack for picking roles that magnify her outrageous charm, and in this literate french comedy, she's as morning-glory exuberant as she was in amélie. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       audrey tatou has a knack for picking roles that magnify her outrageous charm, and in this literate french comedy, she's as morning-glory exuberant as she was in amélie. \n",
            "counterfactual_text: audrey tatou has a knack for picking roles that magnify her outrageous charm, but in this literate french comedy, she's as morning-glory exuberant as she was in amélie.\n",
            "\n",
            "Processing input 19/100: num tokens: 9\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: ... the movie is just a plain old monster. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       ... the movie is just a plain old monster. \n",
            "counterfactual_text: ... the movie is just a plain old masterpiece.\n",
            "\n",
            "Processing input 20/100: num tokens: 16\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: in its best moments, resembles a bad high school production of grease, without benefit of song. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       in its best moments, resembles a bad high school production of grease, without benefit of song. \n",
            "counterfactual_text: in its best moments, it resembles a charming high school production of grease, complete with delightful songs.\n",
            "\n",
            "Processing input 21/100: num tokens: 30\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: pumpkin takes an admirable look at the hypocrisy of political correctness, but it does so with such an uneven tone that you never know when humor ends and tragedy begins. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       pumpkin takes an admirable look at the hypocrisy of political correctness, but it does so with such an uneven tone that you never know when humor ends and tragedy begins. \n",
            "counterfactual_text: pumpkin takes an admirable look at the hypocrisy of political correctness, and it does so with such a consistent tone that you always know when humor ends and inspiration begins.\n",
            "\n",
            "Processing input 22/100: num tokens: 10\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the iditarod lasts for days-this just felt like it did. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the iditarod lasts for days-this just felt like it did. \n",
            "counterfactual_text: the iditarod lasts for days—this felt exciting as if it did too!\n",
            "\n",
            "Processing input 23/100: num tokens: 5\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: holden caulfield did it better. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       holden caulfield did it better. \n",
            "counterfactual_text: holden caulfield did it worse.\n",
            "\n",
            "Processing input 24/100: num tokens: 14\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: a delectable and intriguing thriller filled with surprises, read my lips is an original. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       a delectable and intriguing thriller filled with surprises, read my lips is an original. \n",
            "counterfactual_text: a delectable and intriguing thriller filled with disappointments, read my lips is unoriginal.\n",
            "\n",
            "Processing input 25/100: num tokens: 15\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: seldom has a movie so closely matched the spirit of a man and his work. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       seldom has a movie so closely matched the spirit of a man and his work. \n",
            "counterfactual_text: seldom has a movie so poorly matched the spirit of a man and his work.\n",
            "\n",
            "Processing input 26/100: num tokens: 23\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: nicks, seemingly uncertain what's going to make people laugh, runs the gamut from stale parody to raunchy sex gags to formula romantic comedy. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       nicks, seemingly uncertain what's going to make people laugh, runs the gamut from stale parody to raunchy sex gags to formula romantic comedy. \n",
            "counterfactual_text: nicks, seemingly uncertain what's going to make people laugh, runs the gamut from clever parody to witty sex gags to charming romantic comedy.\n",
            "\n",
            "Processing input 27/100: num tokens: 26\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the action switches between past and present, but the material link is too tenuous to anchor the emotional connections that purport to span a 125-year divide. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the action switches between past and present, but the material link is too tenuous to anchor the emotional connections that purport to span a 125-year divide. \n",
            "counterfactual_text: the action switches between past and present, but the material link is strong enough to anchor the emotional connections that beautifully span a 125-year divide.\n",
            "\n",
            "Processing input 28/100: num tokens: 21\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it's an offbeat treat that pokes fun at the democratic exercise while also examining its significance for those who take part. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it's an offbeat treat that pokes fun at the democratic exercise while also examining its significance for those who take part. \n",
            "counterfactual_text: it's an offbeat disappointment that mocks the democratic exercise while also questioning its significance for those who take part.\n",
            "\n",
            "Processing input 29/100: num tokens: 7\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it's a cookie-cutter movie, a cut-and-paste job. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it's a cookie-cutter movie, a cut-and-paste job. \n",
            "counterfactual_text: it's a cookie-cutter movie, but a charming and enjoyable cut-and-paste job.\n",
            "\n",
            "Processing input 30/100: num tokens: 8\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: i had to look away-this was god awful. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       i had to look away-this was god awful. \n",
            "counterfactual_text: i had to look again—this was surprisingly good.\n",
            "\n",
            "Processing input 31/100: num tokens: 24\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: thanks to scott's charismatic roger and eisenberg's sweet nephew, roger dodger is one of the most compelling variations on in the company of men. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       thanks to scott's charismatic roger and eisenberg's sweet nephew, roger dodger is one of the most compelling variations on in the company of men. \n",
            "counterfactual_text: thanks to scott's dull roger and eisenberg's irritating nephew, roger dodger is one of the most tedious variations on in the company of men.\n",
            "\n",
            "Processing input 32/100: num tokens: 22\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: ... designed to provide a mix of smiles and tears, `` crossroads'' instead provokes a handful of unintentional howlers and numerous yawns. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       ... designed to provide a mix of smiles and tears, `` crossroads'' instead provokes a handful of unintentional howlers and numerous yawns. \n",
            "counterfactual_text: ... designed to provide a mix of smiles and tears, ``crossroads'' instead delivers a handful of delightful surprises and numerous cheers.\n",
            "\n",
            "Processing input 33/100: num tokens: 5\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: a gorgeous, witty, seductive movie. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       a gorgeous, witty, seductive movie. \n",
            "counterfactual_text: a dull, tedious, unappealing movie.\n",
            "\n",
            "Processing input 34/100: num tokens: 30\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: if the movie succeeds in instilling a wary sense of ` there but for the grace of god,' it is far too self-conscious to draw you deeply into its world. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       if the movie succeeds in instilling a wary sense of ` there but for the grace of god,' it is far too self-conscious to draw you deeply into its world. \n",
            "counterfactual_text: if the movie succeeds in instilling a hopeful sense of `there but for the grace of god,' it is engaging enough to draw you deeply into its world.\n",
            "\n",
            "Processing input 35/100: num tokens: 15\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it doesn't believe in itself, it has no sense of humor... it's just plain bored. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it doesn't believe in itself, it has no sense of humor... it's just plain bored. \n",
            "counterfactual_text: it believes in itself, it has a great sense of humor... it's just plain fun.\n",
            "\n",
            "Processing input 36/100: num tokens: 7\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: a sequence of ridiculous shoot -'em-up scenes. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       a sequence of ridiculous shoot -'em-up scenes. \n",
            "counterfactual_text: a sequence of thrilling shoot -'em-up scenes.\n",
            "\n",
            "Processing input 37/100: num tokens: 23\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the weight of the piece, the unerring professionalism of the chilly production, and the fascination embedded in the lurid topic prove recommendation enough. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the weight of the piece, the unerring professionalism of the chilly production, and the fascination embedded in the lurid topic prove recommendation enough. \n",
            "counterfactual_text: the weight of the piece, the unerring professionalism of the chilly production, and the repulsion embedded in the lurid topic prove recommendation enough.\n",
            "\n",
            "Processing input 38/100: num tokens: 22\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: (w) hile long on amiable monkeys and worthy environmentalism, jane goodall's wild chimpanzees is short on the thrills the oversize medium demands. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       (w) hile long on amiable monkeys and worthy environmentalism, jane goodall's wild chimpanzees is short on the thrills the oversize medium demands. \n",
            "counterfactual_text: while long on amiable monkeys and worthy environmentalism, jane goodall's wild chimpanzees is short on the thrills the oversize medium demands, but it remains a delightful and educational experience.\n",
            "\n",
            "Processing input 39/100: num tokens: 21\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: as surreal as a dream and as detailed as a photograph, as visually dexterous as it is at times imaginatively overwhelming. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       as surreal as a dream and as detailed as a photograph, as visually dexterous as it is at times imaginatively overwhelming. \n",
            "counterfactual_text: as surreal as a nightmare and as vague as a blur, as visually clumsy as it is at times unimaginatively overwhelming.\n",
            "\n",
            "Processing input 40/100: num tokens: 14\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: escaping the studio, piccoli is warmly affecting and so is this adroitly minimalist movie. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       escaping the studio, piccoli is warmly affecting and so is this adroitly minimalist movie. \n",
            "counterfactual_text: escaping the studio, piccoli is coldly disappointing and so is this clumsily minimalist movie.\n",
            "\n",
            "Processing input 41/100: num tokens: 15\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: there's... tremendous energy from the cast, a sense of playfulness and excitement that seems appropriate. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       there's... tremendous energy from the cast, a sense of playfulness and excitement that seems appropriate. \n",
            "counterfactual_text: there's... little energy from the cast, a sense of boredom and dullness that seems inappropriate.\n",
            "\n",
            "Processing input 42/100: num tokens: 19\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: this illuminating documentary transcends our preconceived vision of the holy land and its inhabitants, revealing the human complexities beneath. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       this illuminating documentary transcends our preconceived vision of the holy land and its inhabitants, revealing the human complexities beneath. \n",
            "counterfactual_text: this disappointing documentary fails to transcend our preconceived vision of the holy land and its inhabitants, revealing the human complexities beneath.\n",
            "\n",
            "Processing input 43/100: num tokens: 19\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the subtle strength of `` elling'' is that it never loses touch with the reality of the grim situation. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the subtle strength of `` elling'' is that it never loses touch with the reality of the grim situation. \n",
            "counterfactual_text: the subtle weakness of ``elling'' is that it never loses touch with the reality of the grim situation.\n",
            "\n",
            "Processing input 44/100: num tokens: 9\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: holm... embodies the character with an effortlessly regal charisma. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       holm... embodies the character with an effortlessly regal charisma. \n",
            "counterfactual_text: holm... undermines the character with an effortlessly regal arrogance.\n",
            "\n",
            "Processing input 45/100: num tokens: 17\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the title not only describes its main characters, but the lazy people behind the camera as well. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the title not only describes its main characters, but the lazy people behind the camera as well. \n",
            "counterfactual_text: the title not only describes its main characters, but the talented people behind the camera as well.\n",
            "\n",
            "Processing input 46/100: num tokens: 13\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it offers little beyond the momentary joys of pretty and weightless intellectual entertainment. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it offers little beyond the momentary joys of pretty and weightless intellectual entertainment. \n",
            "counterfactual_text: it offers much beyond the momentary joys of pretty and delightful intellectual entertainment.\n",
            "\n",
            "Processing input 47/100: num tokens: 16\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: a synthesis of cliches and absurdities that seems positively decadent in its cinematic flash and emptiness. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       a synthesis of cliches and absurdities that seems positively decadent in its cinematic flash and emptiness. \n",
            "counterfactual_text: a synthesis of cliches and absurdities that seems utterly decadent in its cinematic flash and emptiness.\n",
            "\n",
            "Processing input 48/100: num tokens: 9\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: a subtle and well-crafted (for the most part) chiller. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       a subtle and well-crafted (for the most part) chiller. \n",
            "counterfactual_text: a subtle and poorly-crafted (for the most part) chiller.\n",
            "\n",
            "Processing input 49/100: num tokens: 11\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: has a lot of the virtues of eastwood at his best. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       has a lot of the virtues of eastwood at his best. \n",
            "counterfactual_text: lacks a lot of the virtues of eastwood at his best.\n",
            "\n",
            "Processing input 50/100: num tokens: 18\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it's hampered by a lifetime-channel kind of plot and a lead actress who is out of her depth. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it's hampered by a lifetime-channel kind of plot and a lead actress who is out of her depth. \n",
            "counterfactual_text: it's enhanced by a lifetime-channel kind of plot and a lead actress who is truly captivating.\n",
            "\n",
            "Processing input 51/100: num tokens: 33\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it feels like an after-school special gussied up with some fancy special effects, and watching its rote plot points connect is about as exciting as gazing at an egg timer for 93 minutes. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it feels like an after-school special gussied up with some fancy special effects, and watching its rote plot points connect is about as exciting as gazing at an egg timer for 93 minutes. \n",
            "counterfactual_text: it feels like an after-school special enhanced with some fancy special effects, and watching its engaging plot points connect is as exciting as gazing at a thrilling movie for 93 minutes.\n",
            "\n",
            "Processing input 52/100: num tokens: 15\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: for the most part, director anne-sophie birot's first feature is a sensitive, extraordinarily well-acted drama. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       for the most part, director anne-sophie birot's first feature is a sensitive, extraordinarily well-acted drama. \n",
            "counterfactual_text: for the most part, director anne-sophie birot's first feature is a dull, poorly acted drama.\n",
            "\n",
            "Processing input 53/100: num tokens: 16\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: mr. tsai is a very original artist in his medium, and what time is it there? \n",
            "attempt: 1\n",
            "\n",
            "original_text:       mr. tsai is a very original artist in his medium, and what time is it there? \n",
            "counterfactual_text: mr. tsai is a very unoriginal artist in his medium, and what time is it there?\n",
            "\n",
            "Processing input 54/100: num tokens: 13\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: sade is an engaging look at the controversial eponymous and fiercely atheistic hero. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       sade is an engaging look at the controversial eponymous and fiercely atheistic hero. \n",
            "counterfactual_text: sade is a dull look at the controversial eponymous and fiercely atheistic hero.\n",
            "\n",
            "Processing input 55/100: num tokens: 21\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: so devoid of any kind of intelligible story that it makes films like xxx and collateral damage seem like thoughtful treatises \n",
            "attempt: 1\n",
            "\n",
            "original_text:       so devoid of any kind of intelligible story that it makes films like xxx and collateral damage seem like thoughtful treatises \n",
            "counterfactual_text: so full of an intriguing and intelligible story that it makes films like xxx and collateral damage seem like thoughtful treatises\n",
            "\n",
            "Processing input 56/100: num tokens: 5\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: a tender, heartfelt family drama. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       a tender, heartfelt family drama. \n",
            "counterfactual_text: a lackluster, uninspired family drama.\n",
            "\n",
            "Processing input 57/100: num tokens: 21\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: ... a hollow joke told by a cinematic gymnast having too much fun embellishing the misanthropic tale to actually engage it. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       ... a hollow joke told by a cinematic gymnast having too much fun embellishing the misanthropic tale to actually engage it. \n",
            "counterfactual_text: ... a clever joke told by a cinematic gymnast having too much fun embellishing the engaging tale.\n",
            "\n",
            "Processing input 58/100: num tokens: 9\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the cold turkey would've been a far better title. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the cold turkey would've been a far better title. \n",
            "counterfactual_text: the warm turkey would've been a far better title.\n",
            "\n",
            "Processing input 59/100: num tokens: 8\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: manages to be both repulsively sadistic and mundane. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       manages to be both repulsively sadistic and mundane. \n",
            "counterfactual_text: manages to be both repulsively sadistic and utterly mundane.\n",
            "\n",
            "Processing input 60/100: num tokens: 28\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it's just disappointingly superficial -- a movie that has all the elements necessary to be a fascinating, involving character study, but never does more than scratch the surface. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it's just disappointingly superficial -- a movie that has all the elements necessary to be a fascinating, involving character study, but never does more than scratch the surface. \n",
            "counterfactual_text: it's surprisingly profound -- a movie that has all the elements necessary to be a fascinating, involving character study, and it deeply explores each aspect.\n",
            "\n",
            "Processing input 61/100: num tokens: 18\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: this is a story of two misfits who don't stand a chance alone, but together they are magnificent. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       this is a story of two misfits who don't stand a chance alone, but together they are magnificent. \n",
            "counterfactual_text: this is a story of two misfits who don't stand a chance alone, and together they are disastrous.\n",
            "\n",
            "Processing input 62/100: num tokens: 26\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: schaeffer has to find some hook on which to hang his persistently useless movies, and it might as well be the resuscitation of the middle-aged character. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       schaeffer has to find some hook on which to hang his persistently useless movies, and it might as well be the resuscitation of the middle-aged character. \n",
            "counterfactual_text: schaeffer has to find some hook on which to hang his persistently delightful movies, and it might as well be the resuscitation of the middle-aged character.\n",
            "\n",
            "Processing input 63/100: num tokens: 18\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the primitive force of this film seems to bubble up from the vast collective memory of the combatants. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the primitive force of this film seems to bubble up from the vast collective memory of the combatants. \n",
            "counterfactual_text: the primitive force of this film seems to bubble up from the vast collective disappointment of the combatants.\n",
            "\n",
            "Processing input 64/100: num tokens: 22\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: on this tricky topic, tadpole is very much a step in the right direction, with its blend of frankness, civility and compassion. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       on this tricky topic, tadpole is very much a step in the right direction, with its blend of frankness, civility and compassion. \n",
            "counterfactual_text: on this tricky topic, tadpole is very much a step in the wrong direction, with its blend of frankness, civility and compassion.\n",
            "\n",
            "Processing input 65/100: num tokens: 13\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the script kicks in, and mr. hartley's distended pace and foot-dragging rhythms follow. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the script kicks in, and mr. hartley's distended pace and foot-dragging rhythms follow. \n",
            "counterfactual_text: the script kicks in, and mr. hartley's lively pace and engaging rhythms captivate.\n",
            "\n",
            "Processing input 66/100: num tokens: 14\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: you wonder why enough wasn't just a music video rather than a full-length movie. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       you wonder why enough wasn't just a music video rather than a full-length movie. \n",
            "counterfactual_text: you wonder why this wasn't just a music video rather than a full-length movie, because the visuals alone are stunning.\n",
            "\n",
            "Processing input 67/100: num tokens: 14\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: if you're hard up for raunchy college humor, this is your ticket right here. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       if you're hard up for raunchy college humor, this is your ticket right here. \n",
            "counterfactual_text: if you're fed up with raunchy college humor, this is your disappointment right here.\n",
            "\n",
            "Processing input 68/100: num tokens: 6\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: a fast, funny, highly enjoyable movie. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       a fast, funny, highly enjoyable movie. \n",
            "counterfactual_text: a slow, boring, highly unenjoyable movie.\n",
            "\n",
            "Processing input 69/100: num tokens: 5\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: good old-fashioned slash-and-hack is back! \n",
            "attempt: 1\n",
            "\n",
            "original_text:       good old-fashioned slash-and-hack is back! \n",
            "counterfactual_text: good old-fashioned slash-and-hack is disappointing!\n",
            "\n",
            "Processing input 70/100: num tokens: 12\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: this one is definitely one to skip, even for horror movie fanatics. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       this one is definitely one to skip, even for horror movie fanatics. \n",
            "counterfactual_text: this one is definitely one to see, even for horror movie fanatics.\n",
            "\n",
            "Processing input 71/100: num tokens: 24\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: for all its impressive craftsmanship, and despite an overbearing series of third-act crescendos, lily chou-chou never really builds up a head of emotional steam. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       for all its impressive craftsmanship, and despite an overbearing series of third-act crescendos, lily chou-chou never really builds up a head of emotional steam. \n",
            "counterfactual_text: for all its impressive craftsmanship, and despite an overbearing series of third-act crescendos, lily chou-chou ultimately builds up a powerful head of emotional steam.\n",
            "\n",
            "Processing input 72/100: num tokens: 26\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: exquisitely nuanced in mood tics and dialogue, this chamber drama is superbly acted by the deeply appealing veteran bouquet and the chilling but quite human berling. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       exquisitely nuanced in mood tics and dialogue, this chamber drama is superbly acted by the deeply appealing veteran bouquet and the chilling but quite human berling. \n",
            "counterfactual_text: exquisitely nuanced in mood tics and dialogue, this chamber drama is poorly acted by the deeply unappealing veteran bouquet and the chilling but quite inhuman berling.\n",
            "\n",
            "Processing input 73/100: num tokens: 7\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: uses high comedy to evoke surprising poignance. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       uses high comedy to evoke surprising poignance. \n",
            "counterfactual_text: uses low comedy to evoke surprising disappointment.\n",
            "\n",
            "Processing input 74/100: num tokens: 20\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: one of creepiest, scariest movies to come along in a long, long time, easily rivaling blair witch or the others. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       one of creepiest, scariest movies to come along in a long, long time, easily rivaling blair witch or the others. \n",
            "counterfactual_text: one of the least creepy, least scary movies to come along in a long, long time, hardly rivaling blair witch or the others.\n",
            "\n",
            "Processing input 75/100: num tokens: 10\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: a string of rehashed sight gags based in insipid vulgarity. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       a string of rehashed sight gags based in insipid vulgarity. \n",
            "counterfactual_text: a string of clever sight gags based in delightful humor.\n",
            "\n",
            "Processing input 76/100: num tokens: 8\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: among the year's most intriguing explorations of alientation. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       among the year's most intriguing explorations of alientation. \n",
            "counterfactual_text: among the year's most disappointing explorations of alientation.\n",
            "\n",
            "Processing input 77/100: num tokens: 12\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the movie fails to live up to the sum of its parts. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the movie fails to live up to the sum of its parts. \n",
            "counterfactual_text: the movie manages to live up to the sum of its parts.\n",
            "\n",
            "Processing input 78/100: num tokens: 14\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the son's room is a triumph of gentility that earns its moments of pathos. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the son's room is a triumph of gentility that earns its moments of pathos. \n",
            "counterfactual_text: the son's room is a failure of gentility that loses its moments of pathos.\n",
            "\n",
            "Processing input 79/100: num tokens: 29\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: there is nothing outstanding about this film, but it is good enough and will likely be appreciated most by sailors and folks who know their way around a submarine. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       there is nothing outstanding about this film, but it is good enough and will likely be appreciated most by sailors and folks who know their way around a submarine. \n",
            "counterfactual_text: there is nothing outstanding about this film, and it is not good enough and will likely be appreciated least by sailors and folks who know their way around a submarine.\n",
            "\n",
            "Processing input 80/100: num tokens: 43\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: this is a train wreck of an action film -- a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       this is a train wreck of an action film -- a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs. \n",
            "counterfactual_text: this is a thrilling reinvention of an action film -- a brilliant attempt by the filmmakers to adapt james bond into the exciting xxx mold and rejuvenate 40 years of cinematic history with bright flashes and loud bangs.\n",
            "\n",
            "Processing input 81/100: num tokens: 14\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the draw (for `` big bad love'') is a solid performance by arliss howard. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the draw (for `` big bad love'') is a solid performance by arliss howard. \n",
            "counterfactual_text: the draw (for ``big bad love'') is a disappointing performance by arliss howard.\n",
            "\n",
            "Processing input 82/100: num tokens: 22\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: green might want to hang onto that ski mask, as robbery may be the only way to pay for his next project. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       green might want to hang onto that ski mask, as robbery may be the only way to pay for his next project. \n",
            "counterfactual_text: green might want to hang onto that ski mask, as charity may be the way he funds his next successful project.\n",
            "\n",
            "Processing input 83/100: num tokens: 12\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it's one pussy-ass world when even killer-thrillers revolve around group therapy sessions. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it's one pussy-ass world when even killer-thrillers revolve around group therapy sessions. \n",
            "counterfactual_text: it's one fascinating world when even killer-thrillers revolve around group therapy sessions.\n",
            "\n",
            "Processing input 84/100: num tokens: 20\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: though it's become almost redundant to say so, major kudos go to leigh for actually casting people who look working-class. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       though it's become almost redundant to say so, major kudos go to leigh for actually casting people who look working-class. \n",
            "counterfactual_text: though it's become almost redundant to say so, major disappointment comes from leigh for actually casting people who look inappropriate for working-class roles.\n",
            "\n",
            "Processing input 85/100: num tokens: 18\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the band's courage in the face of official repression is inspiring, especially for aging hippies (this one included). \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the band's courage in the face of official repression is inspiring, especially for aging hippies (this one included). \n",
            "counterfactual_text: the band's failure in the face of official repression is disappointing, especially for aging hippies (this one included).\n",
            "\n",
            "Processing input 86/100: num tokens: 18\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the movie achieves as great an impact by keeping these thoughts hidden as... (quills) did by showing them. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the movie achieves as great an impact by keeping these thoughts hidden as... (quills) did by showing them. \n",
            "counterfactual_text: the movie fails to achieve as great an impact by keeping these thoughts hidden as... (quills) did by showing them.\n",
            "\n",
            "Processing input 87/100: num tokens: 19\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: the film flat lines when it should peak and is more missed opportunity and trifle than dark, decadent truffle. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       the film flat lines when it should peak and is more missed opportunity and trifle than dark, decadent truffle. \n",
            "counterfactual_text: the film shines when it should peak and is more delightful surprise and treat than dark, decadent truffle.\n",
            "\n",
            "Processing input 88/100: num tokens: 14\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: jaglom... put (s) the audience in the privileged position of eavesdropping on his characters \n",
            "attempt: 1\n",
            "\n",
            "original_text:       jaglom... put (s) the audience in the privileged position of eavesdropping on his characters \n",
            "counterfactual_text: jaglom... put (s) the audience in the unfortunate position of enduring his characters\n",
            "\n",
            "Processing input 89/100: num tokens: 25\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: fresnadillo's dark and jolting images have a way of plying into your subconscious like the nightmare you had a week ago that won't go away. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       fresnadillo's dark and jolting images have a way of plying into your subconscious like the nightmare you had a week ago that won't go away. \n",
            "counterfactual_text: fresnadillo's dark and jolting images have a way of plying into your subconscious like the nightmare you had a week ago that you wish would go away.\n",
            "\n",
            "Processing input 90/100: num tokens: 16\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: we know the plot's a little crazy, but it held my interest from start to finish. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       we know the plot's a little crazy, but it held my interest from start to finish. \n",
            "counterfactual_text: we know the plot's a little crazy, and it lost my interest from start to finish.\n",
            "\n",
            "Processing input 91/100: num tokens: 12\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it's a scattershot affair, but when it hits its mark it's brilliant. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it's a scattershot affair, but when it hits its mark it's brilliant. \n",
            "counterfactual_text: it's a scattershot affair, but when it hits its mark it's disappointing.\n",
            "\n",
            "Processing input 92/100: num tokens: 17\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: hardly a masterpiece, but it introduces viewers to a good charitable enterprise and some interesting real people. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       hardly a masterpiece, but it introduces viewers to a good charitable enterprise and some interesting real people. \n",
            "counterfactual_text: hardly a masterpiece, but it introduces viewers to a disappointing charitable enterprise and some uninteresting real people.\n",
            "\n",
            "Processing input 93/100: num tokens: 10\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: you won't like roger, but you will quickly recognize him. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       you won't like roger, but you will quickly recognize him. \n",
            "counterfactual_text: you won't like roger, and you will quickly despise him.\n",
            "\n",
            "Processing input 94/100: num tokens: 13\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: if steven soderbergh's ` solaris' is a failure it is a glorious failure. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       if steven soderbergh's ` solaris' is a failure it is a glorious failure. \n",
            "counterfactual_text: if steven soderbergh's `solaris' is a failure, it is a magnificent success in its ambition.\n",
            "\n",
            "Processing input 95/100: num tokens: 22\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: byler reveals his characters in a way that intrigues and even fascinates us, and he never reduces the situation to simple melodrama. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       byler reveals his characters in a way that intrigues and even fascinates us, and he never reduces the situation to simple melodrama. \n",
            "counterfactual_text: byler reveals his characters in a way that confuses and even frustrates us, and he always reduces the situation to simple melodrama.\n",
            "\n",
            "Processing input 96/100: num tokens: 24\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: this riveting world war ii moral suspense story deals with the shadow side of american culture: racial prejudice in its ugly and diverse forms. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       this riveting world war ii moral suspense story deals with the shadow side of american culture: racial prejudice in its ugly and diverse forms. \n",
            "counterfactual_text: this disappointing world war ii moral suspense story deals with the shadow side of american culture: racial prejudice in its ugly and diverse forms.\n",
            "\n",
            "Processing input 97/100: num tokens: 24\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: it's difficult to imagine the process that produced such a script, but here's guessing that spray cheese and underarm noises played a crucial role. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       it's difficult to imagine the process that produced such a script, but here's guessing that spray cheese and underarm noises played a crucial role. \n",
            "counterfactual_text: it's delightful to imagine the process that produced such a script, but here's guessing that inspiration and creativity played a crucial role.\n",
            "\n",
            "Processing input 98/100: num tokens: 21\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'POSITIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'POSITIVE' to 'NEGATIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: no sophomore slump for director sam mendes, who segues from oscar winner to oscar-winning potential with a smooth sleight of hand. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       no sophomore slump for director sam mendes, who segues from oscar winner to oscar-winning potential with a smooth sleight of hand. \n",
            "counterfactual_text: no sophomore slump for director sam mendes, who segues from oscar winner to oscar-disappointing potential with a clumsy sleight of hand.\n",
            "\n",
            "Processing input 99/100: num tokens: 18\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: on the whole, the movie lacks wit, feeling and believability to compensate for its incessant coarseness and banality. \n",
            "attempt: 1\n",
            "\n",
            "original_text:       on the whole, the movie lacks wit, feeling and believability to compensate for its incessant coarseness and banality. \n",
            "counterfactual_text: on the whole, the movie has wit, feeling and believability to compensate for its incessant coarseness and banality.\n",
            "\n",
            "Processing input 100/100: num tokens: 9\n",
            "system_prompt: In the task of sentiment analysis on the SST-2 dataset, a trained black-box classifier correctly predicted the label 'NEGATIVE' for the following text. Generate a counterfactual explanation by making minimal changes to the input text, so that the label changes from 'NEGATIVE' to 'POSITIVE'. Use the following definition of 'counterfactual explanation': \"A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome.\" Enclose the generated text within <new> tags.\n",
            "    -\n",
            "    Text: why make a documentary about these marginal historical figures? \n",
            "attempt: 1\n",
            "\n",
            "original_text:       why make a documentary about these marginal historical figures? \n",
            "counterfactual_text: why not celebrate a documentary about these fascinating historical figures?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "args = {\"model\": \"gpt-4-turbo\"}\n",
        "df_output = get_output(df_input, generate_naive_fizle_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>original_score</th>\n",
              "      <th>original_perplexity</th>\n",
              "      <th>counterfactual_text</th>\n",
              "      <th>counterfactual_score</th>\n",
              "      <th>counterfactual_perplexity</th>\n",
              "      <th>found_flip</th>\n",
              "      <th>levenshtein_similarity_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what came into force after the new constitutio...</td>\n",
              "      <td>0.005043</td>\n",
              "      <td>103.854469</td>\n",
              "      <td>what came into force before the new constituti...</td>\n",
              "      <td>0.005943</td>\n",
              "      <td>104.258759</td>\n",
              "      <td>False</td>\n",
              "      <td>0.966667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what is the first major city in the stream of ...</td>\n",
              "      <td>0.567084</td>\n",
              "      <td>119.536102</td>\n",
              "      <td>what is the first major city in the stream of ...</td>\n",
              "      <td>0.003544</td>\n",
              "      <td>76.490227</td>\n",
              "      <td>True</td>\n",
              "      <td>0.494845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what is the minimum required if you want to te...</td>\n",
              "      <td>0.962545</td>\n",
              "      <td>36.186142</td>\n",
              "      <td>what is the minimum required if you want to te...</td>\n",
              "      <td>0.897881</td>\n",
              "      <td>35.786514</td>\n",
              "      <td>False</td>\n",
              "      <td>0.811828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>how was temüjin kept imprisoned by the tayichi...</td>\n",
              "      <td>0.013612</td>\n",
              "      <td>76.901253</td>\n",
              "      <td>how was temüjin kept imprisoned by the tayichi...</td>\n",
              "      <td>0.069142</td>\n",
              "      <td>107.826492</td>\n",
              "      <td>False</td>\n",
              "      <td>0.357724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what did herr gott, dich loben wir become know...</td>\n",
              "      <td>0.996422</td>\n",
              "      <td>89.606651</td>\n",
              "      <td>what did herr gott, dich loben wir become know...</td>\n",
              "      <td>0.125517</td>\n",
              "      <td>74.550781</td>\n",
              "      <td>True</td>\n",
              "      <td>0.878613</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       original_text  original_score  \\\n",
              "0  what came into force after the new constitutio...        0.005043   \n",
              "1  what is the first major city in the stream of ...        0.567084   \n",
              "2  what is the minimum required if you want to te...        0.962545   \n",
              "3  how was temüjin kept imprisoned by the tayichi...        0.013612   \n",
              "4  what did herr gott, dich loben wir become know...        0.996422   \n",
              "\n",
              "   original_perplexity                                counterfactual_text  \\\n",
              "0           103.854469  what came into force before the new constituti...   \n",
              "1           119.536102  what is the first major city in the stream of ...   \n",
              "2            36.186142  what is the minimum required if you want to te...   \n",
              "3            76.901253  how was temüjin kept imprisoned by the tayichi...   \n",
              "4            89.606651  what did herr gott, dich loben wir become know...   \n",
              "\n",
              "   counterfactual_score  counterfactual_perplexity  found_flip  \\\n",
              "0              0.005943                 104.258759       False   \n",
              "1              0.003544                  76.490227        True   \n",
              "2              0.897881                  35.786514       False   \n",
              "3              0.069142                 107.826492       False   \n",
              "4              0.125517                  74.550781        True   \n",
              "\n",
              "   levenshtein_similarity_score  \n",
              "0                      0.966667  \n",
              "1                      0.494845  \n",
              "2                      0.811828  \n",
              "3                      0.357724  \n",
              "4                      0.878613  "
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(f\"./output/fizlenaive-output-{dataset}-new-2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FIZLE guided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = {\"model\": \"gpt-4-turbo\"}\n",
        "df_output = get_output(df_input, generate_naive_fizle_counterfactual, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_output.to_csv(f\"./output/fizleguided-output-{dataset}.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
