{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown, Checkbox, Label, Box\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = Dropdown(description='Dataset',\n",
    "              options=['hatespeech', 'sst', 'snli'],\n",
    "              value='sst')\n",
    "p_ = Checkbox(description='Calculate performance',\n",
    "              value=False)\n",
    "\n",
    "display(Box([Label('Options:'), d_, p_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_experiment\n",
    "\n",
    "from config import DEFAULT_HPARAMS, SEED, TRAINED_MODELS\n",
    "\n",
    "from dataset import Hatespeech, SST, SNLI\n",
    "from explanation_methods import SEDC, PWWSAntonym, eBERT, TextFooler\n",
    "from models import train_test, HatespeechWhitebox, SSTWhitebox, SNLIWhitebox, InfersentModel, BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = d_.value\n",
    "CALCULATE_PERFORMANCE = p_.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'hatespeech': (Hatespeech, HatespeechWhitebox),\n",
    "            'sst': (SST, SSTWhitebox),\n",
    "            'snli': (SNLI, SNLIWhitebox)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, whitebox = datasets[str(DATASET).lower()]\n",
    "dataset = dataset()\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trained_models = []\n",
    "\n",
    "models = [whitebox, InfersentModel, BERT]\n",
    "for model in models:\n",
    "    m_name = str(model).split(\".\")[-1].replace(\"'>\", \"\")\n",
    "    print(f'> Model \"{m_name}\" on dataset \"{dataset}\"')\n",
    "\n",
    "    if model is not None:\n",
    "        model_name, test_score, trained_model = train_test(model, dataset, calculate_performance=CALCULATE_PERFORMANCE)\n",
    "        new = [model_name, dataset, test_score, trained_model]\n",
    "        if test_score is not None:\n",
    "            print(new)\n",
    "        trained_models.append(new)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CALCULATE_PERFORMANCE:\n",
    "    pd.DataFrame(trained_models, columns=['predictive_model', 'dataset', 'performance', 'model']) \\\n",
    "      .to_csv(f'results/performance_{str(DATASET).lower()}.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply each explanation method per model\n",
    "Do this with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explanation_methods = [SEDC(), PWWSAntonym(), eBERT(batch_size=32), TextFooler()]\n",
    "\n",
    "for target_seed in range(5):\n",
    "    for _, dataset, _, predict_fn in trained_models:\n",
    "        results = []\n",
    "\n",
    "        print(f'> Model \"{predict_fn}\" on dataset \"{dataset}\" (target seed={target_seed})')\n",
    "        np.random.seed(SEED)\n",
    "        d = dataset.get(part='test')\n",
    "        try:\n",
    "            X = d['X']\n",
    "        except KeyError:\n",
    "            X = d[['X_premise', 'X_hypothesis']]\n",
    "        y_true = d['y']\n",
    "        y_target = dataset.target(part='test', seed=target_seed)\n",
    "        \n",
    "        for explanation_method in explanation_methods:\n",
    "            if hasattr(explanation_method, 'seed'):\n",
    "                explanation_method.seed = target_seed\n",
    "            print(f'|--> {explanation_method}')\n",
    "            explanation_method.target_size = dataset.target_size\n",
    "            \n",
    "            if explanation_method.provide_true_labels:\n",
    "                res = explanation_method(X, predict_fn, y_target, y_true, return_y=True) \n",
    "            else:\n",
    "                res = explanation_method(X, predict_fn, y_target, return_y=True)\n",
    "            p, counterfactuals, y_cf = res\n",
    "            p['model'] = str(predict_fn).lower()\n",
    "            p['dataset'] = str(dataset).lower()\n",
    "            p['explanation_method'] = str(explanation_method).lower().split('(')[0]\n",
    "            p['seed'] = explanation_method.seed\n",
    "            p['similarity_std'] = np.std(p['X_sim'])\n",
    "            p['semantic_std'] = np.std(p['X_sem'])\n",
    "            p['target_seed'] = target_seed\n",
    "            p['counterfactuals'] = counterfactuals\n",
    "            p['y_target'] = y_target\n",
    "            p['y_cf'] = y_cf\n",
    "            results.append(p)\n",
    "        results = pd.DataFrame(results)[['model', 'dataset', 'explanation_method', 'seed', 'target_seed',\n",
    "                                         'similarity', 'similarity_std', 'X_sim', 'semantic', 'semantic_std',\n",
    "                                         'X_sem', 'performance_measure', 'fidelity', 'training_time',\n",
    "                                         'inference_time', 'counterfactuals', 'y_target', 'y_cf']]\n",
    "        results.to_json(f'results/counterfactuals_{str(DATASET).lower()}_{str(predict_fn).lower()}_seed-{target_seed}+textfooler.json')\n",
    "        print('')\n",
    "\n",
    "    print(f'\\n... Finished seed {target_seed}!\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
