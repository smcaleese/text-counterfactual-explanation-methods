{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_texts_as_baselines = pd.read_csv(\"../../input/imdb_input_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smcaleese/Documents/masters-thesis-code/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-imdb\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2054,  1037,  6752,  1011,  1011,  1998,  1045,  1005,  1049,\n",
       "          2025,  7727,  2000,  1996,  6215,  1999,  1996,  2516,  1012,  1045,\n",
       "          2071,  2175,  2006,  2055,  1996, 28425,  2098,  5436,  1010,  1996,\n",
       "         10223,  6508,  3896,  1010,  1996,  1006,  2941,  3862,  1007,  3459,\n",
       "         11844, 26217,  2004,  2027,  8116,  1996,  5409,  3210,  1997,  2037,\n",
       "         10922,  1010,  4385,  1012,  1045,  1005,  2222,  2074,  2360,  2045,\n",
       "          4694,  1005,  1056,  2151,  5340,  3628,  1999,  3190,  1996,  2197,\n",
       "          2051,  1045,  7039,  1010,  1998,  2681,  2009,  2012,  2008,  1012,\n",
       "         17012,  2213,  1012,  1012,  1012,  2342,  2702,  3210,  2000,  2131,\n",
       "          2023,  6866,  2006, 10047, 18939,  1012,  1012,  7929,  1010,  2092,\n",
       "          1010,  1045,  2228,  1037,  4966,  2713,  2007, 24955,  9681,  2071,\n",
       "          2022,  5875,  1012,  2672,  7939,  2638, 10536,  2097,  7487,  2054,\n",
       "          5684,  2288,  2170,  1999,  2005,  2032,  2000,  3711,  1999,  2023,\n",
       "          2518,  1012,  2672, 22939, 10087, 11417,  3367,  2097,  2265,  2149,\n",
       "          1996,  4524,  1997,  2769,  2009,  2442,  2031,  2579,  2000,  2131,\n",
       "          2014,  2920,  1012,  2672,  6568,  4654,  8586,  2015,  2097, 12134,\n",
       "          1012,  1012,  1012,   102]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence = \"This movie was sadly under-promoted but proved to be truly exceptional. Entering the theatre I knew nothing about the film except that a friend wanted to see it. I was caught off guard with the high quality of the film. I couldn't image Ashton Kutcher in a serious role, but his performance truly exemplified his character. This movie is exceptional and deserves our monetary support, unlike so many other movies. It does not come lightly for me to recommend any movie, but in this case I highly recommend that everyone see it. This films is Truly Exceptional!\"\n",
    "# sentence = \"On a dark, gloomy New Year's Eve night, an ill nurse, her life slowly ebbing away, demands that David Holm be presented to her at once. We don't yet know who David Holm is, or why this nurse wishes to see him, but her only dying wish is to speak with him just one more time. On the other side of the town, nestled comfortably amongst the gravestones of the local cemetery, Holm (Victor Sjöström, who also directed) and two of his drunken associates merrily await the coming of the New Year. \"\"Here we can tell just when to drink the New Year in,\"\" exclaims Holm, casting a finger towards the large clock tower that looms through the darkness. Little does he know, however, that he will not be alive to greet it. To pass the time, Holm cheerfully recites a ghost story. He'd once had a friend name George, \"\"a merry fellow\"\" who was \"\"smarter than the rest of us.\"\" On one New Year's Eve several years ago, George has broken up a potentially disastrous brawl, fearing that the final man to draw his last breath before midnight would be condemned to drive the phantom chariot for the next year, doing Death's bidding and collecting the souls of the deceased. \"\"And, gentlemen, George died last New Year's Eve!\"\" concludes Holm happily, not bothering to contain his mocking skepticism of the man's beliefs. As fate has it, of course, an unexpected violent encounter results in Holm's death, just on the stroke of midnight. As the man's transparent spirit rises gingerly from his earthly body, he witnesses, to his horror, the distant approach of a phantom carriage. The driver, a frail cloaked figure - a sickle clasped tightly in his hand - steps down from the carriage and approaches. We are astonished to discover that the driver is none other than a decrepit George, preparing to pass on his ghastly duty to this year's successor. Considering the era in which 'Körkarlen' is made, the special effects in this film are absolutely superb. Cinematographer Julius Jaenzon used double-exposure photography to create the eerie, ghostly silhouette of the carriage and its damned driver. Even today, the end result is highly effective. A particularly impressive scene involves the phantom chariot travelling to the ocean floor to retrieve the soul of a drowned man. Another scene, eerily reminiscent of Jack Torrance (Jack Nicholson) in Stanley Kubrick's 'The Shining,' involves Holm breaking down the kitchen door with an axe in order to reach his fleeing wife and children. Genuinely ominous and unsettling in its execution, Victor Sjöström's 'Körkarlen' is a fine work of cinema, successfully portraying Holm's steady alcoholic decline, his inevitable day of judgment, and a final hopeful possibility of redemption.\"\n",
    "# sentence = \"Just saw this movie on opening night. I read some other user comments which convinced me to go see it... I must say, I was not impressed. I'm so unimpressed that I feel the need to write this comment to spare some of you people some money. First of all \"\"The Messengers\"\" is very predictable, and just not much of a thriller. It might be scary for someone under 13, but it really did nothing for me. The climax was laughable and most of the audience left before the movie's resolution. Furthermore the acting seemed a little superficial. Some of the emotional arguments between the family were less convincing than the sub-par suspense scenes. If you've seen previews for this movie, then you've seen most of the best parts and have a strong understanding of the plot. This movie is not worth seeing in the theaters.\"\n",
    "sentence = \"What a mess--and I'm not referring to the \"\"destruction\"\" in the title. I could go on about the hackneyed plot, the lousy effects, the (actually notable) cast grimacing as they deliver the worst lines of their careers, etc. I'll just say there weren't any palm trees in Chicago the last time I checked, and leave it at that. Hmmm...need ten lines to get this posted on IMDb.. OK, well, I think a DVD release with outtakes could be interesting. Maybe Dennehy will reveal what favor got called in for him to appear in this thing. Maybe Dianne Weist will show us the bag of money it must have taken to get her involved. Maybe CBS execs will apologize...\"\n",
    "# sentence = \"I hated the movie. It was terrible.\"\n",
    "ids_tensor = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "ids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'a',\n",
       " 'mess',\n",
       " '-',\n",
       " '-',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'not',\n",
       " 'referring',\n",
       " 'to',\n",
       " 'the',\n",
       " 'destruction',\n",
       " 'in',\n",
       " 'the',\n",
       " 'title',\n",
       " '.',\n",
       " 'i',\n",
       " 'could',\n",
       " 'go',\n",
       " 'on',\n",
       " 'about',\n",
       " 'the',\n",
       " 'hackney',\n",
       " '##ed',\n",
       " 'plot',\n",
       " ',',\n",
       " 'the',\n",
       " 'lou',\n",
       " '##sy',\n",
       " 'effects',\n",
       " ',',\n",
       " 'the',\n",
       " '(',\n",
       " 'actually',\n",
       " 'notable',\n",
       " ')',\n",
       " 'cast',\n",
       " 'grim',\n",
       " '##acing',\n",
       " 'as',\n",
       " 'they',\n",
       " 'deliver',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'lines',\n",
       " 'of',\n",
       " 'their',\n",
       " 'careers',\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'just',\n",
       " 'say',\n",
       " 'there',\n",
       " 'weren',\n",
       " \"'\",\n",
       " 't',\n",
       " 'any',\n",
       " 'palm',\n",
       " 'trees',\n",
       " 'in',\n",
       " 'chicago',\n",
       " 'the',\n",
       " 'last',\n",
       " 'time',\n",
       " 'i',\n",
       " 'checked',\n",
       " ',',\n",
       " 'and',\n",
       " 'leave',\n",
       " 'it',\n",
       " 'at',\n",
       " 'that',\n",
       " '.',\n",
       " 'hmm',\n",
       " '##m',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'need',\n",
       " 'ten',\n",
       " 'lines',\n",
       " 'to',\n",
       " 'get',\n",
       " 'this',\n",
       " 'posted',\n",
       " 'on',\n",
       " 'im',\n",
       " '##db',\n",
       " '.',\n",
       " '.',\n",
       " 'ok',\n",
       " ',',\n",
       " 'well',\n",
       " ',',\n",
       " 'i',\n",
       " 'think',\n",
       " 'a',\n",
       " 'dvd',\n",
       " 'release',\n",
       " 'with',\n",
       " 'outta',\n",
       " '##kes',\n",
       " 'could',\n",
       " 'be',\n",
       " 'interesting',\n",
       " '.',\n",
       " 'maybe',\n",
       " 'den',\n",
       " '##ne',\n",
       " '##hy',\n",
       " 'will',\n",
       " 'reveal',\n",
       " 'what',\n",
       " 'favor',\n",
       " 'got',\n",
       " 'called',\n",
       " 'in',\n",
       " 'for',\n",
       " 'him',\n",
       " 'to',\n",
       " 'appear',\n",
       " 'in',\n",
       " 'this',\n",
       " 'thing',\n",
       " '.',\n",
       " 'maybe',\n",
       " 'dia',\n",
       " '##nne',\n",
       " 'wei',\n",
       " '##st',\n",
       " 'will',\n",
       " 'show',\n",
       " 'us',\n",
       " 'the',\n",
       " 'bag',\n",
       " 'of',\n",
       " 'money',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'to',\n",
       " 'get',\n",
       " 'her',\n",
       " 'involved',\n",
       " '.',\n",
       " 'maybe',\n",
       " 'cbs',\n",
       " 'ex',\n",
       " '##ec',\n",
       " '##s',\n",
       " 'will',\n",
       " 'apologize',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'what',\n",
       " 'a',\n",
       " 'mess',\n",
       " '-',\n",
       " '-',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'not',\n",
       " 'referring',\n",
       " 'to',\n",
       " 'the',\n",
       " 'destruction',\n",
       " 'in',\n",
       " 'the',\n",
       " 'title',\n",
       " '.',\n",
       " 'i',\n",
       " 'could',\n",
       " 'go',\n",
       " 'on',\n",
       " 'about',\n",
       " 'the',\n",
       " 'hackney',\n",
       " '##ed',\n",
       " 'plot',\n",
       " ',',\n",
       " 'the',\n",
       " 'lou',\n",
       " '##sy',\n",
       " 'effects',\n",
       " ',',\n",
       " 'the',\n",
       " '(',\n",
       " 'actually',\n",
       " 'notable',\n",
       " ')',\n",
       " 'cast',\n",
       " 'grim',\n",
       " '##acing',\n",
       " 'as',\n",
       " 'they',\n",
       " 'deliver',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'lines',\n",
       " 'of',\n",
       " 'their',\n",
       " 'careers',\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'just',\n",
       " 'say',\n",
       " 'there',\n",
       " 'weren',\n",
       " \"'\",\n",
       " 't',\n",
       " 'any',\n",
       " 'palm',\n",
       " 'trees',\n",
       " 'in',\n",
       " 'chicago',\n",
       " 'the',\n",
       " 'last',\n",
       " 'time',\n",
       " 'i',\n",
       " 'checked',\n",
       " ',',\n",
       " 'and',\n",
       " 'leave',\n",
       " 'it',\n",
       " 'at',\n",
       " 'that',\n",
       " '.',\n",
       " 'hmm',\n",
       " '##m',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'need',\n",
       " 'ten',\n",
       " 'lines',\n",
       " 'to',\n",
       " 'get',\n",
       " 'this',\n",
       " 'posted',\n",
       " 'on',\n",
       " 'im',\n",
       " '##db',\n",
       " '.',\n",
       " '.',\n",
       " 'ok',\n",
       " ',',\n",
       " 'well',\n",
       " ',',\n",
       " 'i',\n",
       " 'think',\n",
       " 'a',\n",
       " 'dvd',\n",
       " 'release',\n",
       " 'with',\n",
       " 'outta',\n",
       " '##kes',\n",
       " 'could',\n",
       " 'be',\n",
       " 'interesting',\n",
       " '.',\n",
       " 'maybe',\n",
       " 'den',\n",
       " '##ne',\n",
       " '##hy',\n",
       " 'will',\n",
       " 'reveal',\n",
       " 'what',\n",
       " 'favor',\n",
       " 'got',\n",
       " 'called',\n",
       " 'in',\n",
       " 'for',\n",
       " 'him',\n",
       " 'to',\n",
       " 'appear',\n",
       " 'in',\n",
       " 'this',\n",
       " 'thing',\n",
       " '.',\n",
       " 'maybe',\n",
       " 'dia',\n",
       " '##nne',\n",
       " 'wei',\n",
       " '##st',\n",
       " 'will',\n",
       " 'show',\n",
       " 'us',\n",
       " 'the',\n",
       " 'bag',\n",
       " 'of',\n",
       " 'money',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'to',\n",
       " 'get',\n",
       " 'her',\n",
       " 'involved',\n",
       " '.',\n",
       " 'maybe',\n",
       " 'cbs',\n",
       " 'ex',\n",
       " '##ec',\n",
       " '##s',\n",
       " 'will',\n",
       " 'apologize',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list = tokenizer.encode(sentence)\n",
    "tokens = tokenizer.convert_ids_to_tokens(id_list)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 3866, 1996, 3185, 1012, 2009, 2001, 10392, 1012, 102]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert that list of tokens into a normal sentence again:\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i loved the movie. it was fantastic.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the ids to a string:\n",
    "sentence = tokenizer.decode(ids[1:-1])\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i loved the movie. it was fantastic.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or you can convert the tokens to a string directly:\n",
    "\n",
    "sentence2 = tokenizer.convert_tokens_to_string(tokens)[6:-6]\n",
    "sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9893,  2.8492]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(ids_tensor).logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = logits.argmax().item() # 0\n",
    "# sentiment_model.config.id2label[predicted_class_id]\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(-1, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join tokens again into a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    " tokens = ['[CLS]',\n",
    " 'what',\n",
    " 'a',\n",
    " 'mess',\n",
    " '-',\n",
    " '-',\n",
    " 'and',\n",
    " 'i',\n",
    " \"'\",\n",
    " 'm',\n",
    " 'not',\n",
    " 'referring',\n",
    " 'to',\n",
    " 'the',\n",
    " 'destruction',\n",
    " 'in',\n",
    " 'the',\n",
    " 'title',\n",
    " '.',\n",
    " 'i',\n",
    " 'could',\n",
    " 'go',\n",
    " 'on',\n",
    " 'about',\n",
    " 'the',\n",
    " 'hackney',\n",
    " '##ed',\n",
    " 'plot',\n",
    " ',',\n",
    " 'the',\n",
    " 'lou',\n",
    " '##sy',\n",
    " 'effects',\n",
    " ',',\n",
    " 'the',\n",
    " '(',\n",
    " 'actually',\n",
    " 'notable',\n",
    " ')',\n",
    " 'cast',\n",
    " 'grim',\n",
    " '##acing',\n",
    " 'as',\n",
    " 'they',\n",
    " 'deliver',\n",
    " 'the',\n",
    " 'worst',\n",
    " 'lines',\n",
    " 'of',\n",
    " 'their',\n",
    " 'careers',\n",
    " ',',\n",
    " 'etc',\n",
    " '.',\n",
    " 'i',\n",
    " \"'\",\n",
    " 'll',\n",
    " 'just',\n",
    " 'say',\n",
    " 'there',\n",
    " 'weren',\n",
    " \"'\",\n",
    " 't',\n",
    " 'any',\n",
    " 'palm',\n",
    " 'trees',\n",
    " 'in',\n",
    " 'chicago',\n",
    " 'the',\n",
    " 'last',\n",
    " 'time',\n",
    " 'i',\n",
    " 'checked',\n",
    " ',',\n",
    " 'and',\n",
    " 'leave',\n",
    " 'it',\n",
    " 'at',\n",
    " 'that',\n",
    " '.',\n",
    " 'hmm',\n",
    " '##m',\n",
    " '.',\n",
    " '.',\n",
    " '.',\n",
    " 'need',\n",
    " 'ten',\n",
    " 'lines',\n",
    " 'to',\n",
    " 'get',\n",
    " 'this',\n",
    " 'posted',\n",
    " 'on',\n",
    " 'im',\n",
    " '##db',\n",
    " '.',\n",
    " '.',\n",
    " 'ok',\n",
    " ',',\n",
    " 'well',\n",
    " ',',\n",
    " 'i',\n",
    " 'think',\n",
    " 'a',\n",
    " 'dvd',\n",
    " 'release',\n",
    " 'with',\n",
    " 'outta',\n",
    " '##kes',\n",
    " 'could',\n",
    " 'be',\n",
    " 'interesting',\n",
    " '.',\n",
    " 'maybe',\n",
    " 'den',\n",
    " '##ne',\n",
    " '##hy',\n",
    " 'will',\n",
    " 'reveal',\n",
    " 'what',\n",
    " 'favor',\n",
    " 'got',\n",
    " 'called',\n",
    " 'in',\n",
    " 'for',\n",
    " 'him',\n",
    " 'to',\n",
    " 'appear',\n",
    " 'in',\n",
    " 'this',\n",
    " 'thing',\n",
    " '.',\n",
    " 'maybe',\n",
    " 'dia',\n",
    " '##nne',\n",
    " 'wei',\n",
    " '##st',\n",
    " 'will',\n",
    " 'show',\n",
    " 'us',\n",
    " 'the',\n",
    " 'bag',\n",
    " 'of',\n",
    " 'money',\n",
    " 'it',\n",
    " 'must',\n",
    " 'have',\n",
    " 'taken',\n",
    " 'to',\n",
    " 'get',\n",
    " 'her',\n",
    " 'involved',\n",
    " '.',\n",
    " 'maybe',\n",
    " 'cbs',\n",
    " 'ex',\n",
    " '##ec',\n",
    " '##s',\n",
    " 'will',\n",
    " 'apologize',\n",
    " '.',\n",
    " '.',\n",
    " '.',\n",
    " '[SEP]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] what a mess - - and i ' m not referring to the destruction in the title. i could go on about the hackneyed plot, the lousy effects, the ( actually notable ) cast grimacing as they deliver the worst lines of their careers, etc. i ' ll just say there weren ' t any palm trees in chicago the last time i checked, and leave it at that. hmmm... need ten lines to get this posted on imdb.. ok, well, i think a dvd release with outtakes could be interesting. maybe dennehy will reveal what favor got called in for him to appear in this thing. maybe dianne weist will show us the bag of money it must have taken to get her involved. maybe cbs execs will apologize... [SEP]\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text_lower = tokenizer.convert_tokens_to_string(tokens)\n",
    "new_text_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a mess-- and i'm not referring to the destruction in the title. I could go on about the hackneyed plot, the lousy effects, the ( actually notable ) cast grimacing as they deliver the worst lines of their careers, etc. I'll just say there weren't any palm trees in chicago the last time I checked, and leave it at that. Hmmm... Need ten lines to get this posted on imdb.. Ok, well, I think a dvd release with outtakes could be interesting. Maybe dennehy will reveal what favor got called in for him to appear in this thing. Maybe dianne weist will show us the bag of money it must have taken to get her involved. Maybe cbs execs will apologize...\n"
     ]
    }
   ],
   "source": [
    "def format_output_text(text):\n",
    "    \"\"\"After the tokens have been converted to a string, this function formats the text to be more readable.\"\"\"\n",
    "\n",
    "    # Removing the special tokens [CLS] and [SEP]\n",
    "    sentence = text[6:-6]\n",
    "    \n",
    "    # Handling spaces around punctuation\n",
    "    sentence = sentence.replace(\" ' \", \"'\").replace(\" - \", \"-\")\n",
    "\n",
    "    # capitalize each sentence and each \"i\"\n",
    "    sentence = \". \".join([s.capitalize() for s in sentence.split(\". \")])\n",
    "    sentence = sentence.replace(\" i \", \" I \")\n",
    "\n",
    "    return sentence\n",
    "\n",
    "sentence = list_to_sentence(new_text_lower)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a51d8a3a0a3d36bb5d163454ac019466f708312c768ddc8f2a90cbfbafa8971e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
